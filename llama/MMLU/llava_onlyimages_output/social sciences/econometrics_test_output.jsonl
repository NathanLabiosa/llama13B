{"question": "Which one of the following is the most appropriate definition of a 99% confidence interval?", "choices": [{"label": "A", "text": "99% of the time in repeated samples, the interval would contain the true value of the parameter"}, {"label": "B", "text": "99% of the time in repeated samples, the interval would contain the estimated value of the parameter"}, {"label": "C", "text": "99% of the time in repeated samples, the null hypothesis will be rejected"}, {"label": "D", "text": "99% of the time in repeated samples, the null hypothesis will not be rejected when it was false"}], "answerKey": "A", "model_answer": "LECTURE: A confidence interval is a range of values within which the true value of a parameter is believed to lie. For example, a 95% confidence interval for the true mean height of a population would mean that if the sample size is large enough, 95% of the confidence intervals would contain the true mean height.\nThe width of a confidence interval is determined by the standard error of the estimate. The standard error is a measure of the spread of the possible true values of the parameter.\nA 99% confidence interval means that 99% of the time in repeated samples, the interval would contain the true value of the parameter. This does not mean that 99% of the confidence intervals will contain the true value. It means that 99% of the time, the true value will be inside the interval.\nSOLUTION: A 99% confidence interval for a parameter will contain the true value of the parameter 99% of the time. So, 99% of the time in repeated samples, the interval would contain the true value of the parameter."}
{"question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?", "choices": [{"label": "A", "text": "ADF is a single equation approach to unit root testing while PP is a systems approach"}, {"label": "B", "text": "PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test"}, {"label": "C", "text": "The PP test incorporates an automatic correction for autocorrelated residuals in the test regression"}, {"label": "D", "text": "PP tests have good power in small samples whereas DF tests do not."}], "answerKey": "C", "model_answer": "SOLUTION: The PP test incorporates an automatic correction for autocorrelated residuals in the test regression. The DF test does not correct for autocorrelated residuals."}
{"question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?", "choices": [{"label": "A", "text": "It would rise more quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "B", "text": "It would be symmetrical about zero"}, {"label": "C", "text": "It would rise less quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "D", "text": "It would be zero for all positive disturbances"}], "answerKey": "A", "model_answer": "SOLUTION: A model that accounted for a leverage effect would have a news impact curve that rose more quickly for negative disturbances than for positive ones of the same magnitude. This means that the news impact curve would have a negative slope, which is the opposite of a horizontal slope and a positive slope."}
{"question": "Which of the following statements is false concerning the linear probability model?", "choices": [{"label": "A", "text": "There is nothing in the model to ensure that the estimated probabilities lie between zero and one"}, {"label": "B", "text": "Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one"}, {"label": "C", "text": "The error terms will be heteroscedastic and not normally distributed"}, {"label": "D", "text": "The model is much harder to estimate than a standard regression model with a continuous dependent variable"}], "answerKey": "D", "model_answer": "LECTURE: A linear probability model is a model of the form:\nP(Y=1|X=x) = f(x)\nwhere P(Y=1|X=x) is the probability that a binary outcome occurs when the independent variable is x, and f(x) is a function that maps values of X to values of P(Y=1|X=x).\nThe error term in a linear probability model is F(x) = P(Y=1|X=x) \u2013 f(x).\nSOLUTION: The error terms in a linear probability model are heteroscedastic and not normally distributed.\nThis is a false statement.\nThe error terms in a linear probability model are normally distributed and equal to the difference between the predicted probability and the true probability.\nThe linear probability model is much harder to estimate than a standard regression model with a continuous dependent variable.\nThis is a false statement.\nLinear probability models are usually simpler to estimate than regression models with a continuous dependent variable.\nThere is nothing in the model to ensure that the estimated probabilities lie between zero and one.\nThis is a false statement.\nThe model includes the restriction that the estimated probabilities lie between zero and one.\nEven if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one.\nThis is a false statement.\nTruncating the estimated probabilities at zero and one will ensure that all observed values of the dependent variable have a corresponding estimated probability."}
{"question": "Which of the following statements concerning the regression population and sample is FALSE?", "choices": [{"label": "A", "text": "The population is the total collection of all items of interest"}, {"label": "B", "text": "The population can be infinite"}, {"label": "C", "text": "In theory, the sample could be larger than the population"}, {"label": "D", "text": "A random sample is one where each individual item from the population is equally likely to be drawn."}], "answerKey": "C", "model_answer": "SOLUTION: The population is the total collection of all items of interest.\nThe sample is a subset of the population.\nIn theory, the sample could be larger than the population.\nA random sample is one where each individual item from the population is equally likely to be drawn.\nThese statements are all true."}
{"question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?", "choices": [{"label": "A", "text": "Asymptotically, the values of the two test statistics will be equal"}, {"label": "B", "text": "The Q test has better small-sample properties than the Q*"}, {"label": "C", "text": "The Q test is sometimes over-sized for small samples"}, {"label": "D", "text": "As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients."}], "answerKey": "B", "model_answer": "SOLUTION: The Q test and the Q* test are used to detect linear dependence in a time series. Both tests calculate the statistical measure of the Box-Pierce Q and the Ljung-Box Q, respectively.\nThe Q test has better small-sample properties than the Q* test.\nThe Q test is sometimes over-sized for small samples.\nAs the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.\nAsymptotically, the values of the two test statistics will be equal.\nThis means that as the sample size gets larger and larger, the values of the Q and Q* test statistics will get closer and closer to each other."}
{"question": "A parsimonious model is one that", "choices": [{"label": "A", "text": "Includes too many variables"}, {"label": "B", "text": "Includes as few variables as possible to explain the data"}, {"label": "C", "text": "Is a well-specified model"}, {"label": "D", "text": "Is a mis-specified model"}], "answerKey": "A", "model_answer": "SOLUTION: A parsimonious model is one that includes as few variables as possible to explain the data.\nA well-specified model is a model that has a clear interpretation and a sensible set of assumptions.\nA mis-specified model is a model that is not a good fit for the data.\nModels that include too many variables can be overly complex and may include variables that are not useful in explaining the data."}
{"question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?", "choices": [{"label": "A", "text": "They can be interpreted as continuously compounded changes in the prices"}, {"label": "B", "text": "They can be added over time to give returns for longer time periods"}, {"label": "C", "text": "They can be added across a portfolio of assets to give portfolio returns"}, {"label": "D", "text": "They are usually fat-tailed"}], "answerKey": "C", "model_answer": "LECTURE: Continuously compounded returns, also known as log-returns, are a way to describe the returns of an investment over time.\nLog-returns are calculated by taking the logarithm of the ratio of the investment's ending value to its beginning value.\nContinuously compounded returns have the following features:\nThey can be interpreted as continuously compounded changes in the prices of investments.\nThey can be added over time to give returns for longer time periods.\nThey can be added across a portfolio of assets to give portfolio returns.\nThey are usually fat-tailed.\nFat-tailed returns are returns that are more likely to be extreme.\nSOLUTION: Addition across a portfolio of assets is not a feature of continuously compounded returns.\nContinuously compounded returns can be used to calculate the returns of a portfolio of assets, but they are not added across the portfolio.\nInstead, the continuously compounded returns of each asset in the portfolio are first calculated, and then they are used to calculate the portfolio's return."}
{"question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "LECTURE: A GARCH model is a type of statistical model. It is used to predict the volatility of a financial asset's returns. In a GARCH model, the volatility of the returns is determined by the volatility of the past returns.\nA GARCH(1,1) model is a type of GARCH model. It uses one historical realization of the volatility of the returns to predict the volatility of the returns.\nA GARCH(1,1) model can capture the following features of a financial asset's return time-series:\nVolatility clustering refers to the tendency of a financial asset's returns to have high volatility at times when the asset's volatility is high.\nVolatility affecting returns refers to the tendency of a financial asset's returns to be more volatile when the asset's volatility is high.\nA GARCH(1,1) model cannot capture the following features of a financial asset's return time-series:\nFat tails in the return distribution refer to the fact that the return distribution has a tail that extends further to the left and right than a normal distribution.\nLeverage effects refer to the fact that a financial asset's returns are higher in periods of high trading volume."}
{"question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?", "choices": [{"label": "A", "text": "Less than -1"}, {"label": "B", "text": "Between -1 and 0"}, {"label": "C", "text": "Between 0 and 1"}, {"label": "D", "text": "Bigger than 1"}], "answerKey": "C", "model_answer": "LECTURE: A GARCH-M model is a type of statistical model that is used to predict the volatility of a time series. The GARCH-M model includes two different types of variables: the mean and the variance of the time series.\nThe mean of the time series is modeled using an exponential function of the previous mean, or a simple moving average of the previous mean. The variance of the time series is modeled using a GARCH(1,1) model. The GARCH(1,1) model uses two variables to model the variance: the previous standard deviation of the time series and the previous variance of the time series.\nThe GARCH-in-mean parameter estimates the effect of the mean of the time series on the variance of the time series. This parameter is important because it affects the value of the GARCH(1,1) model's variance.\nIf the value of the GARCH-in-mean parameter is less than -1, then the mean of the time series has a negative effect on the variance of the time series. This means that when the mean of the time series is higher, the variance of the time series is lower.\nIf the value of the GARCH-in-mean parameter is between -1 and 0, then the mean of the time series has a negative effect on the variance of the time series, but this effect is smaller when the mean of the time series is higher.\nIf the value of the GARCH-in-mean parameter is between 0 and 1, then the mean of the time series has a positive effect on the variance of the time series. This means that when the mean of the time series is higher, the variance of the time series is higher.\nIf the value of the GARCH-in-mean parameter is bigger than 1, then the mean of the time series has a positive effect on the variance of the time series, but this effect is larger when the mean of the time series is higher.\nSOLUTION: A time-series of daily corporate bond percentage returns has a positive mean. So, the value of the GARCH-in-mean parameter is expected to be between 0 and 1."}
{"question": "Under which of the following situations would bootstrapping be preferred to pure simulation?\n\ni) If it is desired that the distributional properties of the data in the experiment\n\nare the same as those of some actual data\n\n\nii) If it is desired that the distributional properties of the data in the experiment\n\nare known exactly\n\n\niii) If the distributional properties of the actual data are unknown\n\n\niv) If the sample of actual data available is very small", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": ""}
{"question": "Which of the following may be consequences of one or more of the CLRM assumptions being violated?\n\ni) The coefficient estimates are not optimal\n\n\nii) The standard error estimates are not optimal\n\n\niii) The distributions assumed for the test statistics are inappropriate\n\n\niv) Conclusions regarding the strength of relationships between the dependent\n\nand independent variables may be invalid.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: The components of the CLRM assume that the errors are independent and identically distributed. Violations of this assumption can lead to conclusions regarding the strength of relationships between variables being invalid.\nViolations of the normality assumption can lead to standard errors and test statistics that are not optimal.\nViolations of the homoscedasticity assumption can lead to standard errors and test statistics that are not optimal.\nViolations of the normality assumption can affect the logic of the test, leading to conclusions regarding the strength of relationships between variables being invalid.\nViolations of the homoscedasticity assumption can affect the logic of the test, leading to conclusions regarding the strength of relationships between variables being invalid."}
{"question": "Which of the following statements is true concerning forecasting in econometrics?", "choices": [{"label": "A", "text": "Forecasts can only be made for time-series data"}, {"label": "B", "text": "Mis-specified models are certain to produce inaccurate forecasts"}, {"label": "C", "text": "Structural forecasts are simpler to produce than those from time series models"}, {"label": "D", "text": "In-sample forecasting ability is a poor test of model adequacy"}], "answerKey": "D", "model_answer": "SOLUTION: Forecasting is an important part of econometric analysis. When developing a model for predictive purposes, a key concern is how well the model will forecast future outcomes.\nThere are several types of econometric forecasts:\nA structural forecast predicts future outcomes based on economic reasoning and a model of the underlying economic relationships.\nA time-series forecast predicts future outcomes based on historical patterns in the data.\nA cross-section forecast predicts future outcomes based on the relationships between variables in a single snapshot of the data.\nIn-sample forecasting ability is a poor test of model adequacy. In-sample forecasting ability refers to how well the model predicts outcomes within the period of time for which there is data. This test is a poor way to evaluate the model's predictive power because the model was fitted to the data it is supposed to predict.\nMis-specified models are certain to produce inaccurate forecasts. A mis-specified model is a model that does not accurately capture the economic relationships in the data. These models are sure to produce inaccurate forecasts.\nStructural forecasts are simpler to produce than those from time series models. Structural forecasts are based on economic reasoning and are often simpler to produce than time-series forecasts. Time-series forecasts can be complex because they can involve many variables and require careful consideration of the structure of the data.\nForecasts can only be made for time-series data. Forecasts can be made for any type of data, including time-series, cross-section, and panel data. However, time-series forecasts are a type of forecast that focuses on the patterns in historical data."}
{"question": "The pacf is necessary for distinguishing between", "choices": [{"label": "A", "text": "An AR and an MA model"}, {"label": "B", "text": "An AR and an ARMA model"}, {"label": "C", "text": "An MA and an ARMA model"}, {"label": "D", "text": "Different models from within the ARMA family"}], "answerKey": "B", "model_answer": "SOLUTION: The pacf is necessary for distinguishing between different models from within the ARMA family.\nAn AR and an MA model are different models from within the ARMA family. The pacf can distinguish between these two models.\nAn AR and an ARMA model are not different models from within the ARMA family. The pacf cannot distinguish between these two models.\nAn MA and an ARMA model are different models from within the ARMA family. The pacf can distinguish between these two models.\nDifferent models from within the ARMA family cannot be distinguished by the autocorrelation function (ACF) or the partial autocorrelation function (PACF)."}
{"question": "Negative residual autocorrelation is indicated by which one of the following?", "choices": [{"label": "A", "text": "A cyclical pattern in the residuals"}, {"label": "B", "text": "An alternating pattern in the residuals"}, {"label": "C", "text": "A complete randomness in the residuals"}, {"label": "D", "text": "Residuals that are all close to zero"}], "answerKey": "B", "model_answer": "LECTURE: In a time series, autocorrelation occurs when the value of a variable at one time is correlated with the value of the same variable at other times. This means that the variable's values at different times are not independent.\nSOLUTION: Negative residual autocorrelation is a pattern in which the residuals from a model have a negative correlation. This pattern indicates that the variables in the model have a negative residual correlation.\nA cyclical pattern in the residuals is a type of pattern that indicates negative residual autocorrelation. So, a cyclical pattern in the residuals would suggest that there is negative residual autocorrelation.\nAn alternating pattern in the residuals is a type of pattern that indicates negative residual autocorrelation. So, an alternating pattern in the residuals would suggest that there is negative residual autocorrelation.\nComplete randomness in the residuals means that the residuals are unpredictable. This pattern indicates that there is no correlation among the residuals. So, complete randomness in the residuals would suggest that there is no negative residual autocorrelation.\nResiduals that are all close to zero would make the model perform well. But this pattern does not give any information about the correlation among the residuals. So, residuals that are all close to zero would not suggest that there is negative residual autocorrelation."}
{"question": "Which of the following statements are true concerning a comparison between ARCH(q) and GARCH(1,1) models?\n\ni) The ARCH(q) model is likely to be the more parsimonious\n\n\nii) The ARCH(q) model is the more likely to violate non-negativity constraints\n\n\niii) The ARCH(q) model can allow for an infinite number of previous lags of squared\n\nreturns to affect the current conditional variance\n\n\niv) The GARCH(1,1) model will usually be sufficient to capture all of the dependence\n\nin the conditional variance", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "LECTURE: When comparing ARCH(q) and GARCH(1,1) models, it is important to consider the differences between the two models.\nThe ARCH(q) model is a generalization of the GARCH(1,1) model. The GARCH(1,1) model uses one lag of squared errors and one lag of innovation to estimate the conditional variance. The ARCH(q) model uses q lags of squared errors and innovation to estimate the conditional variance.\nThe GARCH(1,1) model will usually be sufficient to capture all of the dependence in the conditional variance. This means that the GARCH(1,1) model can often describe the relationship between past returns and the current variance.\nThe ARCH(q) model can allow for an infinite number of previous lags of squared returns to affect the current conditional variance. This makes the ARCH(q) model more powerful than the GARCH(1,1) model for describing the relationship between past returns and the current variance.\nHowever, the ARCH(q) model is also more complex than the GARCH(1,1) model. For example, the ARCH(q) model has more parameters that need to be estimated.\nSo, the ARCH(q) model is more powerful, but also more complex than the GARCH(1,1) model.\nThe GARCH(1,1) model is less powerful because it uses only one lag of squared errors and innovation to estimate the conditional variance. This means that the GARCH(1,1) model can only describe the relationship between one lag of past returns and the current variance.\nThe GARCH(1,1) model will usually be sufficient to capture all of the dependence in the conditional variance. This is a different statement. It means that the GARCH(1,1) model can often describe the relationship between past returns and the current variance, but it does not need to capture all of the dependence.\nThe ARCH(q) model is the more likely to violate non-negativity constraints. This means that the ARCH(q) model may produce values that are less than zero.\nThe GARCH(1,1) model is less likely to violate non-negativity constraints."}
{"question": "Which of the following statements are true concerning a triangular or recursive system?\n\ni) The parameters can be validly estimated using separate applications of OLS to\n\neach equation\n\n\nii) The independent variables may be correlated with the error terms in other\n\nequations\n\n\niii) An application of 2SLS would lead to unbiased but inefficient parameter estimates\n\n\niv) The independent variables may be correlated with the error terms in the equations\n\nin which they appear as independent variables", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: A triangular or recursive system is a system of equations in which one or more equations depend on the parameters to be estimated. The parameters in a triangular system are not normally estimated using separate applications of OLS to each equation. Instead, a technique called 2SLS (Two-Stage LeCTURE: Ordinary least squares (OLS) is a method for estimating parameters in a system of linear equations. One of the assumptions of OLS is that the independent variables do not correlate with the error terms. This assumption is important for getting unbiased estimates of the parameters.\nOne way to deal with a lack of independence between the independent variables and the error terms is to use a method called 2SLS. This method involves two stages of estimation. In the first stage, you estimate the parameters of an instrumental variables (IV) model. In the second stage, you use the parameters from the first stage to estimate the parameters of the original system.\nThe use of 2SLS is appropriate in a triangular system. In a triangular system, one or more equations depend on the parameters to be estimated. So, the parameters cannot be normally estimated using separate applications of OLS to each equation.\nThe independent variables in a triangular system may be correlated with the error terms in other equations. So, the independent variables may be correlated with the error terms in the equations in which they appear as independent variables.\nAn application of 2SLS would lead to unbiased but inefficient parameter estimates. This is because 2SLS uses two stages to estimate the parameters. The first stage involves estimating the parameters of an IV model, which is not used in the second stage. So, the estimates from 2SOLUTION: A triangular or recursive system is a system of equations in which one or more equations depend on the parameters to be estimated. The parameters in a triangular system are not normally estimated using separate applications of OLS to each equation. Instead, a technique called 2SLS (Two-Stage LECTURE: Ordinary least squares (OLS) is a method for estimating parameters in a system of linear equations. One of the assumptions of OLS is that the independent variables do not correlate with the error terms. This assumption is important for getting unbiased estimates of the parameters.\nOne way to deal with a lack of independence between the independent variables and the error terms is to use a method called 2SLS. This method involves two stages of estimation. In the first stage, you estimate the parameters of an instrumental variables (IV) model. In the second stage, you use the parameters from the first stage to estimate the parameters of the original system.\nThe use of 2SLS is appropriate in a triangular system. In a triangular system, one or more equations depend on the parameters to be estimated. So, the parameters cannot be normally estimated using separate applications of OLS to each equation.\nThe independent variables in a triangular system may be correlated with the error terms in other equations. So, the independent variables may be correlated with the error terms in the equations in which they appear as independent variables.\nAn application of 2SLS would lead to unbiased but inefficient parameter estimates. This is because 2SLS uses two stages to estimate the parameters. The first stage involves estimating the parameters of an IV model, which is not used in the second stage. So, the estimates from 2SLS are generally less efficient than the estimates from OLS."}
{"question": "Which one of the following statements best describes the algebraic representation of the fitted regression line?", "choices": [{"label": "A", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}, {"label": "B", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t"}, {"label": "C", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + u_t"}, {"label": "D", "text": "y_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}], "answerKey": "B", "model_answer": "LECTURE: The algebraic representation of a regression line is given by the equation y = \\beta x + \\alpha, where \\beta is the slope of the line and \\alpha is a constant.\nThe slope \\beta and the constant \\alpha of the regression line are estimated using the least squares method. The procedure yields the following formulas:\n\\beta = the slope of the regression line.\n\\alpha = the y-intercept of the regression line.\n\\hat{\\beta} = the estimated slope of the regression line.\n\\hat{\\alpha} = the estimated y-intercept of the regression line.\nThese formulas can be used to write the algebraic representation of the fitted regression line as \\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x, where \\hat{y} is an abbreviation for \\hat{y}_t, the predicted value of the response variable at time t.\nx is an abbreviation for x\\_t, the predictor variable at time t.\nThe symbols \\hat{\\beta} and \\hat{\\alpha} represent the estimated slope and y-intercept, respectively.\nThe actual slope and y-intercept of the regression line are \\beta and \\alpha, respectively.\nThe symbol \\hat{u} represents the error term of the regression line, which is the difference between the predicted value of the response variable and the actual value.\n\\hat{u} is determined by the residual, the difference between the actual and predicted values of the response variable.\nResidual = Actual value of the response variable - \\hat{y}\n\\hat{u} = Residual / x"}
{"question": "What are the dimensions of $\\hat{u}^t \\hat{u}?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "D", "model_answer": "LECTURE: A vector has dimension. The dimension of a vector is a measure of the number of columns in the vector.\nIf a vector has more columns than rows, the vector is in T x k space, where k is the number of columns in the vector.\nIf a vector has more rows than columns, the vector is in k x T space, where k is the number of rows in the vector.\nSOLUTION: The dimension of a matrix is a measure of the number of columns in the matrix.\nSo, the dimension of $\\hat{u}^t \\hat{u}$ is the number of columns in the matrix. Each row of the matrix has one column."}
{"question": "The characteristic roots of the MA process\n\n$y_t = -3u_{t-1} + u_{t-2} + u_t$\n\nare", "choices": [{"label": "A", "text": "1 and 2"}, {"label": "B", "text": "1 and 0.5"}, {"label": "C", "text": "2 and -0.5"}, {"label": "D", "text": "1 and -3"}], "answerKey": "B", "model_answer": "SOLUTION: The characteristic roots of the MA process are the values of \u03bb for which the process is stationary. A stationary process has the property that its statistics do not change over time.\nA stationary MA process has the following property:\nIf $y\\_t$ is a stationary MA process with characteristic roots \u03bb1 and \u03bb2, then Yt is stationary, where Yt is a process with all of its terms equal to 0 except for yt, which is equal to 0.\nThis means that the characteristic roots must satisfy the equation 1 = \u03bb1 + \u03bb2."}
{"question": "Which of the following is an equivalent expression for saying that the explanatory variable is \"non-stochastic\"?", "choices": [{"label": "A", "text": "The explanatory variable is partly random"}, {"label": "B", "text": "The explanatory variable is fixed in repeated samples"}, {"label": "C", "text": "The explanatory variable is correlated with the errors"}, {"label": "D", "text": "The explanatory variable always has a value of one"}], "answerKey": "B", "model_answer": ""}
{"question": "Suppose that the Durbin Watson test is applied to a regression containing two explanatory variables plus a constant with 50 data points. The test statistic takes a value of 1.53. What is the appropriate conclusion?", "choices": [{"label": "A", "text": "Residuals appear to be positively autocorrelated"}, {"label": "B", "text": "Residuals appear to be negatively autocorrelated"}, {"label": "C", "text": "Residuals appear not to be autocorrelated"}, {"label": "D", "text": "The test result is inconclusive"}], "answerKey": "D", "model_answer": "LECTURE: The Durbin Watson test is a test for the presence of autocorrelation in the residuals. Autocorrelation occurs when a variable is correlated with its own lagged values.\nThe test statistic for the Durbin Watson test is based on the differences between consecutive residuals. The test statistic is as follows:\nDW = |RST| / ST,\nwhere RST is the sum of the squared residuals and ST is the sum of the squared terms in the regression.\nIf DW is less than 2, the test suggests that there is at least one lagged response in the regression. If DW is between 2 and 4, the test is inconclusive. If DW is greater than 4, the test suggests that there are no lags in the regression.\nSOLUTION: The test statistic takes a value of 1.53. According to the test statistic, there is at least one lagged response in the regression.\nThe Durbin Watson test suggests that residuals appear to be positively autocorrelated."}
{"question": "If OLS is used in the presence of autocorrelation, which of the following will be likely consequences?\n\ni) Coefficient estimates may be misleading\n\n\nii) Hypothesis tests could reach the wrong conclusions\n\n\niii) Forecasts made from the model could be biased\n\n\niv) Standard errors may inappropriate", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": ""}
{"question": "What will be the properties of the OLS estimator in the presence of multicollinearity?", "choices": [{"label": "A", "text": "It will be consistent, unbiased and efficient"}, {"label": "B", "text": "It will be consistent and unbiased but not efficient"}, {"label": "C", "text": "It will be consistent but not unbiased"}, {"label": "D", "text": "It will not be consistent"}], "answerKey": "A", "model_answer": "SOLUTION: In the presence of multicollinearity, the OLS estimator will be consistent, but not necessarily unbiased or efficient.\nConsistency: A consistent estimator is one that is always finite. The OLS estimator is consistent in the presence of multicollinearity because the coefficients are still bounded and finite.\nUnbiasedness: A consistent estimator is not necessarily unbiased. An unbiased estimator is one that has an expected value of zero. The OLS estimator is not unbiased in the presence of multicollinearity because the expected value of the estimator is not zero.\nEfficiency: An efficient estimator is one that has the minimum variance among all unbiased estimators. The OLS estimator is not efficient in the presence of multicollinearity because the variance of the estimator is higher than it could be."}
{"question": "Which one of the following would NOT be a consequence of using non-stationary data in levels form?", "choices": [{"label": "A", "text": "The regression $R^2$ may be spuriously high"}, {"label": "B", "text": "Test statistics may not follow standard distributions"}, {"label": "C", "text": "Statistical inferences may be invalid"}, {"label": "D", "text": "Parameter estimates may be biased"}], "answerKey": "D", "model_answer": "LECTURE: When data are non-stationary, the model's parameters change over time. So, when non-stationary data are used in levels form, estimates of the model's parameters may be biased.\nHowever, the fact that data are non-stationary in levels form does not affect the regression $R^2$ value. The $R^2$ value is a measure of the goodness of fit of the model. It can be used to assess the strength of the relationship between the independent and dependent variables in a model. The $R^2$ value is not affected by whether the data are non-stationary in levels form or in first differences form.\nTest statistics may not follow standard distributions when data are non-stationary in levels form. This can affect the validity of inferences based on the test statistics.\nStatistical inferences may be invalid when data are non-stationary in levels form. This means that conclusions based on the data may be incorrect."}
{"question": "If a series, y, follows a random walk, what is the optimal one-step ahead forecast of y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of y over the in-sample period"}], "answerKey": "A", "model_answer": "SOLUTION: A random walk is a sequence of numbers in which each number is a function of the number that came before it. In other words, the value of each number in the sequence is determined by its predecessor. So, the value of y at any given time is determined by the value of y at the previous time step.\nIf a series follows a random walk, the one-step ahead forecast is the value of y at the next time step. This forecast is the same as the current value of y."}
{"question": "The order condition is", "choices": [{"label": "A", "text": "A necessary and sufficient condition for identification"}, {"label": "B", "text": "A necessary but not sufficient condition for identification"}, {"label": "C", "text": "A sufficient but not necessary condition for identification"}, {"label": "D", "text": "A condition that is nether necessary nor sufficient for identification"}], "answerKey": "B", "model_answer": "LECTURE: Conditions can be specified to identify an equation. An equation is a set of one or more equations that are equal to each other. Equations can be identified by specifying the values of their variables.\nFor example, the following three equations are equal to each other:\ny = x + 1\ny = x + 2\ny = x + 3\nThey are equal because, for each value of x, the first equation produces the same value as the second equation, which produces the same value as the third equation.\nYou can use conditions to identify which equations these are. The condition y = x + 1 is a necessary and sufficient condition for identifying the first and second equations as equal. The condition y = x + 2 is a necessary but not sufficient condition for identifying the second and third equations as equal.\nA necessary condition for identification is a condition that, when it is true, shows that two equations are equal.\nA sufficient condition for identification is a condition that, when it is true, ensures that two equations are equal."}
{"question": "If an estimator is said to have minimum variance, which of the following statements is NOT implied?", "choices": [{"label": "A", "text": "The probability that the estimate is a long way away from its true value is minimised"}, {"label": "B", "text": "The estimator is efficient"}, {"label": "C", "text": "Such an estimator would be termed \"best\""}, {"label": "D", "text": "Such an estimator will always be unbiased"}], "answerKey": "D", "model_answer": ""}
{"question": "Which of the following are disadvantages of the Dickey-Fuller / Engle-Granger approach to testing for cointegration and modelling cointegrating relationships?\n\ni) Only one cointegrating relationship can be estimated\n\nii) Particularly for small samples. There is a high chance of the tests suggestingthat variables are not cointegrated when they are\n\niii) It is not possible to make inferences on the cointegrating regression\n\niv) The procedure forces the researcher to specify which is the dependent variable and which are the independent variables.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: The Dickey-Fuller and Engle-Granger approaches are statistical tests used to determine if two or more time series variables are cointegrated. Cointegration is a statistical property that allows two or more non-stationary time series variables to be combined into a stationary series.\nThese tests can indicate whether two or more time series variables are likely to be cointegrated. However, they can't determine the specific relationship between the variables.\nThe test statistics are based on the assumptions that the data is stationary and that the data is normally distributed. Violations of these assumptions can lead to incorrect conclusions.\nIn addition, both tests have low power for small samples. This means that if you have a small sample, the tests may not be able to detect cointegration.\nLastly, the tests require you to specify which variable is the dependent variable and which are the independent variables. This specification can be difficult to determine in some cases."}
{"question": "Which of the following statements is true concerning the population regression function (PRF) and sample regression function (SRF)?", "choices": [{"label": "A", "text": "The PRF is the estimated model"}, {"label": "B", "text": "The PRF is used to infer likely values of the SRF"}, {"label": "C", "text": "Whether the model is good can be determined by comparing the SRF and the PRF"}, {"label": "D", "text": "The PRF is a description of the process thought to be generating the data."}], "answerKey": "D", "model_answer": ""}
{"question": "Which one of the following is a disadvantage of the general to specific or \"LSE\" (\"Hendry\") approach to building econometric models, relative to the specific to general approach?", "choices": [{"label": "A", "text": "Some variables may be excluded at the first stage leading to coefficient biases"}, {"label": "B", "text": "The final model may lack theoretical interpretation"}, {"label": "C", "text": "The final model may be statistically inadequate"}, {"label": "D", "text": "If the initial model is mis-specified, all subsequent steps will be invalid."}], "answerKey": "B", "model_answer": "LECTURE: One of the most important decisions in building an econometric model is whether to start with a specific equation and then generalize to other settings, or to start with a general equation and then specialize to specific settings. The first approach is sometimes called the \"LSE\" approach, after the initials of one of its proponents, James Hendry. The second approach is sometimes called the \"TSY\" approach, after the initials of another proponent, Angus Maddison.\nBoth approaches have their own set of trade-offs. One disadvantage of the LSE approach is that it may lead to a model that is statistically inadequate. This means that the model may not have enough statistical power to produce reliable estimates of the coefficients.\nAnother possible disadvantage of the LSE approach is that the final model may lack theoretical interpretation. This means that it may be difficult to understand the underlying economic mechanisms that are driving the results.\nAn advantage of the LSE approach is that it may produce a model that is more easily interpretable. This is because the model is based on a specific equation that is tailored to the setting of the data.\nAnother advantage of the LSE approach is that it may produce a model that is more statistically efficient. This means that the model may be able to produce reliable estimates of the coefficients with fewer observations than a model that is estimated using the TSY approach.\nAnother disadvantage of the LSE approach is that some variables may be excluded at the first stage leading to coefficient biases. This means that the model may produce coefficients that are not accurate because some variables that should have been included in the model were excluded.\nIf the initial model is misspecified, all subsequent steps will be invalid. This means that if the model is estimated using the wrong equations, the estimates of the coefficients will be biased and the model will not produce accurate predictions.\nSOLUTION: The disadvantage of the LSE approach is that some variables may be excluded at the first stage leading to coefficient biases. This means that the model may produce coefficients that are not accurate because some variables that should have been included in the model were excluded."}
{"question": "Which of the following statements are true concerning maximum likelihood (ML) estimation in the context of GARCH models?\n\ni) Maximum likelihood estimation selects the parameter values that maximise the\n\nprobability that we would have actually observed the values of the series y that we\n\nactually did.\n\n\nii) GARCH models can only be estimated by ML and not by OLS\n\n\niii) For estimation of a standard linear model (with no GARCH), the OLS and ML\n\nestimates for the slope and intercept parameters will be identical but the estimator\n\nfor the variance of the disturbances is slightly different\n\n\niv) Most computer packages use numerical procedures to estimate GARCH models\n\nrather than a set of analytical formulae", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: Maximum likelihood estimation selects the parameter values that maximize the probability that we would have actually observed the values of the series y that we actually did.\nFor a GARCH model, this means that maximum likelihood estimation selects the parameter values that make the observed volatility of the series most likely, given the values of the other model parameters.\nIn other words, maximum likelihood estimation allows us to estimate the parameters of a GARCH model by finding the values of the parameters that produce the observed volatility of the series, given the values of the other model parameters.\nBecause GARCH models can capture a wide variety of volatility patterns, including those that cannot be captured by traditional time series models, many researchers consider GARCH models to be the state-of-the-art for modeling volatility.\nTherefore, it is only through maximum likelihood estimation that the parameters of a GARCH model can be estimated.\nStandard linear models and GARCH models are fundamentally different.\nA standard linear model is a model that uses a straight line to model the relationship between a dependent variable and one or more independent variables.\nIn a standard linear model, the OLS and ML estimates for the slope and intercept parameters will be identical, and the estimator for the variance of the disturbances will be identical to the estimate of the standard error of the regression.\nA GARCH model is a model that uses a more flexible form to model the relationship between a dependent variable and one or more independent variables.\nIn a GARCH model, the OLS and ML estimates for the parameters may be different.\nThe OLS and ML estimates for the parameters of a GARCH model may be different because the OLS estimates may fail to account for the fact that the GARCH model is a more flexible form than a standard linear model.\nThis is because the OLS estimates may be based on the assumptions of a standard linear model, which are not satisfied by a GARCH model.\nThere are many software packages that can estimate GARCH models.\nSome of these software packages use numerical procedures to estimate GARCH models, while others use a set of analytical formulae.\nHowever, the use of numerical procedures or analytical formulae to estimate a GARCH model does not affect the fact that maximum likelihood estimation is the only method for estimating the parameters of a GARCH model."}
{"question": "Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to which one of the following?", "choices": [{"label": "A", "text": "Biased but consistent coefficient estimates"}, {"label": "B", "text": "Biased and inconsistent coefficient estimates"}, {"label": "C", "text": "Unbiased but inconsistent coefficient estimates"}, {"label": "D", "text": "Unbiased and consistent but inefficient coefficient estimates."}], "answerKey": "A", "model_answer": "LECTURE: Including lagged values of the dependent variable on the right hand side of a regression equation may lead to biased but consistent coefficient estimates.\nA regression model with a lagged dependent variable includes one or more lags of the dependent variable as independent variables in the model. This can help improve the model's ability to predict future values of the dependent variable, because the model can use information from previous values of the dependent variable to do so.\nHowever, including a lagged dependent variable in a regression model can also make the coefficient estimates biased. This happens because the use of lagged values of the dependent variable as independent variables in the model can cause the model to include a feedforward loop. A feedforward loop happens when a variable affects a variable that feeds back to affect the first variable. Such a loop can make the model include a direct effect of the dependent variable on the dependent variable. This direct effect is not a true effect, but it can make the coefficient estimates inconsistent.\nDespite the bias, the coefficient estimates are consistent. This means that the estimates are not affected by the use of lagged values of the dependent variable in the model. This consistency can be helpful because it means that the estimates are not subject to the potential problems that can arise when coefficient estimates are influenced by the inclusion of irrelevant or inappropriate variables in the model.\nHowever, the bias in the coefficient estimates means that they may not be as accurate as they could be. In addition, the inconsistency of the coefficient estimates means that they may not be reliable from one iteration of the model to the next.\nSOLUTION: Including a lagged dependent variable in a regression model can lead to biased but consistent coefficient estimates. These estimates are consistent, but they may be biased. This means that the estimates are not subject to the potential problems that can arise when coefficient estimates are influenced by the inclusion of irrelevant or inappropriate variables in the model. But it also means that the estimates may not be as accurate as they could be."}
{"question": "Which one of the following factors is likely to lead to a relatively high degree of out-of-sample forecast accuracy?", "choices": [{"label": "A", "text": "A model that is based on financial theory"}, {"label": "B", "text": "A model that contains many variables"}, {"label": "C", "text": "A model whose dependent variable has recently exhibited a structural change"}, {"label": "D", "text": "A model that is entirely statistical in nature with no room for judgmental modification of forecasts"}], "answerKey": "A", "model_answer": "SOLUTION: A model that is based on financial theory is likely to lead to a relatively high degree of out-of-sample forecast accuracy.\nModels that are based on financial theory are designed to capture the relationships that exist between different financial variables. These models can be powerful tools for making accurate out-of-sample forecasts.\nModels that contain many variables are more complex, and therefore more likely to be overfitted to the data. Overfitting occurs when a model is too closely tailored to the data it is based on. This can make the model accurate for the data it was trained on, but it does not make the model accurate for new, unseen data.\nA model whose dependent variable has recently exhibited a structural change may require a fundamentally different type of model than was previously used. Such a model may be less likely to be able to make accurate out-of-sample forecasts.\nA model that is entirely statistical in nature with no room for judgmental modification of forecasts is a purely mechanistic model. Such models can be accurate, but they do not allow for any consideration of the specific context in which the model is being used. In many cases, the specific context in which a model is being used can have a significant impact on the accuracy of the model's out-of-sample forecasts."}
{"question": "Which of the following are plausible approaches to dealing with residual autocorrelation?\n\ni) Take logarithms of each of the variables\n\nii) Add lagged values of the variables to the regression equation\n\niii) Use dummy variables to remove outlying observations\n\niv) Try a model in first differenced form rather than in levels.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: Residual autocorrelation occurs when the residuals from a regression have a correlation with the dependent variable. This can be a problem because it can lead to biased estimates of the parameters.\nOne of the ways to deal with residual autocorrelation is to include lagged values of the dependent variable in the regression equation. This can help to reduce the correlation of the residuals with the dependent variable.\nAnother way is to use dummy variables to remove outlying observations. This can help to reduce the correlation of the residuals with the dependent variable.\nTaking logarithms of each of the variables and using a model in first differenced form are not typically used to deal with residual autocorrelation."}
{"question": "For an autoregressive process to be considered stationary", "choices": [{"label": "A", "text": "The roots of the characteristic equation must all lie inside the unit circle"}, {"label": "B", "text": "The roots of the characteristic equation must all lie on the unit circle"}, {"label": "C", "text": "The roots of the characteristic equation must all lie outside the unit circle"}, {"label": "D", "text": "The roots of the characteristic equation must all be less than one in absolute value"}], "answerKey": "C", "model_answer": "SOLUTION: To prove that an autoregressive process is stationary, you need to show that its autocorrelation function is constant. One way to do this is to look at the characteristic equation of the process.\nThe characteristic equation of an autoregressive process with a root inside the unit circle will have a constant term that is equal to the value of the root. So, the roots of the characteristic equation must all lie inside the unit circle.\nThe characteristic equation of an autoregressive process with a root on the unit circle will have a constant term that is equal to the value of the root. So, the roots of the characteristic equation must all lie on the unit circle.\nThe roots of the characteristic equation must be less than one in absolute value for the process to be stationary.\nThe roots of the characteristic equation must all lie outside the unit circle for the process to be stationary."}
{"question": "Which of the following statements are true concerning information criteria?\n\n(i) Adjusted R-squared is an information criterion\n\n(ii) If the residual sum of squares falls when an additional term is added, the value of the information criterion will fall\n\n(iii) Akaike's information criterion always leads to model orders that are at least as large as those of Schwarz's information criterion\n\n(iv) Akaike's information criterion is consistent", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "LECTURE: Information criteria are measures of the goodness of fit of a model. They can be used to compare models to each other.\nTwo of the most popular information criteria are Akaike's information criterion (AIC) and Schwarz's information criterion (SIC).\nBoth criteria consider the difference between the observed data and the model's predictions, as well as the number of parameters in the model. AIC considers the number of parameters in the model and penalizes additional parameters. SIC considers the residual sum of squares, which is the difference between the observed data and the model's predictions, and penalizes additional parameters.\nBoth criteria produce smaller values as the model order increases. This means that a lower value of AIC or SIC is better.\nAIC and SIC can be used to compare models to each other. If the value of the information criterion is lower for a model than for another model, then the former model is better according to the information criterion.\nAIC and SIC are consistent. This means that if a model is the best model according to the information criterion, then it is also the best model according to the statistical tests that are based on the residual sum of squares.\nAIC and SIC are not consistent with respect to model order. This means that a model may be the best model according to the information criterion, but there is no statistical evidence to support the conclusion that the model is better than another model. In other words, the information criterion does not necessarily lead to the same conclusion as the statistical tests that are based on the residual sum of squares.\nThe residual sum of squares is a measure of the difference between the observed data and the model's predictions. It can be used to compare models to each other.\nIf the residual sum of squares falls when an additional term is added to a model, then the value of the information criterion will fall. This means that adding the additional term to the model is beneficial according to the information criterion."}
{"question": "The \"within transform\" involves", "choices": [{"label": "A", "text": "Taking the average values of the variables"}, {"label": "B", "text": "Subtracting the mean of each entity away from each observation on that entity"}, {"label": "C", "text": "Estimating a panel data model using least squares dummy variables"}, {"label": "D", "text": "Using both time dummies and cross-sectional dummies in a fixed effects panel model"}], "answerKey": "B", "model_answer": ""}
{"question": "The purpose of \"augmenting\" the Dickey-Fuller test regression is to", "choices": [{"label": "A", "text": "Ensure that there is no heteroscedasticity in the test regression residuals."}, {"label": "B", "text": "Ensure that the test regression residuals are normally distributed"}, {"label": "C", "text": "Ensure that there is no autocorrelation in the test regression residuals"}, {"label": "D", "text": "Ensure that all of the non-stationarity is taken into account."}], "answerKey": "C", "model_answer": "SOLUTION: The Dickey-Fuller test is a test for stationarity. Stationarity is a necessary condition for many time series models. One way to satisfy this condition is to use a differencing scheme. Differencing can remove non-stationary trends and/or seasonality, and can make a time series stationary.\nThe augmented Dickey-Fuller (ADF) test is a test for stationarity that has been augmented by adding lagged test regression residuals to the residuals of the Dickey-Fuller test. The purpose of augmenting the test is to ensure that all of the non-stationarity is taken into account. This means that the null hypothesis of the ADF test is that the time series is non-stationary, which is the same null hypothesis as the Dickey-Fuller test. In other words, the ADF test is a way to test the same hypothesis as the Dickey-Fuller test, but with more information.\nThe test regression in the ADF test includes lagged test regression residuals. This means that the ADF test includes information about how the time series has changed in the past. This can help the test identify non-stationary patterns more effectively than the Dickey-Fuller test alone."}
{"question": "If a series, y, follows a random walk with drift b, what is the optimal one-step ahead forecast of the change in y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of the change in y over the in-sample period"}], "answerKey": "D", "model_answer": "SOLUTION: The optimal one-step ahead forecast of the change in y is the average value of the change in y over the in-sample period."}
{"question": "Which of the following are plausible approaches to dealing with a model that exhibits heteroscedasticity?\n\ni) Take logarithms of each of the variables\n\nii) Use suitably modified standard errors\n\niii) Use a generalised least squares procedure\n\niv) Add lagged values of the variables to the regression equation.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: Heteroscedasticity leads to inconsistent standard errors and test statistics. To deal with heteroscedasticity, we can use one of the following approaches:\ni. Take logarithms of each of the variables.\nTaking the logarithm of a variable can help to reduce the impact of heteroscedasticity. This is because the logarithm of a variable has a smaller variance than the variable itself.\nii. Use suitably modified standard errors.\nOne way to deal with heteroscedasticity is to use standard errors that are modified to account for the heteroscedasticity. For example, White's method uses a modified version of the standard error to estimate the t-statistic.\niii. Use a generalised least squares procedure.\nGeneralised least squares (GLS) is a technique for addressing heteroscedasticity. GLS uses a weighted least squares approach, where the weights are chosen to account for the heteroscedasticity.\niv. Add lagged values of the variables to the regression equation.\nAdding lagged values of the variables to the regression equation can help to reduce the impact of heteroscedasticity. This is because lagged values of a variable have a smaller variance than the variable itself."}
{"question": "Which of the following statements are true concerning the standardised residuals (residuals divided by their respective conditional standard deviations) from an estimated GARCH model?\n\ni) They are assumed to be normally distributed\n\n\nii) Their squares will be related to their lagged squared values if the GARCH model is\n\nappropriate\n\n\niii) In practice, they are likely to have fat tails\n\n\niv) If the GARCH model is adequate, the standardised residuals and the raw residuals\n\nwill be identical", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The GARCH model is a type of statistical model. Statistical models are represented by equations. Equations have variables and coefficients. The variables in the equation are used to describe the data. The coefficients measure how much the variables affect the output.\nWhen you estimate a GARCH model, you use the data to calculate the coefficients. The standardised residuals are the residuals, which are the difference between the observed and predicted values, divided by their respective conditional standard deviations. They are a measure of the relative size of the residuals.\nStandardised residuals are assumed to be normally distributed. This means that they have a normal distribution. A normal distribution is a distribution that is symmetric and has a bell-shaped curve.\nThe normal distribution has two parameters: the mean and the standard deviation. The standardised residuals will have a mean of zero and a standard deviation of one.\nThe squares of the standardised residuals will be related to the lagged squared values if the GARCH model is appropriate. This means that the GARCH model is a good model for the data.\nIn practice, standardised residuals are likely to have fat tails. This means that the standardised residuals will have a greater spread than a normal distribution would predict.\nThe raw residuals and the standardised residuals will be identical if the GARCH model is adequate. An adequate model is a model that is appropriate for the data."}
{"question": "Which one of the following statements is true concerning VARs?", "choices": [{"label": "A", "text": "The coefficient estimates have intuitive theoretical interpretations"}, {"label": "B", "text": "The coefficient estimates usually have the same sign for all of the lags of a given variable in a given equation"}, {"label": "C", "text": "VARs often produce better forecasts than simultaneous equation structural models"}, {"label": "D", "text": "All of the components of a VAR must be stationary before it can be used for forecasting"}], "answerKey": "C", "model_answer": "LECTURE: A vector auto regression (VAR) is a model that shows how several variables change over time. It is a type of linear model, and it is similar in structure to a simultaneous equation model.\nOne key difference between a VAR and a simultaneous equation model is that a VAR does not require the variables to be influenced jointly by other variables. This means that the VAR can include variables that are not included in the simultaneous equation model.\nAnother key difference is that a VAR can include variables that are not normally distributed. This means that the VAR can include variables that are skewed or have outliers.\nThe coefficients in a VAR have theoretical interpretations. For example, the coefficient in a VAR for a variable represents the change in that variable when one of the other variables in the model changes by one. These changes are called impulse responses.\nThe coefficient estimates in a VAR have the same sign as the theoretical coefficients. This means that the signs of the estimates are correct.\nThe magnitude of the coefficient estimates is not the same as the magnitude of the coefficients. The magnitude of a coefficient is a measure of the size of the coefficient. The estimates from a VAR are standardized, which means that their magnitude is equal to the magnitude of the coefficients.\nThe signs of the estimates are correct. So, if the coefficients are positive, the estimates are positive. If the coefficients are negative, the estimates are also negative.\nThe magnitude of the estimates is equal to the magnitude of the coefficients. So, if the coefficients are small, the estimates will also be small.\nA VAR with stationary components is used for forecasting. A stationary variable is a variable whose statistical properties do not change over time. This means that the mean, the variance, and the autocorrelation of a stationary variable are constant.\nAll of the components of a VAR must be stationary before it can be used for forecasting. This is because forecasting is based on the assumption that the statistical properties of the variables will not change over the forecast period.\nIf one or more components of a VAR are not stationary, then the VAR cannot be used to forecast future values of the variables.\nThe signs of the estimates are correct. So, if the coefficients are positive, the estimates are positive. If the coefficients are negative, the estimates are also negative.\nThe magnitude of the estimates is equal to the magnitude of the coefficients. So, if the coefficients are small, the estimates will also be small."}
{"question": "Which of the following statements is INCORRECT concerning the classical hypothesis testing framework?", "choices": [{"label": "A", "text": "If the null hypothesis is rejected, the alternative is accepted"}, {"label": "B", "text": "The null hypothesis is the statement being tested while the alternative encompasses the remaining outcomes of interest"}, {"label": "C", "text": "The test of significance and confidence interval approaches will always give the same conclusions"}, {"label": "D", "text": "Hypothesis tests are used to make inferences about the population parameters."}], "answerKey": "A", "model_answer": "SOLUTION: The classical hypothesis testing framework is a statistical method used to make inferences about population parameters. It involves testing a null hypothesis against an alternative hypothesis.\nThe null hypothesis is the statement being tested, while the alternative encompasses the remaining outcomes of interest.\nIf the null hypothesis is rejected, the alternative is accepted. This is the basis of the statistical significance of the test.\nTests of significance and confidence intervals can be used to determine whether the null hypothesis should be rejected. They will not always give the same conclusions.\nFor example, a confidence interval can indicate that the null hypothesis should not be rejected, while a test of significance can indicate that the null hypothesis should be rejected."}
{"question": "An ARMA(p,q) (p, q are integers bigger than zero) model will have", "choices": [{"label": "A", "text": "An acf and pacf that both decline geometrically"}, {"label": "B", "text": "An acf that declines geometrically and a pacf that is zero after p lags"}, {"label": "C", "text": "An acf that declines geometrically and a pacf that is zero after q lags"}, {"label": "D", "text": "An acf that is zero after p lags and a pacf that is zero after q lags"}], "answerKey": "A", "model_answer": "LECTURE: In an ARMA(p,q) model, the acf declines geometrically as the lags increase. This means that the autocorrelations for lags greater than q are all equal to 0.\nThe pacf is zero after q lags. This means that all the correlations for lags greater than q are equal to 0.\nSo, the only thing that the given question guarantees is that the acf will decline geometrically, and that the pacf will be zero after q lags. It does not guarantee that the pacf will decline geometrically."}
{"question": "Suppose that the following regression is estimated using 27 quarterly observations:\n\n$y_t = \\beta_1 + \\beta_2 x_2 + \\beta_3 x_{3t} + u_t$\n\nWhat is the appropriate critical value for a 2-sided 5% size of test of $H_0: \\beta_3 = 1$?", "choices": [{"label": "A", "text": "1.64"}, {"label": "B", "text": "1.71"}, {"label": "C", "text": "2.06"}, {"label": "D", "text": "1.96"}], "answerKey": "C", "model_answer": "LECTURE: In a regression model, the estimated coefficients can be used to make predictions about the outcome variable. These predictions may be used to calculate a critical value.\nA critical value is a value that can be compared to a test statistic. If the test statistic is less than the critical value, you can reject the null hypothesis.\nOne example of a test is the F-test. The F-test can be used to determine whether to reject the null hypothesis that all the estimated coefficients are zero.\nTo perform the F-test, calculate the F-statistic, which is the ratio of the magnitude of the test statistic to the magnitude of its asymptotic covarianter. Then, compare the F-statistic to a critical value.\nThe critical value for a 2-sided 5% size of test is 1.96 for the F-statistic.\nSo, the appropriate critical value for a 2-sided 5% size of test of H0: \u03b23 = 1 is 1.96."}
{"question": "Suppose that two researchers, using the same 3 variables and the same 250 observations on each variable, estimate a VAR. One estimates a VAR(6), while the other estimates a VAR(4). The determinants of the variance-covariance matrices of the residuals for each VAR are 0.0036 and 0.0049 respectively. What is the values of the test statistic for performing a test of whether the VAR(6) can be restricted to a VAR(4)?", "choices": [{"label": "A", "text": "77.07"}, {"label": "B", "text": "0.31"}, {"label": "C", "text": "0.33"}, {"label": "D", "text": "4.87"}], "answerKey": "A", "model_answer": "LECTURE: A test can be applied to determine whether a VAR(6) can be reduced to a VAR(4) without loss of information. One way to do this is to estimate a VAR(6) and a VAR(4) using the same data, and then test the residuals from each model. The test compares the determinants of the variance-covariance matrices of the residuals for the two models.\nThe test statistic for this test is given by the following formula:\nwhere k is the number of variables in the restricted model.\nIn this case, k = 4.\nSo, the test statistic is 77.07."}
{"question": "Which of the following is a DISADVANTAGE of using pure time-series models (relative to structural models)?", "choices": [{"label": "A", "text": "They are not theoretically motivated"}, {"label": "B", "text": "They cannot produce forecasts easily"}, {"label": "C", "text": "They cannot be used for very high frequency data"}, {"label": "D", "text": "It is difficult to determine the appropriate explanatory variables for use in pure time-series models"}], "answerKey": "A", "model_answer": "SOLUTION: Pure time-series models use only historical data to make predictions. They do not consider any explanatory variables.\nStructural models use explanatory variables to make forecasts.\nThe advantage of using structural models is that they can provide more accurate forecasts. This is because structural models use explanatory variables to make predictions.\nThe disadvantage of using pure time-series models is that they cannot use explanatory variables to make predictions. This means that pure time-series models may be less accurate than structural models."}
{"question": "Which of the following are alternative names for the dependent variable (usually denoted by y) in linear regression analysis?\n\n(i) The regressand\n\n(ii) The regressor\n\n(iii) The explained variable\n\n(iv) The explanatory variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The explanatory variable is an alternative name for the independent variable.\nThe regressand is an alternative name for the dependent variable.\nThe regressor is an alternative name for the independent variable.\nThe explained variable is an alternative name for the dependent variable."}
{"question": "Which of the following are advantages of the VAR approach to modelling the relationship between variables relative to the estimation of full structural models?\n\ni) VARs receive strong motivation from financial and economic theory\n\n\nii) VARs in their reduced forms can be used easily to produce time-series forecasts\n\n\niii) VAR models are typically highly parsimonious\n\n\niv) OLS can be applied separately to each equation in a reduced form VAR", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: TheVAR approach provides several advantages over full structural models:\nVAR models are typically highly parsimonious.\nThe reduced form of a VAR contains only the coefficients that determine the relationship between the variables in the form of a linear combination of lags.\nThe VAR approach allows for easy identification of the groups of variables that have co-integrating relationships.\nCo-integration is a relationship that exists when two or more non-stationary time series become stationary together.\nThe VAR approach allows for easy estimation of the long-run coefficients of the co-integrating relationships.\nThe long-run coefficients can be used to make predictions about how the variables will move in the long run.\nOLS can be applied separately to each equation in a reduced form VAR.\nThe coefficients from each equation can be used to make predictions about how each variable will move."}
{"question": "Which of the following statements is TRUE concerning the standard regression model?", "choices": [{"label": "A", "text": "y has a probability distribution"}, {"label": "B", "text": "x has a probability distribution"}, {"label": "C", "text": "The disturbance term is assumed to be correlated with x"}, {"label": "D", "text": "For an adequate model, the residual (u-hat) will be zero for all sample data points"}], "answerKey": "A", "model_answer": "LECTURE: A regression model is a statistical model that describes the relationship between a dependent variable and one or more independent variables. The most common type of regression model is the linear regression model. In a linear regression model, the dependent variable is a continuous variable, and the independent variable is a predictor variable.\nThe standard regression model is the linear regression model with constant errors. In this model, the disturbance term is assumed to be correlated with the predictor variables, but the correlations are constant.\nSOLUTION: In the standard regression model, the disturbance term is assumed to be correlated with the predictor variables, but the correlations are constant. So, the disturbance term does not have a probability distribution.\nThe predictor variables have a probability distribution, but the dependent variable does not.\nThe residual (u-hat) is the difference between the dependent variable and the predicted value of the dependent variable. In an adequate model, the residual will be zero for all sample data points. This does not mean that the dependent variable has a probability distribution."}
{"question": "Consider the following model for $y_t$:\n\n$y_t = \\mu + \\lambda t + u_t$\n\nWhich one of the following most accurately describes the process for $y_t$?", "choices": [{"label": "A", "text": "A unit root process"}, {"label": "B", "text": "A stationary process"}, {"label": "C", "text": "A deterministic trend process"}, {"label": "D", "text": "A random walk with drift"}], "answerKey": "C", "model_answer": "LECTURE: A unit root process is a type of stochastic process, which is a mathematical model that describes how something changes over time.\nA stationary process is a type of stochastic process.\nA deterministic trend process is a type of stochastic process.\nA random walk with drift is a type of stochastic process.\nIn a random walk with drift, the change in a variable is random, and the change is not based on the previous value of the variable.\nIn a unit root process, the process is not stationary. The process may have a trend or a seasonal pattern, and the mean and the variance of the process are not constant over time.\nIn a stationary process, the mean and the variance of the process are constant over time. This type of process is also called a statistically stable process.\nIn a deterministic trend process, the process has a clear upward or downward trend. The mean of the process is not constant over time, but the variance is.\nA random walk with drift has a clear upward or downward trend. The mean of the process is not constant over time, and the variance is not constant over time.\nSOLUTION: A stationary process has a constant mean and a constant variance. So, the following describes a stationary process:\n$y\\_t = \\mu + \\lambda t + u\\_t$\nIf the mean and variance of a process are constant, then the process is stationary."}
{"question": "Which of the following is correct concerning logit and probit models?", "choices": [{"label": "A", "text": "They use a different method of transforming the model so that the probabilities lie between zero and one"}, {"label": "B", "text": "The logit model can result in too many observations falling at exactly zero or exactly one"}, {"label": "C", "text": "For the logit model, the marginal effect of a change in one of the explanatory variables is simply the estimate of the parameter attached to that variable, whereas this is not the case for the probit model"}, {"label": "D", "text": "The probit model is based on a cumulative logistic function"}], "answerKey": "A", "model_answer": "SOLUTION: The logit and probit models are two types of models that can be used to analyze the relationship between one or more independent variables and a dependent variable. These models use a different method of transforming the model so that the probabilities lie between zero and one.\nThe logit model uses the logit function, which is the logarithm of the ratio of the probability of a successful outcome to the probability of an unsuccessful outcome:\nThe probit model uses the probit function, which is the inverse of the cumulative distribution function of a standard normal distribution:\nBoth the logit and probit models estimate a set of parameters that represent the effect of each independent variable on the dependent variable. For the logit model, the marginal effect of a change in one of the explanatory variables is simply the estimate of the parameter attached to that variable. For the probit model, the marginal effect of a change in one of the explanatory variables is not the estimate of the parameter attached to that variable.\nThe logit model can result in too many observations falling at exactly zero or exactly one. This means that the model is not a good fit for the data.\nThe probit model is based on a cumulative logistic function. This means that the probit model has a similar structure to the logit model."}
{"question": "What is the most important disadvantage of the diagonal VECH approach to building multivariate GARCH models that is overcome by the BEKK formulation?", "choices": [{"label": "A", "text": "The diagonal VECH model is hard to interpret intuitively"}, {"label": "B", "text": "The diagonal VECH model contains too many parameters"}, {"label": "C", "text": "The diagonal VECH model does not ensure a positive-definite variance-covariance matrix"}, {"label": "D", "text": "The BEKK model reduces the dimensionality problem that arises when a number of series are modelled together."}], "answerKey": "C", "model_answer": "SOLUTION: The most important disadvantage of the diagonal VECH approach to building multivariate GARCH models that is overcome by the BEKK formulation is that the diagonal VECH model does not ensure a positive-definite variance-covariance matrix. The BEKK model, by using the covariance matrix of the standardized variables, ensures that the variance-covariance matrix is positive-definite. This is important for the model to have well-behaved statistical properties."}
{"question": "If a relevant variable is omitted from a regression equation, the consequences would be that:\n\ni) The standard errors would be biased\n\n\nii) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope coefficients will be inconsistent.\n\n\niii) If the excluded variable is uncorrelated with all of the included variables, the\n\nintercept coefficient will be inconsistent.\n\n\niv) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope and intercept coefficients will be consistent and unbiased but inefficient.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: Omitting a relevant variable from a regression equation can lead to bias in the standard errors of the estimated coefficients. This means that the statistical tests used to determine if the coefficients are significantly different from zero may show false positives or false negatives.\nIt is important to note that the omitted variable is relevant if it is correlated with at least one of the included variables.\nIf the excluded variable is uncorrelated with all of the included variables, all of the slope and intercept coefficients will be consistent and unbiased but inefficient. This means that the coefficients will accurately estimate the effect of the included variables on the outcome variable, but including the irrelevant variable will increase the size of the model and possibly lead to multicollinearity.\nIf the excluded variable is uncorrelated with all of the included variables, the intercept coefficient will be consistent. This means that the sign of the intercept coefficient will be correct.\nIf the excluded variable is uncorrelated with all of the included variables, the slope coefficients will be inconsistent. This means that the estimates of the slope coefficients may be the wrong sign.\nIf the excluded variable is uncorrelated with all of the included variables, the standard errors will be biased. This means that the standard errors may be too small, and the statistical tests used to determine if the coefficients are significantly different from zero may show false positives."}
{"question": "Which of the following are alternative names for the independent variable (usually denoted by x) in linear regression analysis?\n\n(i) The regressor\n\n(ii) The regressand\n\n(iii) The causal variable\n\n(iv) The effect variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": ""}
{"question": "Consider the OLS estimator for the standard error of the slope coefficient. Which of the following statement(s) is (are) true?\n\n(i) The standard error will be positively related to the residual variance\n\n(ii) The standard error will be negatively related to the dispersion of the observations on the explanatory variable about their mean value\n\n(iii) The standard error will be negatively related to the sample size\n\n(iv) The standard error gives a measure of the precision of the coefficient estimate.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: The standard error gives a measure of the precision of the coefficient estimate.\nThe OLS estimator has the following form:\nThe standard error of the coefficient estimate is the square root of the variance of the estimator. The variance of the estimator is given by:\nThis means that the standard error is negatively related to the dispersion of the observations on the explanatory variable about their mean value.\nThe standard error is also positively related to the residual variance.\nThe sample size affects both the residual variance and the dispersion of the observations about their mean value. But the effect of the sample size on the standard error is smaller than the effect of the dispersion of the observations about their mean value. So, the standard error is negatively related to the sample size, but this relationship is weaker than the relationships with the dispersion of the observations and the residual variance."}
{"question": "What is the meaning of the term \"heteroscedasticity\"?", "choices": [{"label": "A", "text": "The variance of the errors is not constant"}, {"label": "B", "text": "The variance of the dependent variable is not constant"}, {"label": "C", "text": "The errors are not linearly independent of one another"}, {"label": "D", "text": "The errors have non-zero mean"}], "answerKey": "A", "model_answer": "LECTURE: When the variance of the errors is not constant, the errors in the model are not equally important. This means that some errors have more impact on the estimate of the dependent variable than do others.\nHeteroscedasticity means that the variance of the errors is not constant. This can happen when the dependent variable is measured with some degree of error. In other cases, heteroscedasticity can occur because of the nature of the independent variable.\nSOLUTION: Heteroscedasticity means that the variance of the errors is not constant. This can happen when the dependent variable is measured with some degree of error.\nThe variance of the dependent variable is not constant. This can happen when the dependent variable is measured with some degree of error.\nThe errors are not linearly independent of one another. Linear independence means that the errors are not predictable from each other.\nThe errors have a non-zero mean. A non-zero mean is a condition that applies to the dependent variable, not the errors."}
{"question": "If a Durbin Watson statistic takes a value close to zero, what will be the value of the first order autocorrelation coefficient?", "choices": [{"label": "A", "text": "Close to zero"}, {"label": "B", "text": "Close to plus one"}, {"label": "C", "text": "Close to minus one"}, {"label": "D", "text": "Close to either minus one or plus one"}], "answerKey": "C", "model_answer": ""}
{"question": "Under the null hypothesis of a Bera-Jarque test, the distribution has", "choices": [{"label": "A", "text": "Zero skewness and zero kurtosis"}, {"label": "B", "text": "Zero skewness and a kurtosis of three"}, {"label": "C", "text": "Skewness of one and zero kurtosis"}, {"label": "D", "text": "Skewness of one and kurtosis of three."}], "answerKey": "B", "model_answer": "LECTURE: In a Bera-Jarque test, the null hypothesis is that the distribution is symmetric. Under the null hypothesis, the distribution has zero skewness and a specific kurtosis. The kurtosis is a measure of the distribution's \"tailedness\" or \"peakedness.\"\nThe kurtosis of a normal distribution is defined to be three. If the kurtosis is greater than three, the distribution is said to have heavier tails than a normal distribution. If the kurtosis is less than three, the distribution is said to have lighter tails than a normal distribution.\nA common alternative to the normal distribution is the lognormal distribution. The kurtosis of a lognormal distribution is undefined. So, the lognormal distribution can have any kurtosis.\nSOLUTION: Under the null hypothesis of a Bera-Jarque test, the distribution has zero skewness and a kurtosis of three.\nThe kurtosis of a normal distribution is three. So, under the null hypothesis, the distribution is normal."}
{"question": "If an estimator is said to be consistent, it is implied that", "choices": [{"label": "A", "text": "On average, the estimated coefficient values will equal the true values"}, {"label": "B", "text": "The OLS estimator is unbiased and no other unbiased estimator has a smaller variance"}, {"label": "C", "text": "The estimates will converge upon the true values as the sample size increases"}, {"label": "D", "text": "The coefficient estimates will be as close to their true values as possible for small and large samples."}], "answerKey": "C", "model_answer": "SOLUTION: Consistent estimators are those that, on average, get closer to the true parameter values as the sample size increases.\nIf an estimator is consistent, it is implied that, on average, the estimated coefficient values will equal the true values.\nThis means that, as the sample size increases, the average of the estimated coefficient values will get closer and closer to the true average coefficient value."}
{"question": "Which of the following is a typical characteristic of financial asset return time-series?", "choices": [{"label": "A", "text": "Their distributions are thin-tailed"}, {"label": "B", "text": "They are not weakly stationary"}, {"label": "C", "text": "They are highly autocorrelated"}, {"label": "D", "text": "They have no trend"}], "answerKey": "D", "model_answer": "LECTURE: A financial asset's return time-series is a series of returns that the asset has generated in the past. The characteristics of a financial asset's return time-series can tell you how the asset is likely to perform in the future.\nSome of the typical characteristics of financial asset return time-series are:\nThey are thin-tailed. This means that the returns are not normally distributed, and the asset is unlikely to experience extreme losses.\nThey are weakly stationary. This means that the mean-return and the autocorrelation of the returns are constant over time.\nThey are highly autocorrelated. This means that the returns are correlated with each other.\nThey have a trend. This means that the returns are correlated with a particular time-series.\nSOLUTION: A financial asset's return time-series is not weakly stationary if the mean-return is not constant over time. So, non-stationary time-series are not weakly stationary.\nA financial asset's return time-series is highly autocorrelated if the returns are correlated with each other. So, a highly autocorrelated time-series has a trend.\nA financial asset's return time-series is thin-tailed if the returns are not normally distributed, and the asset is unlikely to experience extreme losses.\nA financial asset's return time-series has a trend if the returns are correlated with a particular time-series."}
{"question": "Which of the following assumptions are required to show the consistency, unbiasedness and efficiency of the OLS estimator?\n\ni) $E(u_t) = 0$\n\n\nii) $\\text{Var}(u_t) = \\sigma^2$\n\n\niii) $\\text{Cov}(u_t, u_{t-j}) = 0 \\forall j$\n\n\niv) $u_t \\sim N(0, \\sigma^2)$", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": ""}
{"question": "Which of the following is a disadvantage of the fixed effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The model is likely to be technical to estimate"}, {"label": "B", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "C", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "D", "text": "The fixed effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}], "answerKey": "C", "model_answer": ""}
{"question": "Consider an identical situation to that of question 21, except that now a 2-sided alternative is used. What would now be the appropriate conclusion?", "choices": [{"label": "A", "text": "H0 is rejected"}, {"label": "B", "text": "H0 is not rejected"}, {"label": "C", "text": "H1 is rejected"}, {"label": "D", "text": "There is insufficient information given in the question to reach a conclusion"}], "answerKey": "A", "model_answer": "LECTURE: When testing a hypothesis, one of the possible outcomes is that you fail to reject the null hypothesis. This means that, based on the data, you do not have enough evidence to claim that the null hypothesis is false.\nIn other words, the null hypothesis is not rejected. This outcome does not prove that the null hypothesis is true. But it does indicate that, based on the data, you do not have enough evidence to make a case for rejecting the null hypothesis.\nOn the other hand, if you reject the null hypothesis, it means that you have evidence to support the alternative hypothesis. But it does not necessarily mean that you have proven the alternative hypothesis.\nYou can use statistical tests to determine if you have enough evidence to reject the null hypothesis. One of the most common tests is the p-value test. If the p-value is less than a specific critical value, then you reject the null hypothesis.\nSOLUTION: A two-sided alternative can be written as H1: \u03bc \u2260 0. The test statistic is a t-value, and the p-value is given. To determine if the test results reject the null hypothesis, compare the p-value to the significance level.\nIf the p-value is less than the significance level, then you reject the null hypothesis. Since the p-value is less than 0.05, you reject H0.\nIf the p-value is greater than the significance level, then you fail to reject the null hypothesis. Since the p-value is greater than 0.05, you do not reject H0."}
{"question": "The price of a house is best described as what type of number?", "choices": [{"label": "A", "text": "Discrete"}, {"label": "B", "text": "Cardinal"}, {"label": "C", "text": "Ordinal"}, {"label": "D", "text": "Nominal"}], "answerKey": "B", "model_answer": "SOLUTION: The price of a house is a cardinal number, which is a number that can be used to count. For example, the price of a house is a cardinal number."}
{"question": "If a Johansen \"trace\" test for a null hypothesis of 2 cointegrating vectors is applied to a system containing 4 variables is conducted, which eigenvalues would be used in the test?", "choices": [{"label": "A", "text": "All of them"}, {"label": "B", "text": "The largest 2"}, {"label": "C", "text": "The smallest 2"}, {"label": "D", "text": "The second largest"}], "answerKey": "C", "model_answer": "LECTURE: A Johansen test for cointegration tests for the presence of a linear combination of the specified variables that is stationary and has a unit root. The test uses the eigenvalues of the covariance matrix of the residuals.\nThe test statistic is based on the eigenvalues. Specifically, the test statistic is the sum of the squares of the eigenvalues. For small sample sizes, the test statistic may be sensitive to sample size effects. In such cases, the trace statistic is used. It is based on the same eigenvalues as the Johansen test statistic, but is not affected by sample size.\nThe null hypothesis of the Johansen test is that there are m cointegrating vectors. The alternative hypothesis is that there are m + 1 cointegrating vectors. The test can detect the presence of up to m + 1 cointegrating vectors.\nSOLUTION: A 2-vector test can detect up to 2 cointegrating vectors. The null hypothesis for a 2-vector test is that there are 2 cointegrating vectors. The alternative hypothesis is that there are 3 cointegrating vectors. The test can detect the presence of up to 3 vectors.\nThe largest 2 eigenvalues are the ones that would be used in the test. These eigenvalues correspond to the two largest roots of the characteristic equation. For a 2-vector test, the largest 2 eigenvalues are the ones that would be used in the test.\nThe smallest 2 eigenvalues are not the ones that would be used in the test. These eigenvalues correspond to the two smallest roots of the characteristic equation. For a 2-vector test, the smallest 2 eigenvalues are not the ones that would be used in the test.\nThe second largest eigenvalue is not the one that would be used in the test. This eigenvalue corresponds to the second largest root of the characteristic equation. For a 2-vector test, the second largest eigenvalue is not the one that would be used in the test."}
{"question": "Which of the following statements is true concerning variance decomposition analysis of VARs?\n\ni) Variance decompositions measure the impact of a unit shock to each of the variables on the VAR\n\nii) Variance decompositions can be thought of as measuring the proportion of the forecast error variance that is attributable to each variable\n\niii) The ordering of the variables is important for calculating impulse responses but not variance decompositions\n\niv) It is usual that most of the forecast error variance for a given variable is attributable to shocks to that variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: Variance decompositions can be thought of as measuring the proportion of the forecast error variance that is attributable to each variable. So, variance decompositions can be used to identify which variables have the greatest impact on the forecast error variance.\nA unit shock to a variable will change the value of that variable in the VAR model. Then, the forecast error variance for the VAR will change. So, a unit shock to a variable will change the variance decomposition for that variable.\nThe ordering of the variables is important for calculating impulse responses but not variance decompositions. This means that changing the order of the variables in the VAR will change the impulse responses, but it will not change the variance decompositions.\nMost of the forecast error variance for a given variable is attributable to shocks to that variable. This means that, in general, shocks to a variable will change the value of that variable in the VAR. So, shocks to a variable will change the variance decomposition for that variable."}
{"question": "Suppose that we have estimated a GARCH model for daily equity returns, and we are interested in producing a 10-day forecast of the volatility (measured by the standard deviation of returns) for use in a value at risk model. How could such a forecast most validly be calculated?", "choices": [{"label": "A", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up"}, {"label": "B", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up and take the square root"}, {"label": "C", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts, take the square roots of each one and add them up"}, {"label": "D", "text": "Produce a 1-step ahead conditional variance forecast, take its square root and multiply it by the square root of 10"}], "answerKey": "B", "model_answer": "LECTURE: A forecast of the volatility of equity returns can be produced by estimating a GARCH model on a dataset that includes daily returns. The GARCH model will identify the patterns in the data that are associated with future volatility.\nTo produce a forecast, the model can be used with the most recent observations to generate a set of conditional variance forecasts. For example, a 1-step ahead conditional variance forecast can be produced by using the most recent observation to calculate the variance that would be expected if the future followed the patterns in the data. Similarly, a 10-step ahead conditional variance forecast can be produced by using the most recent observation along with the next 9 observations to calculate the variance that would be expected if the future followed the patterns in the data.\nTo calculate the value at risk, a forecast of the volatility is needed. One valid approach is to take the square root of the 10-step ahead conditional variance forecast. This approach is consistent with the way that the value at risk is commonly calculated.\nSOLUTION: To calculate the value at risk, a forecast of the volatility is needed. One valid approach is to take the square root of the 10-step ahead conditional variance forecast. This approach is consistent with the way that the value at risk is commonly calculated.\nTo calculate the square root of the 10-step ahead conditional variance forecast, you need to produce the 10-step ahead conditional variance forecast and then take the square root of each forecast and add them up."}
{"question": "Suppose that the value of $R^2$ for an estimated regression model is exactly zero. Which of the following are true?\n\ni) All coefficient estimates on the slopes will be zero\n\nii) The fitted line will be horizontal with respect to all of the explanatory variables\n\niii) The regression line has not explained any of the variability of y about its mean value\n\niv) The intercept coefficient estimate must be zero.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: A value of $R^2$ equal to zero means that the regression model does not explain any of the variability of the dependent variable about its mean.\nSo, the regression line has not explained any of the variability of y about its mean value.\nThe intercept coefficient estimate must be zero.\nSince the intercept is estimated to be zero, the value of the intercept is zero. So, the regression line will have a y-intercept of zero.\nThe fitted line will be horizontal with respect to all of the explanatory variables.\nSince the intercept is zero, the regression line will have a y-intercept of zero. So, the regression line will be horizontal with respect to all of the explanatory variables.\nAll coefficient estimates on the slopes will be zero.\nSince the intercept is zero, the only coefficient estimate that can be nonzero is the slope coefficient estimate for the constant term.\nBut the slope coefficient estimate for the constant term is zero, so all coefficient estimates on the slopes will be zero."}
{"question": "A white noise process will have\n\n(i) A zero mean\n\n(ii) A constant variance\n\n(iii) Autocovariances that are constant\n\n(iv) Autocovariances that are zero except at lag zero", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: A white noise process has the following properties:\n(i) A zero mean.\n(ii) Autocovariances that are constant.\n(iii) A constant variance.\n(iv) Autocovariances that are zero except at lag zero.\nProperty (i) tells you that the mean of the white noise process is zero.\nProperties (ii) and (iii) tell you that the variance of the white noise process is constant.\nProperty (iv) tells you that the autocovariances of the white noise process are zero except at lag zero.\nBecause the mean of the white noise process is zero, any autocovariance is determined by the variance of the process. And because the variance of the white noise process is constant, any autocovariance is also constant.\nSo, a white noise process has constant autocovariances."}
{"question": "Which of the following statements are true concerning the Box-Jenkins approach to diagnostic testing for ARMA models?\n\n(i) The tests will show whether the identified model is either too large or too small\n\n(ii) The tests involve checking the model residuals for autocorrelation, heteroscedasticity, and non-normality\n\n(iii) If the model suggested at the identification stage is appropriate, the acf and pacf for the residuals should show no additional structure\n\n(iv) If the model suggested at the identification stage is appropriate, the coefficients on the additional variables under the overfitting approach will be statistically insignificant", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: The Box-Jenkins approach to diagnostic testing for ARMA models involves three main tests: the autocorrelation function (ACF) test, the partial autocorrelation function (PACF) test, and the heteroscedasticity test. These tests can show whether the model is properly specified.\nThe ACF and PACF can show whether the model is correctly specifying the autocorrelation structure of the errors. The ACF should show a gradual decay to zero, and the PACF should show a sharp decrease to zero.\nThe heteroscedasticity test can show whether the model is correctly specifying the error variance. The constant in the heteroscedasticity test can be compared to the standard deviation of the residuals to see if they are equal.\nIf the model is properly specified, the ACF and PACF for the residuals should show no additional structure. This means that the ACF and PACF should show a gradual decay to zero, and no sharp peaks.\nIf the model is overfitting the data, the coefficients on the additional variables will be statistically significant. This means that the additional variables will have a statistically significant p-value.\nIf the model is properly specified, the coefficients on the additional variables under the overfitting approach will be statistically insignificant. This means that the additional variables will have a statistically insignificant p-value."}
{"question": "Which one of the following would be a plausible response to a finding of residual non-normality?", "choices": [{"label": "A", "text": "Use a logarithmic functional form instead of a linear one"}, {"label": "B", "text": "Add lags of the variables on the right hand side of the regression model"}, {"label": "C", "text": "Estimate the model in first differenced form"}, {"label": "D", "text": "Remove any large outliers from the data."}], "answerKey": "D", "model_answer": "LECTURE: Regression models are based on the assumption that the dependent variable is normally distributed. This assumption can be tested with a residual plot.\nThe normality assumption is important because it determines the distribution of the errors in the model. If the errors are not normally distributed, the model's assumptions are violated, and the model's results may be difficult to interpret.\nOne way to address this issue is to use a non-linear functional form. A non-linear form may help to transform the variables in the model into a normal distribution. For example, a logarithmic functional form could be used instead of a linear one.\nAdding lags of the variables on the right hand side of the regression model can also help to address non-normality. This is because lagged values of the variables may help to transform the variables into a normal distribution.\nEstimating the model in first differenced form can also help to address non-normality. This is because differencing can help to transform a non-stationary time series into a stationary one.\nRemoving any large outliers from the data can also help to address non-normality. Outliers can have a large effect on the residuals, and their removal can help to transform the variables into a normal distribution."}
{"question": "The fixed effects panel model is also sometimes known as", "choices": [{"label": "A", "text": "A seemingly unrelated regression model"}, {"label": "B", "text": "The least squares dummy variables approach"}, {"label": "C", "text": "The random effects model"}, {"label": "D", "text": "Heteroscedasticity and autocorrelation consistent"}], "answerKey": "B", "model_answer": "SOLUTION: A fixed effects panel model is a type of random effects model. In a random effects model, the errors are allowed to be heteroscedastic and autocorrelated. This is different from a seemingly unrelated regression model, which has homoscedastic and non-autocorrelated errors.\nThe fixed effects panel model is sometimes called the least squares dummy variables approach. This is because the fixed effects model is similar to the least squares dummy variables approach, which is a way to include fixed effects in a regression."}
{"question": "Which of the following statements is TRUE concerning OLS estimation?", "choices": [{"label": "A", "text": "OLS minimises the sum of the vertical distances from the points to the line"}, {"label": "B", "text": "OLS minimises the sum of the squares of the vertical distances from the points to the line"}, {"label": "C", "text": "OLS minimises the sum of the horizontal distances from the points to the line"}, {"label": "D", "text": "OLS minimises the sum of the squares of the horizontal distances from the points to the line."}], "answerKey": "B", "model_answer": "LECTURE: In simple linear regression, the objective is to fit a line to the data that minimizes the sum of the squares of the vertical distances from the points to the line. This is called the least-squares estimate.\nThe line that minimizes the sum of the squares of the vertical distances from the points to the line is the ordinary least squares (OLS) line. The OLS line is the line that minimizes the sum of the squares of the vertical distances from the points to the line.\nSOLUTION: To minimize the sum of the squares of the vertical distances from the points to the line, the OLS line is chosen. The OLS line is the line that minimizes the sum of the squares of the vertical distances from the points to the line."}
{"question": "If the standard tools for time-series analysis, such as estimation of the acf, pacf and spectral analysis, find no evidence of structure in the data, this implies that the data are which of the following?", "choices": [{"label": "A", "text": "Normally distributed"}, {"label": "B", "text": "Uncorrelated"}, {"label": "C", "text": "Independent"}, {"label": "D", "text": "Fat-tailed"}], "answerKey": "B", "model_answer": "SOLUTION: The absence of evidence of structure in the data does not mean that the data are normally distributed. The absence of evidence of structure can also mean that the data are uncorrelated and independent. So, the data are uncorrelated and independent."}
{"question": "If two variables, $x_t$ and $y_t$ are said to be cointegrated, which of the following statements are true?\n\ni) $x_t$ and $y_t$ must both be stationary\n\n\nii) Only one linear combination of $x_t$ and $y_t$ will be stationary\n\n\niii) The cointegrating equation for $x_t$ and $y_t$ describes the short-run relationship\n\nbetween the two series\n\n\niv) The residuals of a regression of $y_t$ on $x_t$ must be stationary", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: To determine if a variable is stationary, you need to check whether its mean, variance, and covariance are all constant. If they are, the variable is stationary.\nA cointegrated pair is made up of two non-stationary variables whose difference is stationary. So, the two variables are not individually stationary, but their difference is.\nThe cointegrating equation relates the two variables and describes the short-run relationship between them. It may also describe their long-run relationship, but the long-run relationship is not the same as the short-run relationship.\nAfter they are differenced, the residuals from a regression of one variable on the other will be stationary. This is one way to test for cointegration."}
{"question": "A dependent variable whose values are not observable outside a certain range but where the corresponding values of the independent variables are still available would be most accurately described as what kind of variable?", "choices": [{"label": "A", "text": "Censored"}, {"label": "B", "text": "Truncated"}, {"label": "C", "text": "Multinomial variable"}, {"label": "D", "text": "Discrete choice"}], "answerKey": "A", "model_answer": "LECTURE: Variables can have different types of distributions, or ways in which their values cluster. Some common types of distributions include the normal distribution, the binary distribution, and the multinomial distribution.\nDependent variables are usually written in terms of independent variables. So, for a dependent variable to be accurately described, you need to know the distribution of the independent variables.\nA censored variable is a variable whose values are not observable below a certain point. This point is called a censoring point. The censoring point is often set to the smallest observable value of the variable. For example, if a variable cannot go below 0, then any values below 0 are considered censored.\nA truncated variable is a variable whose values are not observable above a certain point. This point is called a truncation point. The truncation point is often set to the largest observable value of the variable. For example, if a variable cannot go above 20, then any values above 20 are considered truncated.\nA multinomial variable has a distribution that can include negative as well as positive values. The term \"multinomial\" refers to the fact that the variable can take on a multitude of values. For example, a multinomial variable might have a distribution that looks like a histogram with three bars.\nA discrete choice variable has a distribution that includes only integer values. The term \"discrete\" refers to the fact that the variable can only take on integer values. For example, a discrete choice variable might have a distribution that looks like a histogram with evenly spaced bars.\nSOLUTION: To describe a variable, you need to know the values it can take and the probabilities of those values. A censored variable is not described by its entire distribution. A censored variable is described by its entire distribution below the censoring point. So, a censored variable is not a discrete choice variable.\nA truncated variable is not described by its entire distribution. A truncated variable is described by its entire distribution above the truncation point. So, a truncated variable is not a discrete choice variable.\nA multinomial variable is not described by its entire distribution. A multinormal variable is described by its entire distribution. So, a multinomial variable is not a discrete choice variable.\nA discrete choice variable is described by its entire distribution. So, a discrete choice variable is a discrete variable."}
{"question": "A Hausman test would be used for", "choices": [{"label": "A", "text": "Determining whether an equation that is part of a simultaneous system is identified"}, {"label": "B", "text": "Determining whether a simultaneous framework is needed for a particular variable"}, {"label": "C", "text": "Determining whether 2SLS or ILS is optimal"}, {"label": "D", "text": "Determining whether the structural form equations can be obtained via substitution from the reduced forms"}], "answerKey": "B", "model_answer": "SOLUTION: The Hausman test can be used to determine whether 2SLS or ILS is optimal. The test can also be used to determine whether the structural form equations can be obtained via substitution from the reduced forms."}
{"question": "Under the matrix notation for the classical linear regression model, $y = X \\beta + u$, what are the dimensions of $u$?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "B", "model_answer": "LECTURE: The matrix notation for the classical linear regression model is as follows:\nThe output is a vector of scalar values, and the predictor is a matrix of scalar values. This matrix has T rows and k columns. The output is a vector of length k.\n\nThe dimension of a matrix is a property of the matrix. The dimension of a matrix is called its order. The order of a matrix is denoted by its row and column dimensions.\nFor a matrix A, the order of A is written as A:k x r, which means that A is a matrix of k rows and r columns.\nFor a matrix B, the order of B is written as B:r x k, which means that B is a matrix of r columns and k rows.\nSOLUTION: The predictor is a matrix of size T x k. So, the output is a vector of size k."}
{"question": "How many parameters will be required to be estimated in total for all equations of a standard form, unrestricted, tri-variate VAR(4), ignoring the intercepts?", "choices": [{"label": "A", "text": "12"}, {"label": "B", "text": "4"}, {"label": "C", "text": "3"}, {"label": "D", "text": "36"}], "answerKey": "D", "model_answer": ""}
{"question": "A researcher tests for structural stability in the following regression model:\n\n$y_t = \\beta_1 + \\beta_2 x_{2t} + \\beta_3 x_{3t} + u_t$\n\nThe total sample of 200 observations is split exactly in half for the sub-sample regressions. Which would be the unrestricted residual sum of squares?", "choices": [{"label": "A", "text": "The RSS for the whole sample"}, {"label": "B", "text": "The RSS for the first sub-sample"}, {"label": "C", "text": "The RSS for the second sub-sample"}, {"label": "D", "text": "The sum of the RSS for the first and second sub-samples"}], "answerKey": "D", "model_answer": ""}
{"question": "Suppose that we are interested in testing the null hypothesis that a GARCH(2,2) model can be restricted to a process with a constant conditional variance using the likelihood ratio test approach. Which of the following statements are true?", "choices": [{"label": "A", "text": "The test statistic will follow a chi-squared distribution with 2 degrees of freedom under the null hypothesis"}, {"label": "B", "text": "The value of the log-likelihood function will almost always be bigger for the restricted model than for the unrestricted model"}, {"label": "C", "text": "If the relevant values of the log-likelihood functions are -112.3 and -118.4, the value of the test statistic is 12.2"}, {"label": "D", "text": "The likelihood ratio test compares the slopes of the log-likelihood function at the maximum and at the restricted parameter value."}], "answerKey": "C", "model_answer": ""}
{"question": "Which one of the following is NOT a plausible remedy for near multicollinearity?", "choices": [{"label": "A", "text": "Use principal components analysis"}, {"label": "B", "text": "Drop one of the collinear variables"}, {"label": "C", "text": "Use a longer run of data"}, {"label": "D", "text": "Take logarithms of each of the variables"}], "answerKey": "D", "model_answer": "LECTURE: Near multicollinearity occurs when two or more independent variables are highly correlated. In this case, the usual technique of ordinary least squares is not used. Instead, the Generalized Lecture: LeCTURE: When examining the predictive power of a model, one of the things you can look at is the R-squared value. This is a measure of how well the independent variables in the model predict the dependent variable.\nR-squared is a statistic that ranges from 0 to 1. An R-squared value of 0 means that the independent variables in the model do not predict the dependent variable. An R-squared value of 1 means that the independent variables in the model perfectly predict the dependent variable.\nSOLUTION: Near multicollinearity occurs when two or more independent variables are highly correlated. In this case, the usual technique of ordinary least squares is not used. Instead, the following methods can be used to fit the linear equation:\n* Use principal components analysis to reduce the data to a set of uncorrelated variables.\n* Drop one of the collinear variables.\n* Use a longer run of data.\n* Take logarithms of each of the variables.\nPrincipal components analysis is a method for transforming the variables in a set of data to uncorrelated variables. This can help to mitigate multicollinearity.\nWhen using principal components analysis, you can trade off the amount of variance explained for the amount of correlation in the variables. This can help you to find a set of uncorrelated variables that together explain the maximum amount of variance in the data."}
{"question": "Consider the following AR(2) process:\n\nyt = 1.5 yt-1 - 0.5 yt-2 + ut\n\nThis is a", "choices": [{"label": "A", "text": "Stationary process"}, {"label": "B", "text": "Unit root process"}, {"label": "C", "text": "Explosive process"}, {"label": "D", "text": "Stationary and unit root process"}], "answerKey": "B", "model_answer": "LECTURE: A stationary process is a time series whose statistical properties are constant over time.\nA unit root process is a time series that has a unit root. A unit root is a root of the polynomial xt = a + brx, where a and b are constants, and r is the root such that |r| \u2264 1.\nAn explosive process is a time series whose statistical properties change suddenly and dramatically over a short period of time.\nA stationary and unit root process is a time series that is both a stationary process and a unit root process.\nSOLUTION: An AR(2) process has the form\nyt = 1.5yt-1 - 0.5yt-2 + ut\nwhere u is a stationary process with zero mean.\nTo decide whether the process is stationary, look at the mean and variance of yt.\nThe mean of yt is 0.\nThe variance of yt is 1.5.\nThese are constant, so the process is stationary."}
{"question": "Which of the following could be used as a test for autocorrelation up to third order?", "choices": [{"label": "A", "text": "The Durbin Watson test"}, {"label": "B", "text": "White's test"}, {"label": "C", "text": "The RESET test"}, {"label": "D", "text": "The Breusch-Godfrey test"}], "answerKey": "D", "model_answer": "LECTURE: A test for autocorrelation can help you determine if a variable's current value is related to its previous values. Autocorrelation can be of different orders, or distances. For example, a variable's current value could be related to its value from one step ago (first-order autocorrelation), two steps ago (second-order autocorrelation), or three steps ago (third-order autocorrelation).\nTests for autocorrelation can be used to determine if a variable has a specific type of autocorrelation. The Durbin Watson test can be used to determine if a time series has first-order autocorrelation. White's test can be used to determine if a time series has first-order autocorrelation. The RESET test can be used to determine if a time series has first-order autocorrelation. The Breusch-Godfrey test can be used to determine if a time series has no autocorrelation.\nSOLUTION: To determine if a test can be used to detect autocorrelation up to third order, look at the specific types of autocorrelation the test can detect. The Durbin Watson test and White's test can be used to determine if a time series has first-order autocorrelation. These tests cannot be used to determine if a time series has third-order autocorrelation. The RESET test can be used to determine if a time series has first-order autocorrelation. This test cannot be used to determine if a time series has third-order autocorrelation. The Breusch-Godfrey test can be used to determine if a time series has no autocorrelation. This test can be used to determine if a time series has third-order autocorrelation."}
{"question": "The residual from a standard regression model is defined as", "choices": [{"label": "A", "text": "The difference between the actual value, y, and the mean, y-bar"}, {"label": "B", "text": "The difference between the fitted value, y-hat, and the mean, y-bar"}, {"label": "C", "text": "The difference between the actual value, y, and the fitted value, y-hat"}, {"label": "D", "text": "The square of the difference between the fitted value, y-hat, and the mean, y-bar"}], "answerKey": "C", "model_answer": "SOLUTION: The residual is the difference between the actual value and the predicted value.\nThe actual value is y, and the predicted value is y-hat. So, the residual is the difference between y and y-hat.\nThis is the definition of the residual."}
{"question": "If OLS is applied separately to each equation that is part of a simultaneous system, the resulting estimates will be", "choices": [{"label": "A", "text": "Unbiased and consistent"}, {"label": "B", "text": "Biased but consistent"}, {"label": "C", "text": "Biased and inconsistent"}, {"label": "D", "text": "It is impossible to apply OLS to equations that are part of a simultaneous system"}], "answerKey": "C", "model_answer": "LECTURE: When you have a simultaneous system of equations, you can use the ordinary least squares (OLS) method to estimate the parameters in each equation. But when you do this, the estimates for the parameters in each equation will be biased. This means that the estimates will systematically underestimate or overestimate the true values of the parameters.\nIf OLS is applied separately to each equation that is part of a simultaneous system, the resulting estimates will be biased but consistent. This means that the estimates will be systematically off, but they will be consistent with each other.\nIf OLS is applied to a single equation that is part of a simultaneous system, the resulting estimates will be unbiased and consistent. This means that the estimates will be accurate, and they will be consistent with each other.\nIf OLS is applied to a single equation that is not part of a simultaneous system, the resulting estimates will be unbiased and consistent. This means that the estimates will be accurate, and they will be consistent with each other.\nIt is impossible to apply OLS to equations that are part of a simultaneous system. This means that you cannot use the OLS method to estimate the parameters in a simultaneous system.\nSOLUTION: When you apply OLS to a simultaneous system, the estimates for the parameters in each equation will be biased. So, the estimates will be biased but consistent."}
{"question": "Which one of the following is NOT an example of mis-specification of functional form?", "choices": [{"label": "A", "text": "Using a linear specification when y scales as a function of the squares of x"}, {"label": "B", "text": "Using a linear specification when a double-logarithmic model would be more appropriate"}, {"label": "C", "text": "Modelling y as a function of x when in fact it scales as a function of 1/x"}, {"label": "D", "text": "Excluding a relevant variable from a linear regression model"}], "answerKey": "D", "model_answer": "LECTURE: Functional form refers to the way that a dependent variable is written in terms of its independent variables.\nOne of the key decisions in creating a model is choosing the functional form. The functional form may include exchangeable variables, which are dummy variables that take the value 1 for one group and 0 for another group. For example, in a model about the effect of a policy change, a dummy variable could be included to indicate whether a state raised its drinking age.\nHere are some examples of common functional forms:\nlinear: y = b0 + b1x\ndouble logarithmic: y = b0 + b1\\log(x) + b2\\log(x)^2\nlogistic: y = e^(-b1 - b2\\log(x))\nExponential: y = b0 \\* exp(b1 \\* x)\nUsing a linear specification when y scales as a function of the squares of x.\nA linear specification is appropriate when the independent variable has a constant effect. The functional form y = b0 + b1x has a constant effect.\nIf y scales as a function of the squares of x, it means that x has a squared effect on y. The functional form y = b0 + b1x^2 has a squared effect.\nUsing a linear specification when a double-logarithmic model would be more appropriate.\nA double-logarithmic model has a log-log effect. The functional form y = b0 + b1\\log(x) + b2\\log(x)^2 has a log-log effect.\nLog-log models are appropriate when the independent variable has a significant effect on the logged-dependent variable.\nModelling y as a function of x when in fact it scales as a function of 1/x.\nIf y scales as a function of 1/x, it means that x has an inverse effect on y. The functional form y = b0 + b1/x has an inverse effect.\nExcluding a relevant variable from a linear regression model.\nExcluding a relevant variable from a linear regression model is a form of specification error. A linear regression model includes all of the significant variables."}
{"question": "Suppose that we wished to evaluate the factors that affected the probability that an investor would choose an equity fund rather than a bond fund or a cash investment. Which class of model would be most appropriate?", "choices": [{"label": "A", "text": "A logit model"}, {"label": "B", "text": "A multinomial logit"}, {"label": "C", "text": "A tobit model"}, {"label": "D", "text": "An ordered logit model"}], "answerKey": "B", "model_answer": "LECTURE: Logit is an abbreviation for logistic function. A logit model is a regression model that uses a logistic function to describe the probability that a binary outcome will occur. The logit model is a special case of a generalized linear model.\nThe logit model is used to predict the probability that a binary choice will be made. The dependent variable in a logit model is the logit of the probability that the binary choice will be made.\nOne useful thing about a logit model is that the probability predicted by the model can be interpreted as the probability that the binary choice will be made. This can be useful in situations where a certain decision needs to be made, and the decision maker needs to choose between alternatives.\nA multinomial logit model is a type of logit model. It is used to predict the probability that a choice among several alternatives will be made.\nA tobit model is a type of regression model. It is used to predict a response that must be positive. The tobit model is useful for modeling limited dependent variables.\nAn ordered logit model is a type of logit model. It is used to predict the probability that a choice among several alternatives will be made.\nSOLUTION: An ordered logit model is a type of logit model. It is used to predict the probability that a choice among several alternatives will be made.\nA multinomial logit model is a type of logit model. It is used to predict the probability that a choice among several alternatives will be made.\nA tobit model is a type of regression model. It is used to predict a response that must be positive. The tobit model is useful for modeling limited dependent variables.\nA logit model is a regression model that uses a logistic function to describe the probability that a binary outcome will occur. The logit model is used to predict the probability that a binary choice will be made."}
{"question": "Which of the following statements will be true if the number of replications used in a Monte Carlo study is small?\n\ni) The statistic of interest may be estimated imprecisely\n\n\nii) The results may be affected by unrepresentative combinations of random draws\n\n\niii) The standard errors on the estimated quantities may be unacceptably large\n\n\niv) Variance reduction techniques can be used to reduce the standard errors", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "LECTURE: A Monte Carlo study uses random sampling to estimate a quantity of interest. The number of replications used in a study is called the sample size.\nThe sample size affects the accuracy and precision of the estimates.\nA small sample size may result in an estimate that is imprecise or affected by unrepresentative combinations of random draws. This may lead to an estimate that is far from the true value of the quantity of interest.\nMoreover, a small sample size may also result in unacceptably large standard errors. This means that the estimates may be subject to large fluctuations from one draw to the next.\nVariance reduction techniques can be used to reduce the standard errors. These techniques reduce the variability of the random calculations. They do this by using more calculations for each replication.\nUsing variance reduction techniques may allow for a significant reduction in the sample size required to achieve a given level of precision. This means that a smaller number of replications can be used to estimate a quantity of interest, while still obtaining accurate and precise results.\nSOLUTION: A small sample size may result in imprecise estimates and unacceptably large standard errors. Variance reduction techniques can help to reduce the standard errors.\nTherefore, statements ii and iv are correct.\nA small sample size may also result in estimates that are imprecise. This may lead to an estimate that is far from the true value of the quantity of interest.\nStatement i is correct.\nMoreover, a small sample size may also result in unacceptably large standard errors. This means that the estimates may be subject to large fluctuations from one draw to the next.\nStatement iii is correct."}
{"question": "Which of the following is a disadvantage of the random effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "B", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "C", "text": "The random effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}, {"label": "D", "text": "All of (a) to (c) are potential disadvantages of the random effects approach."}], "answerKey": "A", "model_answer": "SOLUTION: The random effects approach can capture cross-sectional heterogeneity and temporal variation in the dependent variable. It can also account for correlation in the error term. So, the potential disadvantages of the random effects approach are that the number of parameters to estimate may be large, and the approach may not be valid if the composite error term is correlated with one or more of the explanatory variables."}
{"question": "Which of the following could result in autocorrelated residuals?\n\ni) Slowness of response of the dependent variable to changes in the values of the independent variables\n\nii) Over-reactions of the dependent variable to changes in the independent variables\n\niii) Omission of relevant explanatory variables that are autocorrelated\n\niv) Outliers in the data", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "LECTURE: Autocorrelation is a type of correlation that occurs within a time series. It can result from slowness of response of the dependent variable to changes in the values of the independent variables, from omitted relevant explanatory variables that are autocorrelated, or from outliers in the data.\nOmitted relevant explanatory variables that are autocorrelated can result in autocorrelated residuals. This type of autocorrelation occurs when two or more variables are strongly related to each other. In a regression model, these variables can cause the residuals to be autocorrelated.\nSOLUTION: Outliers in the data can also result in autocorrelated residuals. This is because outliers can cause the model to have a higher than expected sum of squared residuals. This can result in the model being forced to fit the data with a slower response than is actually present in the data. This slower response can lead to autocorrelated residuals.\nSlowness of response of the dependent variable to changes in the values of the independent variables can also result in autocorrelated residuals. This is because the model may underestimate the effect of the independent variables and may cause the residuals to be autocorrelated.\nOver-reactions of the dependent variable to changes in the independent variables can also result in autocorrelated residuals. This is because the model may overestimate the effect of the independent variables and may cause the residuals to be autocorrelated."}
{"question": "Which of the following statements are true concerning the acf and pacf?\n\n(i) The acf and pacf are often hard to interpret in practice\n\n(ii) The acf and pacf can be difficult to calculate for some data sets\n\n(iii) Information criteria represent an alternative approach to model order determination\n\n(iv) If applied correctly, the acf and pacf will always deliver unique model selections", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: The autocorrelation function (acf) and partial autocorrelation function (pacf) are tools for model order determination, not for interpreting model results. The acf and pacf can be difficult to calculate for some data sets, but they can be calculated. Information criteria represent an alternative approach to model order determination. If applied correctly, the acf and pacf will always deliver unique model selections."}
{"question": "Which of the following conditions are necessary for a series to be classifiable as a weakly stationary process?\n\n(i) It must have a constant mean\n\n(ii) It must have a constant variance\n\n(iii) It must have constant autocovariances for given lags\n\n(iv) It must have a constant probability distribution", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: A weakly stationary process has the following properties:\nIt must have a constant mean.\nIt must have a constant probability distribution.\nIt must have constant autocovariances for given lags.\nIt does not have a constant variance.\nThe property of a constant variance is not necessary for a process to be weakly stationary. However, the following properties are: a constant mean, a constant probability distribution, and constant autocovariances."}
{"question": "Consider the following equation and determine the class of model that it best represents\n\n$y_{it} = \\alpha + \\beta_{it} + \\mu_i + \\nu_{it}$", "choices": [{"label": "A", "text": "An entity fixed effects model"}, {"label": "B", "text": "A time fixed effects model"}, {"label": "C", "text": "A random effects model"}, {"label": "D", "text": "A pure time series model"}], "answerKey": "A", "model_answer": "LECTURE: In economics, researchers often use data that are clustered or correlated within groups. For example, if a researcher wanted to study the effect of a particular policy on wages, they might want to compare workers at different firms to each other. In this case, workers at each firm would form a group. Within each group, the wages of workers would be correlated.\nThere are several types of models that can be used to analyze data that are clustered or correlated within groups. One way to describe these models is in terms of the type of variables that are fixed across groups and the type of variables that are variable across groups.\nIn an entity fixed effects model, the units of observation are entities, and the variables that are fixed across groups are the entity-specific variables. The variables that are variable across groups are the ordinary variables.\nIn a time fixed effects model, the units of observation are time periods, and the variables that are fixed across groups are the ordinary variables. The variables that are variable across groups are the entity-specific variables.\nIn a random effects model, both the entity-specific variables and the ordinary variables are allowed to vary across groups randomly.\nIn a pure time series model, the variables are allowed to depend only on the order of observation within a group.\nThere is a great deal of discussion among economists about the appropriate use of each type of model. One helpful way to think about this discussion is in terms of the questions that each type of model can answer.\nAn entity fixed effects model can answer questions about the effect of entity-specific variables.\nA time fixed effects model can answer questions about the effect of ordinary variables.\nA random effects model can answer questions about the effect of both entity-specific variables and ordinary variables.\nA pure time series model can answer questions about how variables change over time."}
{"question": "Note that statistical tables are not necessary to answer this question. For a sample of 1000 observations, the Dickey-Fuller test statistic values are", "choices": [{"label": "A", "text": "More negative than (i.e. bigger in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "B", "text": "Less negative than (i.e. smaller in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "C", "text": "Obtained from an analytical formula for the density of the Dickey-Fuller distribution"}, {"label": "D", "text": "More negative (i.e. bigger in absolute value) for a 10% size of test than a 5% test."}], "answerKey": "A", "model_answer": "SOLUTION: The Dickey-Fuller test statistic is a measure of the difference between the observed and hypothetical serial correlation in a time series. The test statistic has a distribution that depends on the value of the lag and the sample size.\nFor a sample of 1000 observations, the Dickey-Fuller test statistic values are less negative (i.e. smaller in absolute value) than those in the left hand tail of a normal distribution with the same mean and standard deviation as the distribution of the test statistic."}
{"question": "Suppose that a hypothesis test is conducted using a 5% significance level. Which of the following statements are correct?\n\n(i) The significance level is equal to the size of the test\n\n(ii) The significance level is equal to the power of the test\n\n(iii) 2.5% of the total distribution will be in each tail rejection region for a 2-sided test\n\n(iv) 5% of the total distribution will be in each tail rejection region for a 2-sided test.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The significance level is a measure of the probability of rejecting the null hypothesis when it is true. The size of a test is a measure of the proportion of the population that is sampled.\nThe power of a test is a measure of the probability of rejecting the null hypothesis when the null hypothesis is false.\nBoth of these quantities are measures of the effectiveness of a test in detecting a effect. They are related to the significance level, but they are not the same as the significance level.\nThe regions in a distribution that are outside of the null hypothesis value represent the tail areas. The smaller the significance level, the smaller the tail areas. For a 2-sided test, the smaller the tail areas, the more powerful the test.\nFor a 2-sided test, the significance level is equal to the size of the test.\nThe size of a test and the power of a test are equal to the significance level.\nFor a 2-sided test, 5% of the total distribution will be in each tail rejection region.\nThis means that the significance level is 5%."}
{"question": "Which one of the following criticisms of the Dickey-Fuller/Engle-Granger approach to dealing with cointegrated variables is overcome by the Engle-Yoo (EY) procedure?", "choices": [{"label": "A", "text": "In the context of small samples, Dickey Fuller tests are prone to conclude that there is a unit root in a series when there is not"}, {"label": "B", "text": "The Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one"}, {"label": "C", "text": "The variables are treated asymmetrically in the cointegrating tests"}, {"label": "D", "text": "It is not possible to perform tests about the cointegrating relationship"}], "answerKey": "D", "model_answer": "SOLUTION: The Engle-Yoo (EY) procedure can overcome the criticism that the Dickey-Fuller/Engle-Granger approach is prone to conclude that there is a unit root in a series when there is not. The EY procedure uses a different method to detect cointegration. It does not rely on testing for the presence of a unit root.\nThe EY procedure can also overcome the criticism that the variables are treated asymmetrically in the cointegrating tests. The EY procedure tests for cointegration in a way that is consistent with the assumptions of the cointegrating relationship. It does not require that the variables be stationary.\nThe EY procedure can also overcome the criticism that the variables are not normally distributed. The EY procedure uses a different method to estimate the parameters of the cointegrating relationship. It does not require that the variables be normally distributed.\nThe EY procedure can also overcome the criticism that the Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one. The EY procedure can detect all of the cointegrating relationships that exist among the variables.\nIt is not possible to perform tests about the cointegrating relationship without specifying the form of the cointegrating relationship. So, the EY procedure can overcome the criticism that it is not possible to perform tests about the cointegrating relationship."}
{"question": "Consider a series that follows an MA(1) with zero mean and a moving average coefficient of 0.4. What is the value of the autocovariance at lag 1?", "choices": [{"label": "A", "text": "0.4"}, {"label": "B", "text": "1"}, {"label": "C", "text": "0.34"}, {"label": "D", "text": "It is not possible to determine the value of the autocovariances without knowing the disturbance variance."}], "answerKey": "D", "model_answer": "SOLUTION: An MA(1) series has the following property:\nIt has a autocovariance at lag 1.\nThe autocovariance at lag 1 tells you the correlation of the series with itself one step ago. A common mistake is to assume that this correlation is also the correlation with itself at other lags. This is incorrect.\nThe autocovariance at lag 1 is given by the product of the moving average coefficient and the variance of the disturbance:\nIt is not possible to determine the value of the autocovariances without knowing the disturbance variance.\nThe disturbance variance is given by the inverse of the autocovariance at lag 1. So, you need to know the autocovariance at lag 1 to determine the disturbance variance. You also need the disturbance variance to determine the value of the autocovariance at other lags.\nSo, it is not possible to determine the value of the autocovariance at lag 1 without knowing the disturbance variance."}
{"question": "Which of the following estimation techniques are available for the estimation of over-identified systems of simultaneous equations?\n\ni) OLS\n\nii) ILS\n\niii) 2SLS\n\niv) IV", "choices": [{"label": "A", "text": "(iii) only"}, {"label": "B", "text": "(iii) and (iv) only"}, {"label": "C", "text": "(ii), (iii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii) and (iv)"}], "answerKey": "B", "model_answer": "LECTURE: In an over-identified system of simultaneous equations, the number of unknowns is greater than the number of equations. In this case, leCTURE: In an over-identified system of simultaneous equations, the number of unknowns is greater than the number of equations. In this case, there are usually many ways to solve for the unknowns.\nOne way to solve for the unknowns is to use an iterative algorithm, which is an algorithm that has a loop. One popular iterative algorithm is the least squares algorithm, which is used in the ordinary least squares (OLS) method. The OLS method is an estimation technique that uses the least squares criterion to estimate the parameters of a linear regression model.\nAnother way to solve for the unknowns is to use instrumental variables. An instrumental variable is a variable that affects the dependent variable but does not directly influence it. The use of instrumental variables to solve for the unknowns in a system of simultaneous equations is called instrumental variables (IV) estimation.\nThe two methods described above can be used to estimate the parameters of an over-identified system of simultaneous equations. The method that is used depends on the nature of the variables in the system.\nIf the variables in the system are in a linear relation to each other, then the OLS method is used to estimate the parameters. If the variables in the system are not in a linear relation to each other, then the IV method is used.\nSOLUTION: The IV method can be used to estimate the parameters of an over-identified system of simultaneous equations. This is shown in the top part of the table.\nThe OLS method can also be used to estimate the parameters of an over-identified system of simultaneous equations. This is shown in the bottom part of the table."}
{"question": "Which one of the following statements best describes a Type II error?", "choices": [{"label": "A", "text": "It is the probability of incorrectly rejecting the null hypothesis"}, {"label": "B", "text": "It is equivalent to the power of the test"}, {"label": "C", "text": "It is equivalent to the size of the test"}, {"label": "D", "text": "It is the probability of failing to reject a null hypothesis that was wrong"}], "answerKey": "D", "model_answer": ""}
{"question": "Which one of the following would be the most appropriate as a 95% (two-sided) confidence interval for the intercept term of the model given in question 21?", "choices": [{"label": "A", "text": "(-4.79,2.19)"}, {"label": "B", "text": "(-4.16,4.16)"}, {"label": "C", "text": "(-1.98,1.98)"}, {"label": "D", "text": "(-5.46,2.86)"}], "answerKey": "D", "model_answer": "LECTURE: A confidence interval is a range of values. It is used to estimate the uncertainty in a sample statistic. For example, the sample mean is a statistic. A confidence interval for the sample mean can be used to estimate the uncertainty in the mean.\nA 95% confidence interval for the intercept term of a model means that 95% of the time, the confidence interval will contain the true intercept term.\nThe formula for a confidence interval is:\nconfidence interval = sample statistic \u00b1 margin of error\nThe margin of error is a measure of the uncertainty in the sample statistic. It is calculated by taking the sample size and dividing it by the square root of the sample size.\nFor example, the margin of error for a 95% confidence interval based on a sample of 45 is:\nmargin of error = margin of error value = sample size / sqrt(sample size) = 45 / 5.304 = 0.082\nSo, for a 95% confidence interval, the margin of error is 0.082.\nTo calculate the confidence interval, use the formula:\nconfidence interval = 2.19 \u00b1 0.082\nThe confidence interval includes the entire range of possible values for the intercept term. So, all of the following would be appropriate:\n* LECTURE: A confidence interval is a range of values. It is used to estimate the uncertainty in a sample statistic. For example, the sample mean is a statistic. A confidence interval for the sample mean can be used to estimate the uncertainty in the mean.\nA 95% confidence interval for the intercept term of a model means that 95% of the time, the confidence interval will contain the true intercept term.\nThe formula for a confidence interval is:\nconfidence interval = sample statistic \u00b1 margin of error\nThe margin of error is a measure of the uncertainty in the sample statistic. It is calculated by taking the sample size and dividing it by the square root of the sample size.\nFor example, the margin of error for a 95% confidence interval based on a sample of 45 is:\nmargin of error = margin of error value = sample size / sqrt(sample size) = 45 / 5.304 = 0.082\nSo, for a 95% confidence interval, the margin of error is 0.082.\nTo calculate the confidence interval, use the formula:\nconfidence interval = 2.19 \u00b1 0.082"}
{"question": "Which of the following are characteristics of a stationary process?\n\ni) It crosses its mean value frequently\n\n\nii) It has constant mean and variance\n\n\niii) It contains no trend component\n\n\niv) It will be stationary in first difference form", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": ""}
{"question": "Consider again the VAR model of equation 16. Which of the following conditions must hold for it to be said that there is bi-directional feedback?", "choices": [{"label": "A", "text": "The b and d coefficients significant and the a and c coefficients insignificant"}, {"label": "B", "text": "The a and c coefficients significant and the b and d coefficients insignificant"}, {"label": "C", "text": "The a and c coefficients significant"}, {"label": "D", "text": "The b and d coefficients significant"}], "answerKey": "D", "model_answer": "LECTURE: In a VAR model, there are multiple equations. Each equation in the model contains multiple variables. For example, the model might contain equations for how each variable changes over time. An impulse in one variable is a change in that variable. So, an impulse in one equation is a change in multiple variables.\nFeedback is a type of impulse that happens when the output of a variable is fed back into that variable. Feedback can cause the output to affect the input.\nThere are two types of feedback: uni-directional and bi-directional. Uni-directional feedback happens when the output of a variable is fed back into that variable, but the input is not fed back into the output. So, the output has an effect on the output, but the input does not have an effect on the output.\nBi-directional feedback happens when the output of a variable is fed back into that variable, and the input is also fed back into the output. So, the output has an effect on the input, and the input has an effect on the output.\nSOLUTION: To determine which conditions must hold for a VAR model to have bi-directional feedback, consider what bi-directional feedback is. An impulse in one variable is fed back into that variable. So, the output of a variable is fed back into the output equation. And, the input is fed back into the input equation. This happens in the b and d equations. So, the b and d coefficients must be significant. The a and c coefficients must be insignificant."}
{"question": "Consider the following sample autocorrelation estimates obtained using 250 data points:\n\n1) Lag 1 2 3\n\n2) Coefficient 0.2 -0.15 -0.1\n\n3) Assuming that the coefficients are approximately normally distributed, which of the coefficients are statistically significant at the 5% level?", "choices": [{"label": "A", "text": "1 only"}, {"label": "B", "text": "1 and 2 only"}, {"label": "C", "text": "1, 2 and 3 only"}, {"label": "D", "text": "It is not possible to determine the statistical significance since no standard errors have been given"}], "answerKey": "B", "model_answer": "SOLUTION: The test of statistical significance for autocorrelation uses the t-test, which requires that the coefficients be normally distributed. The sample coefficients are given as 0.2, -0.15, and -0.1.\nFirst, standardize the coefficients by subtracting the mean and dividing by the standard deviation:\nStandardized coefficients:\n0.2 -0.15 -0.1\nThen, use the t-test to determine which coefficients are statistically significant at the 5% level:\nFor a 2-tail test, the critical values for the t-test are -1.98 and 1.98.\nThe coefficients are normally distributed, and all three of them have absolute values greater than the critical values. So, all three coefficients are statistically significant at the 5% level."}
{"question": "Which one of the following is examined by looking at a goodness of fit statistic?", "choices": [{"label": "A", "text": "How well the population regression function fits the data"}, {"label": "B", "text": "How well the sample regression function fits the population regression function"}, {"label": "C", "text": "How well the sample regression function fits the data"}, {"label": "D", "text": "How well the population regression function fits the sample regression function."}], "answerKey": "C", "model_answer": "LECTURE: A goodness of fit statistic can help you determine how well a function fits your data.\nThere are many different goodness of fit statistics, such as the mean squared error, which tells you the average squared difference between the function's predictions and the actual values.\nSOLUTION: A goodness of fit statistic can examine how well a function fits the data. This is achieved by looking at how well the function's predictions match the actual values.\nThe population regression function is defined by the population data. Examining how well the population regression function fits the data requires looking at the data from the population. So, how well the population regression function fits the data is examined by looking at a goodness of fit statistic.\nThe sample regression function is defined by the sample data. Examining how well the sample regression function fits the population regression function requires looking at the data from the population. So, how well the sample regression function fits the population regression function is not examined by looking at a goodness of fit statistic.\nExamining how well the sample regression function fits the data requires looking at a goodness of fit statistic. So, how well the sample regression function fits the data is examined by looking at a goodness of fit statistic.\nExamining how well the population regression function fits the sample regression function requires looking at a goodness of fit statistic. So, how well the population regression function fits the sample regression function is examined by looking at a goodness of fit statistic."}
{"question": "Which of the following statements are correct concerning the use of antithetic variates as part of a Monte Carlo experiment?\n\ni) Antithetic variates work by reducing the number of replications required to cover the whole probability space\n\nii) Antithetic variates involve employing a similar variable to that used in the simulation, but whose properties are known analytically\n\niii) Antithetic variates involve using the negative of each of the random draws and repeating the experiment using those values as the draws\n\niv) Antithetic variates involve taking one over each of the random draws and repeating the experiment using those values as the draws", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: Look at the statements:\nI. Antithetic variates work by reducing the number of replications required to cover the whole probability space.\nII. Antithetic variates involve employing a similar variable to that used in the simulation, but whose properties are known analytically.\nIII. Antithetic variates involve using the negative of each of the random draws and repeating the experiment using those values as the draws.\nIV. Antithetic variates involve taking one over each of the random draws and repeating the experiment using those values as the draws.\nI and III are correct.\nII and IV are incorrect.\nI. is correct.\nII. is incorrect.\nAn antithetic variable is a variable whose properties are known analytically. This property is used to generate antithetic variates. But II. is incorrect. An antithetic variable is not a similar variable to the one used in the simulation.\nIV. is correct.\nTaking the negative of each random draw and repeating the experiment using those values as the draws is one way to use antithetic variates. This method is called the \"opposite approach.\""}
{"question": "Which one of the following statements is true concerning alternative forecast accuracy measures?", "choices": [{"label": "A", "text": "Mean squared error is usually highly correlated with trading rule profitability"}, {"label": "B", "text": "Mean absolute error provides a quadratic loss function"}, {"label": "C", "text": "Mean absolute percentage error is a useful measure for evaluating asset return forecasts"}, {"label": "D", "text": "Mean squared error penalises large forecast errors disproportionately more than small forecast errors"}], "answerKey": "D", "model_answer": "SOLUTION: Mean squared error penalises large forecast errors disproportionately more than small forecast errors.\nThis means that when evaluating alternative forecast accuracy measures, mean squared error is a more accurate measure of the quality of the forecasts."}
{"question": "Which of the following criticisms of standard (\"plain vanilla\") GARCH models can be overcome by EGARCH models?\n\ni) Estimated coefficient values from GARCH models may be negative\n\nii) GARCH models cannot account for leverage effects\n\niii) The responsiveness of future volatility to positive and negative shocks is symmetric under a GARCH formulation\n\niv) GARCH models cannot allow for a feedback from the volatility to the returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: Look at the criticisms.\nEstimated coefficient values from GARCH models may be negative.\nGARCH models cannot account for leverage effects.\nThe responsiveness of future volatility to positive and negative shocks is symmetric under a GARCH formulation.\nGARCH models cannot allow for a feedback from the volatility to the returns.\nOnly EGARCH models can account for leCTURE: A GARCH model is a type of statistical model that is used to estimate the volatility of a financial asset. These models have several criticisms, or limitations.\nOne criticism is that the estimated coefficient values from GARCH models may be negative. This means that the models do not capture all of the movements in the volatility of the financial asset.\n Another criticism is that GARCH models cannot account for leCTURE: LeCTURE: A feedback loop is a mechanism that allows a change in one part of a system to affect the change in another part.\nGARCH models do not allow for a feedback from the volatility to the returns. This means that the models do not consider how the volatility of a financial asset affects the returns from the asset.\nOne example of a feedback loop is when the volatility of a financial asset increases, the returns from the asset decrease. This decrease in returns can then lead to a decrease in the volatility of the asset."}
{"question": "Suppose that 100 separate firms were tested to determine how many of them \"beat the market\" using a Jensen-type regression, and it is found that 3 fund managers significantly do so. Does this suggest prima facie evidence for stock market inefficiency?", "choices": [{"label": "A", "text": "Yes"}, {"label": "B", "text": "No"}, {"label": "C", "text": "In order to answer this question, you would need to test every fund manager trading in that market"}, {"label": "D", "text": "There is insufficient information given in the question to draw a conclusion about market efficiency."}], "answerKey": "B", "model_answer": "SOLUTION: The fact that three fund managers out of 100 significantly beat the market suggests that market efficiency is not absolute. If the market were entirely efficient, no fund manager would be able to consistently beat it.\nHowever, the fact that some fund managers do beat the market does not necessarily prove that the market is inefficient. It is possible that these fund managers were able to find undervalued stocks using information that was not available to other investors.\nIn order to determine whether this evidence proves that the market is inefficient, you would need to test every fund manager trading in the market. If a significant portion of the fund managers consistently beat the market, it would suggest that the market is inefficient."}
{"question": "Which of the following are advantages of the use of panel data over pure cross-sectional or pure time-series modelling?\n\n(i) The use of panel data can increase the number of degrees of freedom and therefore the power of tests\n\n(ii) The use of panel data allows the average value of the dependent variable to vary either cross-sectionally or over time or both\n\n(iii) The use of panel data enables the researcher allows the estimated relationship between the independent and dependent variables to vary either cross-sectionally or over time or both", "choices": [{"label": "A", "text": "(i) only"}, {"label": "B", "text": "(i) and (ii) only"}, {"label": "C", "text": "(ii) only"}, {"label": "D", "text": "(i), (ii), and (iii)"}], "answerKey": "B", "model_answer": "SOLUTION: Panel data have the advantages of both cross-sectional and time-series data. So, the following are advantages of the use of panel data over pure cross-sectional or pure time-series modelling:\nThe use of panel data allows the average value of the dependent variable to vary either cross-sectionally or over time or both.\nThe use of panel data enables the researcher to estimate the relationship between the independent and dependent variables at different points in time.\nThe use of panel data can increase the number of degrees of freedom and therefore the power of tests.\nThe researcher can use the estimated relationship between the independent and dependent variables to make predictions about future values of the dependent variable."}
{"question": "If the Engle-Granger test is applied to the residuals of a potentially cointegrating regression, what would be the interpretation of the null hypothesis?", "choices": [{"label": "A", "text": "The variables are cointegrated"}, {"label": "B", "text": "The variables are not cointegrated"}, {"label": "C", "text": "Both variables are stationary"}, {"label": "D", "text": "Both variables are non-stationary"}], "answerKey": "B", "model_answer": "LECTURE: A cointegrating regression is a regression in which two non-stationary time series become stationary when regressed on each other. This happens because the regression \"binds\" the two series together, making them behave as a stationary series.\nThe Engle-Granger test can be used to test for cointegration. The test involves regressing the first series on the second series to see if they are stationary together. One of the criteria for cointegration is that the residuals of this regression are stationary.\nSo, the null hypothesis in the Engle-Granger test is that the residuals are not stationary.\nIf the null hypothesis is rejected, it means that the residuals are stationary, which means that the two variables are cointegrated.\nIf the null hypothesis is not rejected, it means that the residuals are not stationary, which means that the two variables are not cointegrated.\nThe Engle-Granger test can only identify that the two series are cointegrated. It cannot identify which series leads or lags the other.\n\nSOLUTION: The null hypothesis in the Engle-Granger test is that the residuals are not stationary. If the null hypothesis is rejected, it means that the residuals are stationary, which means that the two variables are cointegrated."}
{"question": "Which of the following statements are true concerning the autocorrelation function (acf) and partial autocorrelation function (pacf)?\n\ni) The acf and pacf will always be identical at lag one whatever the model\n\nii) The pacf for an MA(q) model will in general be non-zero beyond lag q\n\niii) The pacf for an AR(p) model will be zero beyond lag p\n\niv) The acf and pacf will be the same at lag two for an MA(1) model", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": ""}
