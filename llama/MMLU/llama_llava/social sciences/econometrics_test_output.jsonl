{"question": "Which one of the following is the most appropriate definition of a 99% confidence interval?", "choices": [{"label": "A", "text": "99% of the time in repeated samples, the interval would contain the true value of the parameter"}, {"label": "B", "text": "99% of the time in repeated samples, the interval would contain the estimated value of the parameter"}, {"label": "C", "text": "99% of the time in repeated samples, the null hypothesis will be rejected"}, {"label": "D", "text": "99% of the time in repeated samples, the null hypothesis will not be rejected when it was false"}], "answerKey": "A", "model_answer": "The correct answer is (A). A 99% confidence interval is a range of values that is estimated from a sample of data, and it contains the true value of the parameter 99% of the time in repeated samples. This means that if the true value of the parameter is within the confidence interval, it is expected that the interval will contain it 99 times out of 100."}
{"question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?", "choices": [{"label": "A", "text": "ADF is a single equation approach to unit root testing while PP is a systems approach"}, {"label": "B", "text": "PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test"}, {"label": "C", "text": "The PP test incorporates an automatic correction for autocorrelated residuals in the test regression"}, {"label": "D", "text": "PP tests have good power in small samples whereas DF tests do not."}], "answerKey": "C", "model_answer": "The correct answer is D. PP tests have good power in small samples whereas DF tests do not.\nThe Dickey-Fuller (DF) test and the Phillips-Perron (PP) test are both commonly used tests for determining the presence of a unit root in a time series. The DF test tests the null hypothesis of a unit root against the alternative hypothesis of a non-stationary time series, while the PP test tests the null of a stationary time series against the alternative of a unit root.\nOne key difference between the two tests is that the PP test has good power in small samples, while the DF test does not. This means that the PP test is more likely to detect a unit root in a time series even if the sample size is small, while the DF test may not be able to detect a unit root with sufficient confidence.\nAnother key difference is that the PP test incorporates an automatic correction for autocorrelated residuals in the test regression, which can improve the power of the test. The PP test also tests the null of a stationary time series against the alternative of a unit root, whereas the DF test tests the null of a stationary time"}
{"question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?", "choices": [{"label": "A", "text": "It would rise more quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "B", "text": "It would be symmetrical about zero"}, {"label": "C", "text": "It would rise less quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "D", "text": "It would be zero for all positive disturbances"}], "answerKey": "A", "model_answer": "The correct answer is (A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude.\nThis is because a leverage effect would lead to a greater impact on the system when it is pushed in a negative direction than when it is pushed in a positive direction. This is because negative disturbances are more likely to have unintended consequences and lead to further negative effects, while positive disturbances may have fewer negative consequences and can lead to positive effects.\nFor example, if a company is leveraged and there is a negative shock to the economy, it may have trouble paying back its debts, leading to negative consequences such as bankruptcy or default. On the other hand, if the company is leveraged and there is a positive shock to the economy, it may be able to pay back its debts more easily and have positive consequences such as increased profits.\nIt's worth noting that a leverage effect may not always be present, and the impact curve may be symmetrical about zero, but in general, a leverage effect would lead to a greater impact on the system when it is pushed in a negative direction than when it is pushed in a positive direction."}
{"question": "Which of the following statements is false concerning the linear probability model?", "choices": [{"label": "A", "text": "There is nothing in the model to ensure that the estimated probabilities lie between zero and one"}, {"label": "B", "text": "Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one"}, {"label": "C", "text": "The error terms will be heteroscedastic and not normally distributed"}, {"label": "D", "text": "The model is much harder to estimate than a standard regression model with a continuous dependent variable"}], "answerKey": "D", "model_answer": "The correct answer is (B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one.\n\nThis statement is false because the linear probability model is a special case of the logistic regression model, which does not truncate the probabilities at zero and one. Instead, it models the probabilities as a function of the predictors, allowing for values between zero and one."}
{"question": "Which of the following statements concerning the regression population and sample is FALSE?", "choices": [{"label": "A", "text": "The population is the total collection of all items of interest"}, {"label": "B", "text": "The population can be infinite"}, {"label": "C", "text": "In theory, the sample could be larger than the population"}, {"label": "D", "text": "A random sample is one where each individual item from the population is equally likely to be drawn."}], "answerKey": "C", "model_answer": "The correct answer is (D) A random sample is one where each individual item from the population is equally likely to be drawn.\nThis is because a random sample is a group of items selected from a larger population of items using a random process, such as random selection or random assignment, to ensure that every item in the population has an equal chance of being included in the sample. In contrast, the population is the total collection of all items of interest, and it can be infinite or finite. The population can be defined in various ways, depending on the context and the research question being asked.\nIt's important to note that a random sample is not necessarily the same as a representative sample. A representative sample is a group of items that is representative of the population, meaning that it reflects the characteristics of the population as a whole. A random sample, on the other hand, is simply a group of items that are selected randomly from the population."}
{"question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?", "choices": [{"label": "A", "text": "Asymptotically, the values of the two test statistics will be equal"}, {"label": "B", "text": "The Q test has better small-sample properties than the Q*"}, {"label": "C", "text": "The Q test is sometimes over-sized for small samples"}, {"label": "D", "text": "As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients."}], "answerKey": "B", "model_answer": "The correct answer is (D).\nIn a time series, the Box-Pierce Q and Ljung-Box Q* statistics are used to test the null hypothesis of zero autocorrelation coefficients. The Q test is based on the maximum absolute deviation from the null hypothesis, while the Q* test is based on the average of the squared residuals from the null hypothesis. Both tests aim to test for the linear dependence of a time series.\nThe correct statement about the comparison of the two test statistics is that as the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis. This means that the power of the test increases as the sample size increases, making it more likely to reject the null hypothesis. This is because as the sample size increases, the probability of observing a time series with random noise decreases, leading to more false positives. However, the values of the Q and Q* statistics may not always be equal, and the Q test may have better small-sample properties than the Q* test. The statement (A) is incorrect because the values of the two test statistics might not be equal even in the long run."}
{"question": "A parsimonious model is one that", "choices": [{"label": "A", "text": "Includes too many variables"}, {"label": "B", "text": "Includes as few variables as possible to explain the data"}, {"label": "C", "text": "Is a well-specified model"}, {"label": "D", "text": "Is a mis-specified model"}], "answerKey": "A", "model_answer": "The correct answer is (A) Includes too many variables.\nA parsimonious model is one that includes as few variables as possible to explain the data. This means that a parsimonious model includes the minimum number of variables needed to accurately predict the outcome. A model that includes too many variables is not parsimonious, and is not considered to be a well-specified model.\nOn the other hand, a well-specified model is a model that accurately represents the relationship between the variables and the data. A well-specified model includes all relevant variables and is not overly complex. A model that is not well-specified is called a mis-specified model, and it can be inaccurate or give incorrect results.\nTherefore, the correct answer is (A) Includes too many variables."}
{"question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?", "choices": [{"label": "A", "text": "They can be interpreted as continuously compounded changes in the prices"}, {"label": "B", "text": "They can be added over time to give returns for longer time periods"}, {"label": "C", "text": "They can be added across a portfolio of assets to give portfolio returns"}, {"label": "D", "text": "They are usually fat-tailed"}], "answerKey": "C", "model_answer": "The correct answer is D) They are usually fat-tailed. Continuously compounded returns (i.e. log-returns) can be interpreted as continuously compounded changes in the prices, added over time to give returns for longer time periods, and added across a portfolio of assets to give portfolio returns. However, they are typically fat-tailed, which means that they have a higher probability of extreme returns (e.g. large gains or losses) than normally distributed returns. This means that the returns for a given investment over a given time period are not normally distributed, and instead follow a power-law distribution."}
{"question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nThe GARCH (Generalized Autoregressive Conditional Heteroskedasticity) model can capture the features of financial asset return time-series, including fat tails, leverage effects, volatility clustering, and volatility affecting returns.\n\nFat tails in the return distribution refer to the fact that financial asset returns can have extreme values that do not follow a normal distribution. The GARCH model can capture this feature by including a volatility process in the model that can have heavy-tailed distributions, such as the t-distribution or the skew-normal distribution.\n\nLeverage effects refer to the fact that financial asset returns can be more volatile when the market is more leveraged. The GARCH model can capture this feature by including a conditional mean process in the model that can depend on the level of market leverage.\n\nVolatility clustering refers to the fact that financial asset returns can exhibit periods of high volatility followed by periods of low volatility. The GARCH model can capture this feature by including a conditional variance process in the model"}
{"question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?", "choices": [{"label": "A", "text": "Less than -1"}, {"label": "B", "text": "Between -1 and 0"}, {"label": "C", "text": "Between 0 and 1"}, {"label": "D", "text": "Bigger than 1"}], "answerKey": "C", "model_answer": "The correct answer is (A) Less than -1.\nThe GARCH-in-mean parameter is used to capture the conditional variance of the error term. It is expected to be negative because a negative value indicates that the conditional variance of the error term is smaller than its average, which means that the errors are less volatile than their average. In contrast, a positive value indicates that the conditional variance of the error term is larger than its average, which means that the errors are more volatile than their average.\nSince we are estimating a GARCH-M model with daily corporate bond percentage returns as the time-series data, it is reasonable to assume that the errors are less volatile than their average over the long-run, which is why the value of the GARCH-in-mean parameter is expected to be less than -1."}
{"question": "Under which of the following situations would bootstrapping be preferred to pure simulation?\n\ni) If it is desired that the distributional properties of the data in the experiment\n\nare the same as those of some actual data\n\n\nii) If it is desired that the distributional properties of the data in the experiment\n\nare known exactly\n\n\niii) If the distributional properties of the actual data are unknown\n\n\niv) If the sample of actual data available is very small", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "Correct answer is D: (i), (ii), (iii), and (iv).\n\nBootstrapping is a technique that can be used in situations where the distributional properties of the data in an experiment are unknown or need to be estimated. It involves resampling the data from the original sample and using the resampled data to estimate the distributional properties of the population from which the sample was drawn.\n\nIn situation (i), it is desired that the distributional properties of the data in the experiment are the same as those of some actual data. In this case, bootstrapping can be useful as it can be used to estimate the distributional properties of the actual data.\n\nIn situation (ii), it is desired that the distributional properties of the data in the experiment are known exactly. In this case, bootstrapping may not be the best option as it involves estimating the distributional properties of the population from the sample data.\n\nIn situation (iii), the distributional properties of the actual data are unknown. In this case, bootstrapping can be useful as it can be used to estimate the distributional properties of the population from the sample data.\n\nIn situation (iv), the sample of actual data available is very"}
{"question": "Which of the following may be consequences of one or more of the CLRM assumptions being violated?\n\ni) The coefficient estimates are not optimal\n\n\nii) The standard error estimates are not optimal\n\n\niii) The distributions assumed for the test statistics are inappropriate\n\n\niv) Conclusions regarding the strength of relationships between the dependent\n\nand independent variables may be invalid.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv). All of the given options are possible consequences of at least one of the CLRM assumptions being violated, so any combination of them could be correct. The CLRM assumptions include:\n\n* Coefficients are optimal: The model assumptions about the level of the independent variables and the relationships between them are valid.\n* Standard error estimates are optimal: The standard error estimates are based on the assumption that the residuals are normally distributed and that the variance of the residuals is constant over all observations.\n* Distributions of test statistics are appropriate: The distribution of the test statistic is based on the assumption that the residuals are normally distributed and that the variance of the residuals is constant over all observations.\n* Invalid conclusions about the strength of relationships: The conclusions drawn from the analysis of the data are based on the assumption that the model assumptions are valid. If any of these assumptions are violated, the conclusions may be invalid."}
{"question": "Which of the following statements is true concerning forecasting in econometrics?", "choices": [{"label": "A", "text": "Forecasts can only be made for time-series data"}, {"label": "B", "text": "Mis-specified models are certain to produce inaccurate forecasts"}, {"label": "C", "text": "Structural forecasts are simpler to produce than those from time series models"}, {"label": "D", "text": "In-sample forecasting ability is a poor test of model adequacy"}], "answerKey": "D", "model_answer": "B) Mis-specified models are certain to produce inaccurate forecasts."}
{"question": "The pacf is necessary for distinguishing between", "choices": [{"label": "A", "text": "An AR and an MA model"}, {"label": "B", "text": "An AR and an ARMA model"}, {"label": "C", "text": "An MA and an ARMA model"}, {"label": "D", "text": "Different models from within the ARMA family"}], "answerKey": "B", "model_answer": "C. An MA and an ARMA model.\nIn statistical modeling, it is important to be able to distinguish between different models to determine which one best fits the data. The pacf (partial autocorrelation function) is a useful tool for distinguishing between models, as it can help to identify patterns in the data that may indicate the presence of certain types of structure, such as trend or seasonality. The pacf can be used to distinguish between autoregressive (AR) and autoregressive moving average (ARMA) models, which are two common types of time series models.\nIf the pacf indicates the presence of a trend or seasonality, then an AR model is more appropriate. If the pacf indicates the presence of a trend or seasonality with a period longer than one, then an ARMA model is more appropriate.\nIt is also important to note that different models within the ARMA family, such as ARMA(p,q), can be distinguished based on the pacf as well."}
{"question": "Negative residual autocorrelation is indicated by which one of the following?", "choices": [{"label": "A", "text": "A cyclical pattern in the residuals"}, {"label": "B", "text": "An alternating pattern in the residuals"}, {"label": "C", "text": "A complete randomness in the residuals"}, {"label": "D", "text": "Residuals that are all close to zero"}], "answerKey": "B", "model_answer": "The correct answer is (B) An alternating pattern in the residuals.\nExplanation:\nAn alternating pattern in the residuals would indicate a cyclical pattern in the data, which is associated with negative residual autocorrelation. This means that the residuals are not random and have a tendency to repeat a specific pattern over time.\n\nOn the other hand, a complete randomness in the residuals would indicate that the residuals are all close to zero, which would not indicate negative residual autocorrelation."}
{"question": "Which of the following statements are true concerning a comparison between ARCH(q) and GARCH(1,1) models?\n\ni) The ARCH(q) model is likely to be the more parsimonious\n\n\nii) The ARCH(q) model is the more likely to violate non-negativity constraints\n\n\niii) The ARCH(q) model can allow for an infinite number of previous lags of squared\n\nreturns to affect the current conditional variance\n\n\niv) The GARCH(1,1) model will usually be sufficient to capture all of the dependence\n\nin the conditional variance", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\ni) The ARCH(q) model is likely to be the more parsimonious.\nii) The ARCH(q) model is the more likely to violate non-negativity constraints.\niii) The ARCH(q) model can allow for an infinite number of previous lags of squared returns to affect the current conditional variance.\niv) The GARCH(1,1) model will usually be sufficient to capture all of the dependence in the conditional variance.\n\nThese statements are all true concerning a comparison between ARCH(q) and GARCH(1,1) models. The ARCH(q) model is a more parsimonious model than the GARCH(1,1) model, and it is less likely to violate non-negativity constraints. The ARCH(q) model can allow for an infinite number of previous lags of squared returns to affect the current conditional variance, while the GARCH(1,1) model is only able to capture one lag of squared returns. Finally, the GARCH(1,1) model is"}
{"question": "Which of the following statements are true concerning a triangular or recursive system?\n\ni) The parameters can be validly estimated using separate applications of OLS to\n\neach equation\n\n\nii) The independent variables may be correlated with the error terms in other\n\nequations\n\n\niii) An application of 2SLS would lead to unbiased but inefficient parameter estimates\n\n\niv) The independent variables may be correlated with the error terms in the equations\n\nin which they appear as independent variables", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv) only, as all of the statements are true concerning a triangular or recursive system.\n\nA triangular or recursive system is one in which the independent variables are correlated with the error terms in other equations. In this case, it is possible to use 2SLS (2-stage least squares) to obtain unbiased but inefficient parameter estimates. The 2SLS estimator is an extension of the ordinary least squares (OLS) method to the triangular system case.\n\nIt is also true that the independent variables may be correlated with the error terms in the equations in which they appear as independent variables. This means that the errors in the independent variables are not independent and can affect the estimates of the parameters in the other equations.\n\nFinally, it is true that the parameters can be validly estimated using separate applications of OLS to each equation in the system, but this may not be optimal if the independent variables are correlated with the error terms in other equations."}
{"question": "Which one of the following statements best describes the algebraic representation of the fitted regression line?", "choices": [{"label": "A", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}, {"label": "B", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t"}, {"label": "C", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + u_t"}, {"label": "D", "text": "y_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}], "answerKey": "B", "model_answer": "The correct answer is (A) \\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x\\_t + \\hat{u}_t.\nChoice (B) is incorrect because it excludes the error term (u\\_t) from the regression equation. Choice (C) is incorrect because it includes both the independent variable (x\\_t) and the error term (u\\_t) in the regression equation. Choice (D) is incorrect because it includes both the independent variable (x\\_t) and the error term (u\\_t) in the regression equation and also includes the intercept (y\\_t) from the data.\nChoice (A) is the correct answer because it represents the fitted regression line as a linear equation that can be used to predict future values of the dependent variable (y\\_t) based on the independent variable (x\\_t). The equation is based on the best-fitting values of the coefficients (\u03b1 and \u03b2) obtained from the least squares regression method."}
{"question": "What are the dimensions of $\\hat{u}^t \\hat{u}?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "D", "model_answer": "The correct answer is (D) 1 x 1.\nThe dimensions of a matrix can be determined by its size, or the number of rows and columns. The size of a matrix is usually given by the number of rows (m) and the number of columns (n).\nIn this case, the matrix $\\hat{u}^t \\hat{u}$ has a size of $T \\times 1$, where $T$ is the number of time steps. The rows in this matrix are the time steps, and the columns are a single element, which is the vector of time-varying coefficients $\\hat{u}$. The dimensions of this matrix are thus 1 x 1 (one element in one dimension).\nTherefore, the correct answer is (D) 1 x 1."}
{"question": "The characteristic roots of the MA process\n\n$y_t = -3u_{t-1} + u_{t-2} + u_t$\n\nare", "choices": [{"label": "A", "text": "1 and 2"}, {"label": "B", "text": "1 and 0.5"}, {"label": "C", "text": "2 and -0.5"}, {"label": "D", "text": "1 and -3"}], "answerKey": "B", "model_answer": "The correct answer is (B) 1 and 0.5.\n\nThe characteristic roots of the MA process can be calculated using the characteristic equation $y' = ky$, where $y = [u_t u_{t-1}]^T$ and $k = [-3 1]$. The characteristic roots are the eigenvalues of the matrix $A = (k - I)$, where $I$ is the identity matrix. The roots of the characteristic equation are given by the formula $\\lambda = e^{\\pm \\operatorname{ln} \\lambda}$, where $\\operatorname{ln}$ is the natural logarithm.\n\nThe characteristic roots of the MA process are given by the roots of the characteristic equation $y' = ky$, which is $e^{-3} \\lambda^2 + e^2 \\lambda + e$. The roots of this equation are $\\lambda = e^2 \\pm \\sqrt{e^4 - 3e^2 + 1}$. These roots are given by $\\lambda = \\sqrt{3} \\pm \\sqrt{2}$, which are the roots of the characteristic equation.\n\n(A) 1 and 2 is not correct because the characteristic roots are complex numbers.\n\n(C)"}
{"question": "Which of the following is an equivalent expression for saying that the explanatory variable is \"non-stochastic\"?", "choices": [{"label": "A", "text": "The explanatory variable is partly random"}, {"label": "B", "text": "The explanatory variable is fixed in repeated samples"}, {"label": "C", "text": "The explanatory variable is correlated with the errors"}, {"label": "D", "text": "The explanatory variable always has a value of one"}], "answerKey": "B", "model_answer": "The correct answer is (A).\nChoice (A) is the most accurate and specific equivalent expression for saying that the explanatory variable is \"non-stochastic.\" \"Stochastic\" generally refers to a variable that is subject to random variation, so a variable that is partly random would be considered stochastic. \"Fixed in repeated samples\" and \"correlated with the errors\" are not accurate or specific expressions for saying that a variable is non-stochastic. Choice (D) is incorrect because a variable that always has a value of one would not be considered non-stochastic, as it would be fixed and not subject to random variation."}
{"question": "Suppose that the Durbin Watson test is applied to a regression containing two explanatory variables plus a constant with 50 data points. The test statistic takes a value of 1.53. What is the appropriate conclusion?", "choices": [{"label": "A", "text": "Residuals appear to be positively autocorrelated"}, {"label": "B", "text": "Residuals appear to be negatively autocorrelated"}, {"label": "C", "text": "Residuals appear not to be autocorrelated"}, {"label": "D", "text": "The test result is inconclusive"}], "answerKey": "D", "model_answer": "The correct answer is (A) Residuals appear to be positively autocorrelated.\nThe Durbin Watson test is used to test for autocorrelation in residuals in a regression model. When the test statistic is greater than 2, it suggests that there is a positive autocorrelation in the residuals, which can be due to a lack of fit in the model. When the test statistic is less than 2, it suggests that there is a negative autocorrelation in the residuals, which can be due to overfitting the model.\nIn the case of a Durbin Watson test with a test statistic of 1.53, the answer choice (A) Residuals appear to be positively autocorrelated is the most appropriate. This is because a test statistic greater than 2 indicates that there is a positive autocorrelation in the residuals, which is the opposite of what we would expect from the null hypothesis of no autocorrelation."}
{"question": "If OLS is used in the presence of autocorrelation, which of the following will be likely consequences?\n\ni) Coefficient estimates may be misleading\n\n\nii) Hypothesis tests could reach the wrong conclusions\n\n\niii) Forecasts made from the model could be biased\n\n\niv) Standard errors may inappropriate", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nOLS is a type of regression analysis that assumes that the errors are uncorrelated and have a normal distribution. In the presence of autocorrelation, this assumption is violated and the standard errors and coefficient estimates may be inappropriate. This can lead to misleading coefficient estimates, incorrect hypothesis tests, biased forecasts, and inappropriate standard errors. Therefore, all of the options are likely consequences of using OLS in the presence of autocorrelation."}
{"question": "What will be the properties of the OLS estimator in the presence of multicollinearity?", "choices": [{"label": "A", "text": "It will be consistent, unbiased and efficient"}, {"label": "B", "text": "It will be consistent and unbiased but not efficient"}, {"label": "C", "text": "It will be consistent but not unbiased"}, {"label": "D", "text": "It will not be consistent"}], "answerKey": "A", "model_answer": "The correct answer is (A). The OLS estimator is consistent, unbiased, and efficient in the presence of multicollinearity.\nMulticollinearity occurs when two or more independent variables are highly correlated with each other. When this happens, the standard errors of the OLS estimators will be inflated, which can lead to inconsistent estimates. However, even in the presence of multicollinearity, the OLS estimator is still consistent, which means that it will tend to converge to the true population parameters in the long run. Additionally, the OLS estimator is unbiased, which means that it will have a zero expected value. Finally, the OLS estimator is also efficient, which means that it will have the smallest possible variance among all possible estimates of the same parameters, given the model.\nSo, even in the presence of multicollinearity, the OLS estimator will still be consistent, unbiased, and efficient."}
{"question": "Which one of the following would NOT be a consequence of using non-stationary data in levels form?", "choices": [{"label": "A", "text": "The regression $R^2$ may be spuriously high"}, {"label": "B", "text": "Test statistics may not follow standard distributions"}, {"label": "C", "text": "Statistical inferences may be invalid"}, {"label": "D", "text": "Parameter estimates may be biased"}], "answerKey": "D", "model_answer": "The correct answer is (C) Statistical inferences may be invalid.\nA spurious high $R^2$ value does not necessarily mean that the regression is not stationary. The $R^2$ value measures the proportion of variation in the dependent variable that is explained by the independent variables. A high $R^2$ value may indicate that the variables are highly correlated and that the regression captures a significant amount of the variation in the dependent variable. However, a high $R^2$ value does not necessarily mean that the regression is stationary.\nTest statistics not following standard distributions can be a consequence of non-stationarity, but it is not a direct consequence of using non-stationary data in levels form.\nInvalid statistical inferences can occur in both stationary and non-stationary data, but it is not a direct consequence of using non-stationary data in levels form.\nParameter estimates being biased is a direct consequence of non-stationarity, but it is not a consequence of using non-stationary data in levels form."}
{"question": "If a series, y, follows a random walk, what is the optimal one-step ahead forecast of y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of y over the in-sample period"}], "answerKey": "A", "model_answer": "The correct answer is (B) Zero.\nThe optimal one-step ahead forecast of y is given by:\nF(y) = E[y] + (y - E[y]) \u03a3(y) / \u03a3(w)\nwhere E[y] is the expected value of y, and \u03a3(y) and \u03a3(w) are the sums of squared deviations from the mean.\nThe optimal forecast is the current value of y plus the expected value of y, adjusted by the sum of squared deviations from the mean, divided by the sum of squared deviations from the mean of the entire in-sample period.\nIn simple terms, the optimal forecast is the current value of y plus an adjustment for the uncertainty of the future values of y.\nSo, option (A) The current value of y is not the optimal forecast, as it does not take into account the uncertainty of the future values of y.\nOption (C) One is not the optimal forecast either, as it would mean that the forecaster believes that the future values of y will be exactly equal to the current value of y.\nOption (D) The average value of y over"}
{"question": "The order condition is", "choices": [{"label": "A", "text": "A necessary and sufficient condition for identification"}, {"label": "B", "text": "A necessary but not sufficient condition for identification"}, {"label": "C", "text": "A sufficient but not necessary condition for identification"}, {"label": "D", "text": "A condition that is nether necessary nor sufficient for identification"}], "answerKey": "B", "model_answer": "The correct answer is (B) A necessary but not sufficient condition for identification.\nA necessary condition is a condition that must be met in order for a phenomenon to occur. A sufficient condition is a condition that, when met, guarantees that a phenomenon will occur. In other words, a necessary condition is a prerequisite for something to happen, while a sufficient condition is a guarantee that it will happen.\nIn the context of identification, a necessary condition is a condition that must be present in order to identify a phenomenon. Since there may be other conditions that are also present, a necessary condition is not sufficient on its own to identify the phenomenon. A sufficient condition, on the other hand, would be a condition that, when met, guarantees that the phenomenon has been identified.\nTherefore, the correct answer is (B) A necessary but not sufficient condition for identification."}
{"question": "If an estimator is said to have minimum variance, which of the following statements is NOT implied?", "choices": [{"label": "A", "text": "The probability that the estimate is a long way away from its true value is minimised"}, {"label": "B", "text": "The estimator is efficient"}, {"label": "C", "text": "Such an estimator would be termed \"best\""}, {"label": "D", "text": "Such an estimator will always be unbiased"}], "answerKey": "D", "model_answer": "The correct answer is (A). The statement \"The probability that the estimate is a long way away from its true value is minimized\" implies that the estimator has minimum variance. Minimum variance means that the variance of the estimator is the smallest among all possible unbiased estimators. This implies that the estimator is efficient, and that it is likely to be close to the true value. However, the statement \"Such an estimator would be termed 'best' \" is not implied by the fact that the estimator has minimum variance. The term \"best\" is subjective and can depend on the specific context and criteria used to evaluate estimators. The statement \"Such an estimator will always be unbiased\" is also not implied by the fact that the estimator has minimum variance. The fact that an estimator has minimum variance does not guarantee that it is always unbiased."}
{"question": "Which of the following are disadvantages of the Dickey-Fuller / Engle-Granger approach to testing for cointegration and modelling cointegrating relationships?\n\ni) Only one cointegrating relationship can be estimated\n\nii) Particularly for small samples. There is a high chance of the tests suggestingthat variables are not cointegrated when they are\n\niii) It is not possible to make inferences on the cointegrating regression\n\niv) The procedure forces the researcher to specify which is the dependent variable and which are the independent variables.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nThe Dickey-Fuller test and the Engle-Granger test are methods for testing for cointegration and establishing the existence of cointegrating relationships between two or more time series. The tests involve estimating the long-run relationship between the variables and testing for the presence of a unit root. The disadvantages of the Dickey-Fuller and Engle-Granger approaches are:\n\ni) Only one cointegrating relationship can be estimated.\n\nii) Particularly for small samples, there is a high chance of the tests suggesting that variables are not cointegrated when they are.\n\niii) It is not possible to make inferences on the cointegrating regression.\n\niv) The procedure forces the researcher to specify which is the dependent variable and which are the independent variables."}
{"question": "Which of the following statements is true concerning the population regression function (PRF) and sample regression function (SRF)?", "choices": [{"label": "A", "text": "The PRF is the estimated model"}, {"label": "B", "text": "The PRF is used to infer likely values of the SRF"}, {"label": "C", "text": "Whether the model is good can be determined by comparing the SRF and the PRF"}, {"label": "D", "text": "The PRF is a description of the process thought to be generating the data."}], "answerKey": "D", "model_answer": "The correct answer is (D).\nThe population regression function (PRF) is a theoretical model that describes the relationship between the predictor variables and the response variable in a population. It is the distribution of values of the response variable for a given set of values of the predictor variables. The sample regression function (SRF) is a similar theoretical model that describes the relationship between the predictor variables and the response variable in a sample of the population. It is the distribution of values of the response variable for a given set of values of the predictor variables in a sample of the population.\nThe PRF and the SRF are both useful for making predictions and inferences about the population. The PRF can be estimated from a sample of the population using a sample regression analysis, and the estimate of the PRF can be used to make predictions about the response variable in the population based on the values of the predictor variables in the sample.\nComparing the PRF and the SRF can also be useful for making inferences about the relationship between the predictor variables and the response variable. If the PRF and the SRF are similar, this suggests that the relationship between the predictor variables and the response variable is likely to be similar in the population as"}
{"question": "Which one of the following is a disadvantage of the general to specific or \"LSE\" (\"Hendry\") approach to building econometric models, relative to the specific to general approach?", "choices": [{"label": "A", "text": "Some variables may be excluded at the first stage leading to coefficient biases"}, {"label": "B", "text": "The final model may lack theoretical interpretation"}, {"label": "C", "text": "The final model may be statistically inadequate"}, {"label": "D", "text": "If the initial model is mis-specified, all subsequent steps will be invalid."}], "answerKey": "B", "model_answer": "D. If the initial model is mis-specified, all subsequent steps will be invalid.\nThe general to specific, or \"LSE\" (\"Hendry\") approach to building econometric models involves starting with a general model and then using specifications to refine and improve the model. This approach has some potential disadvantages relative to the specific to general approach:\n\n(A) Some variables may be excluded at the first stage leading to coefficient biases.\n(B) The final model may lack theoretical interpretation, which could limit its usefulness.\n(C) The final model may be statistically inadequate, which could lead to incorrect inferences and conclusions.\n\nHowever, the most significant disadvantage of the general to specific approach is that if the initial model is mis-specified, all subsequent steps will be invalid. This means that any conclusions drawn from the model will be invalid, and the model will not be able to accurately represent the relationship between variables. Therefore, it is important to carefully test and validate the initial model before proceeding with the refinement process."}
{"question": "Which of the following statements are true concerning maximum likelihood (ML) estimation in the context of GARCH models?\n\ni) Maximum likelihood estimation selects the parameter values that maximise the\n\nprobability that we would have actually observed the values of the series y that we\n\nactually did.\n\n\nii) GARCH models can only be estimated by ML and not by OLS\n\n\niii) For estimation of a standard linear model (with no GARCH), the OLS and ML\n\nestimates for the slope and intercept parameters will be identical but the estimator\n\nfor the variance of the disturbances is slightly different\n\n\niv) Most computer packages use numerical procedures to estimate GARCH models\n\nrather than a set of analytical formulae", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\ni) Maximum likelihood estimation selects the parameter values that maximise the probability that we would have actually observed the values of the series y that we actually did.\nii) GARCH models can only be estimated by ML and not by OLS.\niii) For estimation of a standard linear model (with no GARCH), the OLS and ML estimates for the slope and intercept parameters will be identical but the estimator for the variance of the disturbances is slightly different.\niv) Most computer packages use numerical procedures to estimate GARCH models rather than a set of analytical formulae.\n\nThus, the correct answer is (D) (i), (ii), (iii), and (iv)."}
{"question": "Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to which one of the following?", "choices": [{"label": "A", "text": "Biased but consistent coefficient estimates"}, {"label": "B", "text": "Biased and inconsistent coefficient estimates"}, {"label": "C", "text": "Unbiased but inconsistent coefficient estimates"}, {"label": "D", "text": "Unbiased and consistent but inefficient coefficient estimates."}], "answerKey": "A", "model_answer": "The correct answer is (B) Biased and inconsistent coefficient estimates.\nWhen you include lagged values of the dependent variable on the right hand side of a regression equation, you are creating a correlation between the independent and dependent variables. This can lead to biased and inconsistent coefficient estimates, as the estimates are not reflecting the true relationship between the independent and dependent variables.\nFor example, if you are regressing the dependent variable on the previous year's value of the dependent variable, this would create a correlation between the independent and dependent variables, leading to bias in the coefficient estimates.\nIt is important to note that when you include lagged values of the dependent variable on the right hand side of a regression equation, you are essentially estimating a correlation between the independent and dependent variables, and not necessarily the causal relationship between them. The coefficient estimates may not be accurate representations of the true relationship between the independent and dependent variables."}
{"question": "Which one of the following factors is likely to lead to a relatively high degree of out-of-sample forecast accuracy?", "choices": [{"label": "A", "text": "A model that is based on financial theory"}, {"label": "B", "text": "A model that contains many variables"}, {"label": "C", "text": "A model whose dependent variable has recently exhibited a structural change"}, {"label": "D", "text": "A model that is entirely statistical in nature with no room for judgmental modification of forecasts"}], "answerKey": "A", "model_answer": "The correct answer is (D) A model that is entirely statistical in nature with no room for judgmental modification of forecasts. This is because statistical models are based on mathematical equations that are based on historical data, and do not incorporate any judgment or expertise of the forecaster. Therefore, the accuracy of the forecasts produced by such models will depend entirely on the quality and relevance of the historical data used to train the model. In contrast, a model that is based on financial theory or contains many variables, or one that has a dependent variable that has recently exhibited a structural change, may have a higher degree of accuracy if the model is well-constructed and the data is accurate and relevant. However, these factors alone are unlikely to result in a high degree of out-of-sample forecast accuracy without proper use of statistical models."}
{"question": "Which of the following are plausible approaches to dealing with residual autocorrelation?\n\ni) Take logarithms of each of the variables\n\nii) Add lagged values of the variables to the regression equation\n\niii) Use dummy variables to remove outlying observations\n\niv) Try a model in first differenced form rather than in levels.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nAll of the choices listed are plausible approaches to dealing with residual autocorrelation. Residual autocorrelation is a form of autocorrelation that occurs in a regression model when the error terms (i.e., the differences between the predicted values and the actual values) have a tendency to be positive or negative over time.\n\nChoice (i) taking logarithms of each of the variables is a common approach to dealing with autocorrelation because logarithms have the property of decreasing the variance of the residuals.\n\nChoice (ii) adding lagged values of the variables to the regression equation is another common approach to dealing with autocorrelation, as it can help to capture the temporal dependence in the data.\n\nChoice (iii) using dummy variables to remove outlying observations is another approach to dealing with autocorrelation, as it can help to identify and remove observations that are likely to be influenced by autocorrelation.\n\nChoice (iv) trying a model in first differenced form rather than in levels is another approach to dealing with autocorrelation, as it can"}
{"question": "For an autoregressive process to be considered stationary", "choices": [{"label": "A", "text": "The roots of the characteristic equation must all lie inside the unit circle"}, {"label": "B", "text": "The roots of the characteristic equation must all lie on the unit circle"}, {"label": "C", "text": "The roots of the characteristic equation must all lie outside the unit circle"}, {"label": "D", "text": "The roots of the characteristic equation must all be less than one in absolute value"}], "answerKey": "C", "model_answer": "The correct answer is (A) The roots of the characteristic equation must all lie inside the unit circle.\nThe characteristic equation is a mathematical equation that describes the properties of an autoregressive process. It involves taking the determinant of the autocovariance matrix of the process, which is a square matrix with elements that describe the correlation between each pair of data points in the process.\nThe roots of the characteristic equation are the eigenvalues of the autocovariance matrix. If the roots all lie inside the unit circle, then the process is said to be stationary, meaning that it has a constant mean and constant variance over time. If the roots all lie outside the unit circle, then the process is said to be non-stationary, meaning that it has a time-varying mean and variance.\nTherefore, the correct answer is (A) The roots of the characteristic equation must all lie inside the unit circle."}
{"question": "Which of the following statements are true concerning information criteria?\n\n(i) Adjusted R-squared is an information criterion\n\n(ii) If the residual sum of squares falls when an additional term is added, the value of the information criterion will fall\n\n(iii) Akaike's information criterion always leads to model orders that are at least as large as those of Schwarz's information criterion\n\n(iv) Akaike's information criterion is consistent", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (B) (i) and (iii) only.\n\nThe information criteria are statistical measures used to evaluate the fit of a model to a set of data. They are used to compare different models and help researchers decide which model to choose. The most commonly used information criteria are R-squared, Akaike Information Criterion (AIC), and Schwarz's Bayesian Information Criterion (SBC).\n\nAkaike's Information Criterion (AIC) is a measure of the relative efficiency of a statistical model, based on the residual sum of squares, the number of parameters in the model, and the size of the data. It is a widely used measure for model selection, particularly in the fields of econometrics and time series analysis.\n\nSchwarz's Bayesian Information Criterion (SBC) is a measure of the relative efficiency of a statistical model, based on the residual sum of squares, the number of parameters in the model, and the size of the data. It is also a widely used measure for model selection, particularly in the fields of econometrics and time series analysis.\n\nAdjusted R-squared is a measure of the"}
{"question": "The \"within transform\" involves", "choices": [{"label": "A", "text": "Taking the average values of the variables"}, {"label": "B", "text": "Subtracting the mean of each entity away from each observation on that entity"}, {"label": "C", "text": "Estimating a panel data model using least squares dummy variables"}, {"label": "D", "text": "Using both time dummies and cross-sectional dummies in a fixed effects panel model"}], "answerKey": "B", "model_answer": "The correct answer is (C) Estimating a panel data model using least squares dummy variables.\nLeast squares dummy variables are a type of dummy variable that is used to model the relationship between a dependent variable and one or more independent variables. In a panel data model, dummy variables are used to capture the effect of time dummies and cross-sectional dummies on the dependent variable. Time dummies are used to account for the fact that the variables may be different over time, while cross-sectional dummies are used to account for the fact that the variables may be different across different units in the same time period.\nA and B do not involve taking the average values of the variables or subtracting the mean of each entity away from each observation on that entity. D is not a correct answer as it does not involve using both time dummies and cross-sectional dummies in a fixed effects panel model."}
{"question": "The purpose of \"augmenting\" the Dickey-Fuller test regression is to", "choices": [{"label": "A", "text": "Ensure that there is no heteroscedasticity in the test regression residuals."}, {"label": "B", "text": "Ensure that the test regression residuals are normally distributed"}, {"label": "C", "text": "Ensure that there is no autocorrelation in the test regression residuals"}, {"label": "D", "text": "Ensure that all of the non-stationarity is taken into account."}], "answerKey": "C", "model_answer": "The correct answer is (D).\nThe purpose of augmenting the Dickey-Fuller test regression is to ensure that all forms of non-stationarity are taken into account. This is done by adding additional predictors to the regression that are thought to be related to the time-series characteristics of the data. This can help to improve the accuracy and robustness of the results of the test and provide a more accurate measure of the presence of a unit root in the data.\nThe other choices are not entirely accurate representations of the purpose of augmenting the Dickey-Fuller test regression. While (A) addresses the issue of heteroscedasticity, which can affect the accuracy of the test, it is not the primary purpose of augmenting the test. Similarly, (B) and (C) address issues related to the distribution and autocorrelation of the test regression residuals, but these are not the primary purpose of augmenting the test.\nD. Ensure that all of the non-stationarity is taken into account."}
{"question": "If a series, y, follows a random walk with drift b, what is the optimal one-step ahead forecast of the change in y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of the change in y over the in-sample period"}], "answerKey": "D", "model_answer": "The correct answer is (C) One.\nA random walk with drift has a mean of b, so the expected change in y is b. Therefore, the optimal one-step ahead forecast of the change in y would be the expected value of the change in y, which is b.\nSo, the correct answer is (C) One."}
{"question": "Which of the following are plausible approaches to dealing with a model that exhibits heteroscedasticity?\n\ni) Take logarithms of each of the variables\n\nii) Use suitably modified standard errors\n\niii) Use a generalised least squares procedure\n\niv) Add lagged values of the variables to the regression equation.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv). All of these approaches are potential ways to deal with heteroscedasticity in a linear regression model."}
{"question": "Which of the following statements are true concerning the standardised residuals (residuals divided by their respective conditional standard deviations) from an estimated GARCH model?\n\ni) They are assumed to be normally distributed\n\n\nii) Their squares will be related to their lagged squared values if the GARCH model is\n\nappropriate\n\n\niii) In practice, they are likely to have fat tails\n\n\niv) If the GARCH model is adequate, the standardised residuals and the raw residuals\n\nwill be identical", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is: (C) (i), (ii), and (iii) only.\n\nThe standardised residuals from a GARCH model are not assumed to be normally distributed, as the GARCH model explicitly accounts for the non-normality of the residuals.\n\nThe squared standardised residuals from a GARCH model are indeed related to their lagged squared values if the GARCH model is appropriate, as the squared residuals can be used to capture the autocorrelation of the residuals.\n\nFurthermore, as the GARCH model accounts for the non-normality of the residuals, the standardised residuals and the raw residuals are not identical in practice. In fact, the standardised residuals are often used in practice because they can be more easily interpreted and used in statistical analysis."}
{"question": "Which one of the following statements is true concerning VARs?", "choices": [{"label": "A", "text": "The coefficient estimates have intuitive theoretical interpretations"}, {"label": "B", "text": "The coefficient estimates usually have the same sign for all of the lags of a given variable in a given equation"}, {"label": "C", "text": "VARs often produce better forecasts than simultaneous equation structural models"}, {"label": "D", "text": "All of the components of a VAR must be stationary before it can be used for forecasting"}], "answerKey": "C", "model_answer": "The correct answer is (A) The coefficient estimates have intuitive theoretical interpretations.\nA VAR (Vector Autoregression) is a linear model that represents the dynamic relationship between a set of macroeconomic variables, such as GDP, inflation, and interest rates, over time. The coefficients in a VAR model provide estimates of the impact of one variable on another, and these coefficients have theoretical interpretations that can be used to understand the economy. For example, the coefficient on the lagged value of GDP might represent the short-term impact of GDP on itself, while the coefficient on the lagged value of interest rates might represent the long-term impact of interest rates on GDP. These coefficients can be used to identify the transmission mechanism of policy changes and to forecast future economic events.\n\nOn the other hand, B) The coefficient estimates usually have the same sign for all of the lags of a given variable in a given equation is not entirely true. In a VAR model, the sign of the coefficient on a lagged variable can depend on the other lags and the coefficient on the other variables in the equation, so the sign of the coefficient for a given lagged variable may not be consistent across all equations in the V"}
{"question": "Which of the following statements is INCORRECT concerning the classical hypothesis testing framework?", "choices": [{"label": "A", "text": "If the null hypothesis is rejected, the alternative is accepted"}, {"label": "B", "text": "The null hypothesis is the statement being tested while the alternative encompasses the remaining outcomes of interest"}, {"label": "C", "text": "The test of significance and confidence interval approaches will always give the same conclusions"}, {"label": "D", "text": "Hypothesis tests are used to make inferences about the population parameters."}], "answerKey": "A", "model_answer": "The correct answer is (D).\nHypothesis testing is a statistical technique used to test the null hypothesis against an alternative hypothesis. The null hypothesis is the statement being tested, which is typically the assumption that there is no effect or difference between two groups or variables. The alternative hypothesis encompasses the remaining outcomes of interest.\nThe test of significance is used to determine whether the observed value of a sample statistic is significantly different from the expected value under the null hypothesis, and it is based on the p-value. A low p-value indicates that the null hypothesis can be rejected, and a high p-value indicates that the null hypothesis cannot be rejected.\nA confidence interval is a measure of the uncertainty of an estimate of a population parameter based on a sample. It is used to determine whether the observed difference between two groups or variables is likely to occur by chance or if it is truly due to an effect.\nSo, the correct answer is (D) Hypothesis tests are used to make inferences about the population parameters."}
{"question": "An ARMA(p,q) (p, q are integers bigger than zero) model will have", "choices": [{"label": "A", "text": "An acf and pacf that both decline geometrically"}, {"label": "B", "text": "An acf that declines geometrically and a pacf that is zero after p lags"}, {"label": "C", "text": "An acf that declines geometrically and a pacf that is zero after q lags"}, {"label": "D", "text": "An acf that is zero after p lags and a pacf that is zero after q lags"}], "answerKey": "A", "model_answer": "The correct answer is (C). An ARMA(p,q) model has an autocorrelation function (ACF) that declines geometrically with lag order p, and a partial autocorrelation function (PACF) that declines geometrically with lag order q. The ACF and PACF of an ARMA(p,q) model are both defined using the same parameters p and q, so the choice (A) is incorrect because the ACF and PACF of an ARMA(p,q) model cannot both decline geometrically. The choice (B) is incorrect because an ARMA(p,q) model does not have a PACF that is zero after any specific lag order, so the choice (B) is incorrect because the PACF of an ARMA(p,q) model is not zero after any specific lag order. The choice (D) is incorrect because an ARMA(p,q) model does not have an ACF or PACF that is zero after any specific lag order, so the choice (D) is incorrect because the ACF and PACF of an ARMA(p,q) model are not zero after any specific"}
{"question": "Suppose that the following regression is estimated using 27 quarterly observations:\n\n$y_t = \\beta_1 + \\beta_2 x_2 + \\beta_3 x_{3t} + u_t$\n\nWhat is the appropriate critical value for a 2-sided 5% size of test of $H_0: \\beta_3 = 1$?", "choices": [{"label": "A", "text": "1.64"}, {"label": "B", "text": "1.71"}, {"label": "C", "text": "2.06"}, {"label": "D", "text": "1.96"}], "answerKey": "C", "model_answer": "The correct answer is (D) 1.96.\n\nThe critical value for a two-sided 5% significance test of $H_0: \\beta_3 = 1$ is obtained by using the t-distribution with the degrees of freedom that correspond to the number of independent variables in the regression, which in this case is 2 (since $x_2$ and $x_{3t}$ are both independent variables). The critical value for a two-sided test is obtained by looking up the z-score that corresponds to the desired significance level (5% in this case) and the degrees of freedom in a t-distribution table.\n\nIn this case, the critical value for a two-sided significance test of $H_0: \\beta_3 = 1$ is approximately 1.96, which is the z-score that corresponds to the degrees of freedom for a t-distribution with 2 degrees of freedom."}
{"question": "Suppose that two researchers, using the same 3 variables and the same 250 observations on each variable, estimate a VAR. One estimates a VAR(6), while the other estimates a VAR(4). The determinants of the variance-covariance matrices of the residuals for each VAR are 0.0036 and 0.0049 respectively. What is the values of the test statistic for performing a test of whether the VAR(6) can be restricted to a VAR(4)?", "choices": [{"label": "A", "text": "77.07"}, {"label": "B", "text": "0.31"}, {"label": "C", "text": "0.33"}, {"label": "D", "text": "4.87"}], "answerKey": "A", "model_answer": "The correct answer is (C) 0.33.\nThis question is asking about a test of the hypothesis that the VAR(6) can be restricted to a VAR(4), which means that the VAR(6) model has more parameters than the VAR(4) model, and the null hypothesis is that the VAR(6) model is not better than the VAR(4) model. To test this hypothesis, we can use the Breusch-Pagan test of unit roots, which is based on the residuals of the VAR models.\nThe Breusch-Pagan test statistic is calculated as:\nT = 2(max((abs(residuals(VAR(6))) - abs(residuals(VAR(4))))^2, 0))\nIn this case, the choices given are incorrect.\nOption (A) is incorrect because it would give a very large test statistic of 77.07, which is much larger than the typical values of the test statistic, which are in the range of 1 to 3.\nOption (B) is incorrect because it would give a very small test statistic of 0.0036, which"}
{"question": "Which of the following is a DISADVANTAGE of using pure time-series models (relative to structural models)?", "choices": [{"label": "A", "text": "They are not theoretically motivated"}, {"label": "B", "text": "They cannot produce forecasts easily"}, {"label": "C", "text": "They cannot be used for very high frequency data"}, {"label": "D", "text": "It is difficult to determine the appropriate explanatory variables for use in pure time-series models"}], "answerKey": "A", "model_answer": "The correct answer is (B) They cannot produce forecasts easily.\nPure time-series models are statistical models that are used to analyze and make predictions based on time-series data. Structural models, on the other hand, are statistical models that can be used to analyze and make predictions on a wide range of data types, including time-series data.\nOne of the disadvantages of pure time-series models is that they cannot produce forecasts easily. This is because pure time-series models do not take into account the relationships between variables or the underlying structure of the data. In order to make accurate predictions, structural models are often used in conjunction with pure time-series models.\nStructural models are also more flexible and can be used to analyze a wide range of data types, including time-series data. They can be used to model complex relationships and structures within the data, which can be useful for making accurate predictions and understanding the underlying factors that are driving the data.\nOverall, while pure time-series models can be useful for analyzing and making predictions on time-series data, they have limited capabilities compared to structural models, and they may not be able to produce accurate predictions without additional information or modeling."}
{"question": "Which of the following are alternative names for the dependent variable (usually denoted by y) in linear regression analysis?\n\n(i) The regressand\n\n(ii) The regressor\n\n(iii) The explained variable\n\n(iv) The explanatory variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (C) (i), (ii), and (iii) only.\nThe dependent variable (y) in linear regression analysis is also known as the predictor variable, the criterion variable, and the response variable.\nIt is the variable that is predicted or explained by the independent variables.\nThe independent variables (x) are also known as the regressors, the predictors, and the explanatory variables.\nThe independent variables are the variable that is used to predict the dependent variable."}
{"question": "Which of the following are advantages of the VAR approach to modelling the relationship between variables relative to the estimation of full structural models?\n\ni) VARs receive strong motivation from financial and economic theory\n\n\nii) VARs in their reduced forms can be used easily to produce time-series forecasts\n\n\niii) VAR models are typically highly parsimonious\n\n\niv) OLS can be applied separately to each equation in a reduced form VAR", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is C. The advantages of using a VAR approach to model the relationship between variables include:\n\n1. VARs receive strong motivation from financial and economic theory, which provides a theoretical framework for analyzing economic relationships and their dynamics.\n2. VARs in their reduced forms can be used easily to produce time-series forecasts, which allows for the prediction of future economic outcomes.\n3. VAR models are typically highly parsimonious, which means that they require fewer variables and parameters to explain a given set of data, making them more efficient and less prone to overfitting.\n4. OLS can be applied separately to each equation in a reduced form VAR, which allows for the independent estimation and testing of each equation in the model."}
{"question": "Which of the following statements is TRUE concerning the standard regression model?", "choices": [{"label": "A", "text": "y has a probability distribution"}, {"label": "B", "text": "x has a probability distribution"}, {"label": "C", "text": "The disturbance term is assumed to be correlated with x"}, {"label": "D", "text": "For an adequate model, the residual (u-hat) will be zero for all sample data points"}], "answerKey": "A", "model_answer": "The correct answer is (C) The disturbance term is assumed to be correlated with x.\nThe standard regression model assumes that the disturbances term (u) is correlated with the predictor variable (x), meaning that the disturbances are not independent of the predictor variable. This assumption is known as the exogeneity assumption and is an essential condition for the standard regression model to be valid. If the disturbances are not correlated with the predictor variable, the model is not valid and the estimated coefficients may not be reliable."}
{"question": "Consider the following model for $y_t$:\n\n$y_t = \\mu + \\lambda t + u_t$\n\nWhich one of the following most accurately describes the process for $y_t$?", "choices": [{"label": "A", "text": "A unit root process"}, {"label": "B", "text": "A stationary process"}, {"label": "C", "text": "A deterministic trend process"}, {"label": "D", "text": "A random walk with drift"}], "answerKey": "C", "model_answer": "The correct answer is (D) A random walk with drift.\n\nA random walk with drift is a model in which the series is a random walk with a constant drift, meaning that the series is consistently moving in one direction. In this case, the drift is given by the parameter $\\lambda$, which represents the rate at which the series is moving.\n\nThe other three choices are not accurate descriptions of the process for $y_t$.\n\nOption (A) Unit root process refers to a time series that is non-stationary, meaning that its statistical properties change over time.\nOption (B) Stationary process refers to a time series that is considered to be stationary, which means that it does not have a unit root and its statistical properties do not change over time.\nOption (C) Deterministic trend process refers to a time series that is considered to be stationary, but with a deterministic trend, which means that it has a constant drift."}
{"question": "Which of the following is correct concerning logit and probit models?", "choices": [{"label": "A", "text": "They use a different method of transforming the model so that the probabilities lie between zero and one"}, {"label": "B", "text": "The logit model can result in too many observations falling at exactly zero or exactly one"}, {"label": "C", "text": "For the logit model, the marginal effect of a change in one of the explanatory variables is simply the estimate of the parameter attached to that variable, whereas this is not the case for the probit model"}, {"label": "D", "text": "The probit model is based on a cumulative logistic function"}], "answerKey": "A", "model_answer": "The correct answer is (B). The logit model can result in too many observations falling at exactly zero or exactly one. This is because the logit model is a translogit model, which means it is a nonlinear model that transforms the data so that the probabilities lie between zero and one. The probit model, on the other hand, is a linear model that is used to model binary responses. The probit model does not have this issue of too many observations falling at exactly zero or exactly one."}
{"question": "What is the most important disadvantage of the diagonal VECH approach to building multivariate GARCH models that is overcome by the BEKK formulation?", "choices": [{"label": "A", "text": "The diagonal VECH model is hard to interpret intuitively"}, {"label": "B", "text": "The diagonal VECH model contains too many parameters"}, {"label": "C", "text": "The diagonal VECH model does not ensure a positive-definite variance-covariance matrix"}, {"label": "D", "text": "The BEKK model reduces the dimensionality problem that arises when a number of series are modelled together."}], "answerKey": "C", "model_answer": "The correct answer is (C) The diagonal VECH model does not ensure a positive-definite variance-covariance matrix.\nThis answer is correct because the diagonal VECH approach does not ensure that the variance-covariance matrix is positive-definite, which can lead to issues with the estimation and interpretation of the model. The BEKK formulation solves this problem by using a different set of parameters that ensure the positive-definiteness of the variance-covariance matrix, making it a more reliable and valid approach to building multivariate GARCH models."}
{"question": "If a relevant variable is omitted from a regression equation, the consequences would be that:\n\ni) The standard errors would be biased\n\n\nii) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope coefficients will be inconsistent.\n\n\niii) If the excluded variable is uncorrelated with all of the included variables, the\n\nintercept coefficient will be inconsistent.\n\n\niv) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope and intercept coefficients will be consistent and unbiased but inefficient.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nWhen a relevant variable is omitted from a regression equation, the consequences can include biased standard errors, inconsistent slope and intercept coefficients, and inefficient estimates. Option (A) (ii) and (iv) only is incorrect because bias in standard errors can occur even if the excluded variable is not uncorrelated with the included variables. Option (B) (i) and (iii) only is incorrect because inconsistency in the intercept coefficient can occur even if the excluded variable is not uncorrelated with the included variables. Option (C) (i), (ii), and (iii) only is incorrect because bias in the slope and intercept coefficients can occur even if the excluded variable is not uncorrelated with the included variables. Option (D) (i), (ii), (iii), and (iv) is the correct answer, as the omitted variable could lead to any of these consequences depending on the specifics of the situation."}
{"question": "Which of the following are alternative names for the independent variable (usually denoted by x) in linear regression analysis?\n\n(i) The regressor\n\n(ii) The regressand\n\n(iii) The causal variable\n\n(iv) The effect variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (C) (i), (ii), (iii), and (iv)."}
{"question": "Consider the OLS estimator for the standard error of the slope coefficient. Which of the following statement(s) is (are) true?\n\n(i) The standard error will be positively related to the residual variance\n\n(ii) The standard error will be negatively related to the dispersion of the observations on the explanatory variable about their mean value\n\n(iii) The standard error will be negatively related to the sample size\n\n(iv) The standard error gives a measure of the precision of the coefficient estimate.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nOption (D) includes all four statements, which are true for the OLS estimator.\n\nOption (A) (ii) and (iv) only is not true, as the standard error is not only related to the dispersion of the observations on the explanatory variable about their mean value.\n\nOption (B) (i) and (iii) only is not true, as the standard error is negatively related to both the residual variance and the sample size.\n\nOption (C) (i), (ii), (iii), and (iv) only is almost correct, as the standard error is related to the residual variance, the dispersion of the observations on the explanatory variable about their mean value, and the sample size, but not only to the dispersion of the observations on the explanatory variable about their mean value."}
{"question": "What is the meaning of the term \"heteroscedasticity\"?", "choices": [{"label": "A", "text": "The variance of the errors is not constant"}, {"label": "B", "text": "The variance of the dependent variable is not constant"}, {"label": "C", "text": "The errors are not linearly independent of one another"}, {"label": "D", "text": "The errors have non-zero mean"}], "answerKey": "A", "model_answer": "The correct answer is (B) The variance of the dependent variable is not constant.\nHeteroscedasticity refers to the phenomenon where the variance of a variable, in this case the dependent variable, is not constant across different observations. It means that the spread of the data is not consistent and can vary significantly from one observation to another. The term comes from the Greek words \"heteros\" (meaning different) and \"skedastic\" (meaning capable of estimating). The variance of the errors is not constant, and the errors are not linearly independent of one another."}
{"question": "If a Durbin Watson statistic takes a value close to zero, what will be the value of the first order autocorrelation coefficient?", "choices": [{"label": "A", "text": "Close to zero"}, {"label": "B", "text": "Close to plus one"}, {"label": "C", "text": "Close to minus one"}, {"label": "D", "text": "Close to either minus one or plus one"}], "answerKey": "C", "model_answer": "The correct answer is (C) Close to minus one.\nThe Durbin Watson statistic is a test statistic used to test for the presence of autocorrelation in a time series. If the Durbin Watson statistic takes a value close to zero, it suggests that the autocorrelation is not significant and the series can be considered to be stationary.\nThe first order autocorrelation coefficient, also known as the Pearson's correlation coefficient, is a measure of the linear relationship between two time series. If the series is stationary, the autocorrelation coefficient should be close to zero. However, if the series is non-stationary, the autocorrelation coefficient will have a different value.\nTherefore, the correct answer is (C) Close to minus one."}
{"question": "Under the null hypothesis of a Bera-Jarque test, the distribution has", "choices": [{"label": "A", "text": "Zero skewness and zero kurtosis"}, {"label": "B", "text": "Zero skewness and a kurtosis of three"}, {"label": "C", "text": "Skewness of one and zero kurtosis"}, {"label": "D", "text": "Skewness of one and kurtosis of three."}], "answerKey": "B", "model_answer": "The correct answer is (A) Zero skewness and zero kurtosis.\nThe Bera-Jarque test is used to test for the normality of a data distribution. If the null hypothesis of the test is true, then the distribution of the data is assumed to be normal. The test compares the sample skewness and kurtosis values to their respective population values.\nIf the null hypothesis is true, then the sample skewness and kurtosis values should be equal to the population skewness and kurtosis values. If they are not equal, then the test rejects the null hypothesis and indicates that the data distribution is not normal.\nIf the null hypothesis is false, then the sample skewness and kurtosis values will not be equal to the population skewness and kurtosis values. In this case, the test will not reject the null hypothesis and the data distribution is considered to be normal.\nTherefore, the correct answer is (A) Zero skewness and zero kurtosis, as this indicates that the data distribution is normal according to the null hypothesis of the Bera-Jarque test."}
{"question": "If an estimator is said to be consistent, it is implied that", "choices": [{"label": "A", "text": "On average, the estimated coefficient values will equal the true values"}, {"label": "B", "text": "The OLS estimator is unbiased and no other unbiased estimator has a smaller variance"}, {"label": "C", "text": "The estimates will converge upon the true values as the sample size increases"}, {"label": "D", "text": "The coefficient estimates will be as close to their true values as possible for small and large samples."}], "answerKey": "C", "model_answer": "The correct answer is (C) The estimates will converge upon the true values as the sample size increases.\nChoice (A) is incorrect because it states that the estimated coefficient values will equal the true values, which is not necessarily true for all consistent estimators.\nChoice (B) is also incorrect because it states that the OLS estimator is unbiased and no other unbiased estimator has a smaller variance, which is not the same as implying that the estimated coefficient values will equal the true values.\nChoice (D) is also incorrect because it states that the coefficient estimates will be as close to their true values as possible for small and large samples, which does not necessarily imply that the estimated coefficient values will equal the true values.\nTherefore, the correct answer is (C) The estimates will converge upon the true values as the sample size increases. This statement implies that if the estimator is consistent, as the sample size increases, the coefficient estimates will get closer and closer to their true values. This is a key characteristic of consistent estimators and is an important aspect of statistical inference."}
{"question": "Which of the following is a typical characteristic of financial asset return time-series?", "choices": [{"label": "A", "text": "Their distributions are thin-tailed"}, {"label": "B", "text": "They are not weakly stationary"}, {"label": "C", "text": "They are highly autocorrelated"}, {"label": "D", "text": "They have no trend"}], "answerKey": "D", "model_answer": "The correct answer is (A) Their distributions are thin-tailed.\nFinancial asset return time-series are typically characterized by having thin-tailed distributions, which means that their tails are relatively narrow compared to other types of distributions, such as fat-tailed distributions. This characteristic is desirable in financial asset returns because it suggests that returns are more likely to be close to the mean, which is a characteristic of efficient markets. Additionally, thin-tailed distributions are more stable and less volatile than fat-tailed distributions, which makes them less susceptible to extreme events and crashes."}
{"question": "Which of the following assumptions are required to show the consistency, unbiasedness and efficiency of the OLS estimator?\n\ni) $E(u_t) = 0$\n\n\nii) $\\text{Var}(u_t) = \\sigma^2$\n\n\niii) $\\text{Cov}(u_t, u_{t-j}) = 0 \\forall j$\n\n\niv) $u_t \\sim N(0, \\sigma^2)$", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is:\n\n(D) (i), (ii), (iii), and (iv)."}
{"question": "Which of the following is a disadvantage of the fixed effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The model is likely to be technical to estimate"}, {"label": "B", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "C", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "D", "text": "The fixed effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}], "answerKey": "C", "model_answer": "Answer: D. The fixed effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable.\nThe fixed effects approach to estimating a panel model assumes that the error term is uncorrelated with both the regressors (control variables) and the time dimension, which is not the case when there is temporal variation in the dependent variable. Therefore, the fixed effects approach is not valid if the composite error term is correlated with one or more of the explanatory variables. The approach is useful for capturing cross-sectional heterogeneity, but it cannot capture temporal variation in the dependent variable.\nAnswer (A) is incorrect because the model is not necessarily technical to estimate, but may depend on the complexity of the model. Answer (B) is correct, as the fixed effects approach is not valid if the composite error term is correlated with one or more of the explanatory variables. Answer (C) is incorrect, as the number of parameters to estimate may be large, but this does not necessarily result in a loss of degrees of freedom."}
{"question": "Consider an identical situation to that of question 21, except that now a 2-sided alternative is used. What would now be the appropriate conclusion?", "choices": [{"label": "A", "text": "H0 is rejected"}, {"label": "B", "text": "H0 is not rejected"}, {"label": "C", "text": "H1 is rejected"}, {"label": "D", "text": "There is insufficient information given in the question to reach a conclusion"}], "answerKey": "A", "model_answer": "The correct answer is (B) H0 is not rejected. The null hypothesis (H0) states that there is no difference between the two groups. If the conclusion is that H0 is not rejected, it means that the data does not provide enough evidence to conclude that there is a difference between the two groups. So, the appropriate conclusion in this case would be that the difference between the two groups is not statistically significant, meaning that it could be due to chance or random variation rather than any real effect."}
{"question": "The price of a house is best described as what type of number?", "choices": [{"label": "A", "text": "Discrete"}, {"label": "B", "text": "Cardinal"}, {"label": "C", "text": "Ordinal"}, {"label": "D", "text": "Nominal"}], "answerKey": "B", "model_answer": "The correct answer is (D) Nominal.\nA nominal number is a non-numeric label or category assigned to something, such as the price of a house. The price of a house is not a measurable quantity and cannot be assigned a numerical value, so it is a nominal number.\nDiscrete numbers are numbers that can only take on certain integer values, such as 1, 2, 3, etc. Cardinal numbers are used to count or measure quantities, such as 2 apples, 5 oranges, etc. Ordinal numbers are used to order or label things, such as 1st, 2nd, 3rd, etc.\nSo, while the price of a house is not a discrete number, it is not a cardinal or ordinal number either. It is a nominal number, which is a non-numeric label or category."}
{"question": "If a Johansen \"trace\" test for a null hypothesis of 2 cointegrating vectors is applied to a system containing 4 variables is conducted, which eigenvalues would be used in the test?", "choices": [{"label": "A", "text": "All of them"}, {"label": "B", "text": "The largest 2"}, {"label": "C", "text": "The smallest 2"}, {"label": "D", "text": "The second largest"}], "answerKey": "C", "model_answer": "The correct answer is (A) All of them.\nWhen conducting a Johansen trace test for a null hypothesis of two cointegrating vectors, all of the eigenvalues are used to determine whether the null hypothesis is supported or not. The trace statistic is calculated as the sum of the eigenvalues, and if the trace statistic is larger than a critical value, the null hypothesis of two cointegrating vectors is rejected. Therefore, all of the eigenvalues are relevant in the test and should be used in the calculation."}
{"question": "Which of the following statements is true concerning variance decomposition analysis of VARs?\n\ni) Variance decompositions measure the impact of a unit shock to each of the variables on the VAR\n\nii) Variance decompositions can be thought of as measuring the proportion of the forecast error variance that is attributable to each variable\n\niii) The ordering of the variables is important for calculating impulse responses but not variance decompositions\n\niv) It is usual that most of the forecast error variance for a given variable is attributable to shocks to that variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (C) (i), (ii), (iii), and (iv).\n\nVariance decomposition analysis is a method used to decompose the variance of a vector auto-regression (VAR) model into its component parts, including the variance due to each variable, the variance due to shocks to each variable, and the variance due to shocks to the error term.\n\nOption (C) is the correct answer as it includes all four statements about variance decomposition analysis of VARs. Options (A) and (B) only include two statements each, while option (D) includes all four statements."}
{"question": "Suppose that we have estimated a GARCH model for daily equity returns, and we are interested in producing a 10-day forecast of the volatility (measured by the standard deviation of returns) for use in a value at risk model. How could such a forecast most validly be calculated?", "choices": [{"label": "A", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up"}, {"label": "B", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up and take the square root"}, {"label": "C", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts, take the square roots of each one and add them up"}, {"label": "D", "text": "Produce a 1-step ahead conditional variance forecast, take its square root and multiply it by the square root of 10"}], "answerKey": "B", "model_answer": "The correct answer is (C). Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts, take the square roots of each one, and then add them up. This is because the GARCH model is a mean-variance model, which forecasts the conditional variance based on past values of the variance. By taking the square root of each forecast and then adding them up, you are effectively calculating the forecast of the conditional variance."}
{"question": "Suppose that the value of $R^2$ for an estimated regression model is exactly zero. Which of the following are true?\n\ni) All coefficient estimates on the slopes will be zero\n\nii) The fitted line will be horizontal with respect to all of the explanatory variables\n\niii) The regression line has not explained any of the variability of y about its mean value\n\niv) The intercept coefficient estimate must be zero.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nAll of the statements are true. When the value of $R^2$ is exactly zero, it means that the regression line has not explained any of the variability of $y$ about its mean value. This means that the intercept coefficient estimate must be zero. Also, since $R^2$ is the proportion of variance in the dependent variable $y$ that is explained by the independent variables, a value of zero indicates that none of the variability in $y$ is explained by the variables. Therefore, the fitted line will be horizontal with respect to all of the explanatory variables. Lastly, since the regression line has not explained any of the variability of $y$, all of the coefficient estimates on the slopes will be zero."}
{"question": "A white noise process will have\n\n(i) A zero mean\n\n(ii) A constant variance\n\n(iii) Autocovariances that are constant\n\n(iv) Autocovariances that are zero except at lag zero", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The statement is true. Therefore, the correct answer is (D) (i), (ii), (iii), and (iv)."}
{"question": "Which of the following statements are true concerning the Box-Jenkins approach to diagnostic testing for ARMA models?\n\n(i) The tests will show whether the identified model is either too large or too small\n\n(ii) The tests involve checking the model residuals for autocorrelation, heteroscedasticity, and non-normality\n\n(iii) If the model suggested at the identification stage is appropriate, the acf and pacf for the residuals should show no additional structure\n\n(iv) If the model suggested at the identification stage is appropriate, the coefficients on the additional variables under the overfitting approach will be statistically insignificant", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (C) (i), (ii), and (iii) only. None of the other statements are true concerning the Box-Jenkins approach to diagnostic testing for ARMA models. The Box-Jenkins approach is a method for analyzing time series data and selecting the appropriate model to fit the data. The tests it involves checking the model residuals for autocorrelation, heteroscedasticity, and non-normality, and if the model suggested at the identification stage is appropriate, the acf and pacf for the residuals should show no additional structure."}
{"question": "Which one of the following would be a plausible response to a finding of residual non-normality?", "choices": [{"label": "A", "text": "Use a logarithmic functional form instead of a linear one"}, {"label": "B", "text": "Add lags of the variables on the right hand side of the regression model"}, {"label": "C", "text": "Estimate the model in first differenced form"}, {"label": "D", "text": "Remove any large outliers from the data."}], "answerKey": "D", "model_answer": "The correct answer is (A) Use a logarithmic functional form instead of a linear one. This is a plausible response to a finding of residual non-normality because logarithmic functions are less sensitive to outliers than linear functions, and therefore may produce more normal residuals. Choice (D) Remove any large outliers from the data is a good option to consider, but it is not a plausible response to a finding of residual non-normality in the context of a regression model. Choice (B) Add lags of the variables on the right hand side of the regression model is also not a plausible response to a finding of residual non-normality. Choice (C) Estimate the model in first differenced form is not a common response to a finding of residual non-normality, as first differencing a model can lead to loss of information and decreased efficiency."}
{"question": "The fixed effects panel model is also sometimes known as", "choices": [{"label": "A", "text": "A seemingly unrelated regression model"}, {"label": "B", "text": "The least squares dummy variables approach"}, {"label": "C", "text": "The random effects model"}, {"label": "D", "text": "Heteroscedasticity and autocorrelation consistent"}], "answerKey": "B", "model_answer": "B) The least squares dummy variables approach.\nThe fixed effects panel model is also sometimes known as the least squares dummy variables approach. This approach is used to estimate the impact of time-invariant variables on a dependent variable, while controlling for time-variant variables. It is typically used in panel data analysis, where data is collected at multiple points in time for the same units.\nA seemingly unrelated regression model is a type of regression analysis that is used to evaluate the relationship between a dependent variable and multiple independent variables, where the relationship between the independent variables and the dependent variable is not directly related.\nThe random effects model is a type of panel data analysis that is used to estimate the impact of time-invariant variables on a dependent variable, while controlling for time-variant variables, and it can be used to model heterogeneity among units.\nHeteroscedasticity and autocorrelation consistent is a type of model testing that is used to test for heteroscedasticity and autocorrelation in a time series data, and it is typically used to identify the presence of these problems and to develop appropriate models to address them."}
{"question": "Which of the following statements is TRUE concerning OLS estimation?", "choices": [{"label": "A", "text": "OLS minimises the sum of the vertical distances from the points to the line"}, {"label": "B", "text": "OLS minimises the sum of the squares of the vertical distances from the points to the line"}, {"label": "C", "text": "OLS minimises the sum of the horizontal distances from the points to the line"}, {"label": "D", "text": "OLS minimises the sum of the squares of the horizontal distances from the points to the line."}], "answerKey": "B", "model_answer": "The correct answer is (A) OLS minimizes the sum of the vertical distances from the points to the line.\nOLS (ordinary least squares) is a statistical method used to estimate the relationship between a dependent variable and one or more independent variables. The goal of OLS is to minimize the sum of the vertical distances from the points to the line, which is also known as the residual sum of squares. The residual sum of squares is calculated by subtracting the predicted values from the actual values for each observation. The residuals are then used to evaluate the goodness-of-fit of the model.\nThe other options are incorrect because they describe the sum of the squares of the vertical distances, which is not the goal of OLS."}
{"question": "If the standard tools for time-series analysis, such as estimation of the acf, pacf and spectral analysis, find no evidence of structure in the data, this implies that the data are which of the following?", "choices": [{"label": "A", "text": "Normally distributed"}, {"label": "B", "text": "Uncorrelated"}, {"label": "C", "text": "Independent"}, {"label": "D", "text": "Fat-tailed"}], "answerKey": "B", "model_answer": "The correct answer is (A) Normally distributed.\nIf the standard tools for time-series analysis, such as estimation of the ACF, PACF, and spectral analysis, do not find evidence of structure in the data, it could suggest that the data are normally distributed. The ACF and PACF can be used to examine the autocorrelation and partial autocorrelation of a time series, respectively. The spectral analysis can be used to examine the stationarity of a time series. If none of these tests find evidence of structure in the data, it may suggest that the data are normally distributed. However, it's important to note that other assumptions such as stationarity and non-trendiness are important as well for time-series analysis results to be reliable."}
{"question": "If two variables, $x_t$ and $y_t$ are said to be cointegrated, which of the following statements are true?\n\ni) $x_t$ and $y_t$ must both be stationary\n\n\nii) Only one linear combination of $x_t$ and $y_t$ will be stationary\n\n\niii) The cointegrating equation for $x_t$ and $y_t$ describes the short-run relationship\n\nbetween the two series\n\n\niv) The residuals of a regression of $y_t$ on $x_t$ must be stationary", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nWhen two variables, $x_t$ and $y_t$, are said to be cointegrated, it means that they are related through a long-run equilibrium relationship. This means that the two series are likely to move together in the long run, but not necessarily in the short run.\n\nStatement (i) is true, as both $x_t$ and $y_t$ must be stationary for the two series to be cointegrated.\n\nStatement (ii) is also true, as only one linear combination of $x_t$ and $y_t$ will be stationary. This is because cointegration implies that there is a long-run relationship between the two series, and the short-run relationship will be described by the cointegrating equation.\n\nStatement (iii) is false, as the cointegrating equation for $x_t$ and $y_t$ describes the long-run relationship between the two series, not the short-run relationship.\n\nStatement (iv) is also true, as the residuals of a regression of $y_t$ on $x_"}
{"question": "A dependent variable whose values are not observable outside a certain range but where the corresponding values of the independent variables are still available would be most accurately described as what kind of variable?", "choices": [{"label": "A", "text": "Censored"}, {"label": "B", "text": "Truncated"}, {"label": "C", "text": "Multinomial variable"}, {"label": "D", "text": "Discrete choice"}], "answerKey": "A", "model_answer": "The correct answer is (D) Discrete choice.\nWhen the values of the dependent variable are not observable outside a certain range, but the corresponding values of the independent variables are still available, the variable can be described as a discrete choice variable.\nA discrete choice variable is a type of variable that can only take on a limited number of values, such as the number of apples and oranges that a person might choose to eat. In this case, the variable is not observable outside a certain range because it can only take on a limited number of values, but the corresponding values of the independent variables are still available and can be used to make predictions about the dependent variable.\nFor example, if a survey asked people how many apples and oranges they would choose to eat, and the number of apples and oranges that people chose was not observable outside a certain range, such as between 0 and 10 apples and oranges, then the variable would be described as a discrete choice variable."}
{"question": "A Hausman test would be used for", "choices": [{"label": "A", "text": "Determining whether an equation that is part of a simultaneous system is identified"}, {"label": "B", "text": "Determining whether a simultaneous framework is needed for a particular variable"}, {"label": "C", "text": "Determining whether 2SLS or ILS is optimal"}, {"label": "D", "text": "Determining whether the structural form equations can be obtained via substitution from the reduced forms"}], "answerKey": "B", "model_answer": "A Hausman test is used for (A) determining whether an equation that is part of a simultaneous system is identified.\n\nThe Hausman test is a statistical test used to determine whether a system of equations is identified or not. The test compares the full system of equations to the reduced form of the system, and it helps determine which equation set is more parsimonious. The test provides valuable information on the nature of the relationship between the variables and can help identify whether the system of equations is identified or not."}
{"question": "Under the matrix notation for the classical linear regression model, $y = X \\beta + u$, what are the dimensions of $u$?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "B", "model_answer": "The correct answer is (C) k x 1, where k is the number of independent variables in the model.\n\nIn the matrix notation for the classical linear regression model, $y = X \\beta + u$, $X$ is a matrix of independent variables and $\\beta$ is a vector of coefficients. $u$ is a vector of residuals, which is the difference between the predicted values $y$ and the actual values of the response variable $y$. The dimensions of $u$ are $k \\times 1$, where $k$ is the number of independent variables.\n\nThe elements of $u$ are the differences between the predicted values and the actual values of each independent variable, multiplied by a constant term called the error variance. The error variance represents the variability of the residuals, which is assumed to be normally distributed with zero mean.\n\nTherefore, the correct answer is (C) k x 1, where k is the number of independent variables in the model."}
{"question": "How many parameters will be required to be estimated in total for all equations of a standard form, unrestricted, tri-variate VAR(4), ignoring the intercepts?", "choices": [{"label": "A", "text": "12"}, {"label": "B", "text": "4"}, {"label": "C", "text": "3"}, {"label": "D", "text": "36"}], "answerKey": "D", "model_answer": "The correct answer is (A) 12.\nWhen estimating the parameters in a standard form, unrestricted, tri-variate VAR(4), ignoring the intercepts, we need to estimate 3 VAR(4) models, each with 4 parameters (2 co-integration relations and 2 causality relations). So, we need to estimate a total of 3 \\* 4 = 12 parameters.\nPlease note that the correct answer is (A), 12, and not (D), 36."}
{"question": "A researcher tests for structural stability in the following regression model:\n\n$y_t = \\beta_1 + \\beta_2 x_{2t} + \\beta_3 x_{3t} + u_t$\n\nThe total sample of 200 observations is split exactly in half for the sub-sample regressions. Which would be the unrestricted residual sum of squares?", "choices": [{"label": "A", "text": "The RSS for the whole sample"}, {"label": "B", "text": "The RSS for the first sub-sample"}, {"label": "C", "text": "The RSS for the second sub-sample"}, {"label": "D", "text": "The sum of the RSS for the first and second sub-samples"}], "answerKey": "D", "model_answer": "The correct answer is (A) The RSS for the whole sample.\n\nWhen a researcher splits a sample into two sub-samples and performs a separate regression analysis on each sub-sample, the unrestricted residual sum of squares is the sum of the RSS for the whole sample and the RSS for each sub-sample.\n\nThe RSS for the whole sample is the difference between the actual and predicted values for the entire sample, and it represents the total amount of variation in the response variable (y) that cannot be explained by the regression model.\n\nThe RSS for each sub-sample is the difference between the actual and predicted values for only the observations in that sub-sample, and it represents the variation in the response variable that cannot be explained by the regression model for that sub-sample only.\n\nTherefore, the unrestricted residual sum of squares is the sum of the RSS for the whole sample and the RSS for each sub-sample, which gives us the total amount of variation in the response variable that cannot be explained by the regression model."}
{"question": "Suppose that we are interested in testing the null hypothesis that a GARCH(2,2) model can be restricted to a process with a constant conditional variance using the likelihood ratio test approach. Which of the following statements are true?", "choices": [{"label": "A", "text": "The test statistic will follow a chi-squared distribution with 2 degrees of freedom under the null hypothesis"}, {"label": "B", "text": "The value of the log-likelihood function will almost always be bigger for the restricted model than for the unrestricted model"}, {"label": "C", "text": "If the relevant values of the log-likelihood functions are -112.3 and -118.4, the value of the test statistic is 12.2"}, {"label": "D", "text": "The likelihood ratio test compares the slopes of the log-likelihood function at the maximum and at the restricted parameter value."}], "answerKey": "C", "model_answer": "The correct answer is (C). If the relevant values of the log-likelihood functions are -112.3 and -118.4, the value of the test statistic is 12.2. The likelihood ratio test compares the slopes of the log-likelihood function at the maximum and at the restricted parameter value."}
{"question": "Which one of the following is NOT a plausible remedy for near multicollinearity?", "choices": [{"label": "A", "text": "Use principal components analysis"}, {"label": "B", "text": "Drop one of the collinear variables"}, {"label": "C", "text": "Use a longer run of data"}, {"label": "D", "text": "Take logarithms of each of the variables"}], "answerKey": "D", "model_answer": "The correct answer is (D) Take logarithms of each of the variables. Using logarithms of the variables can help reduce the correlation between variables in a multicollinearity situation since the logarithm of a variable is less sensitive to the scale of the variable than the original variable."}
{"question": "Consider the following AR(2) process:\n\nyt = 1.5 yt-1 - 0.5 yt-2 + ut\n\nThis is a", "choices": [{"label": "A", "text": "Stationary process"}, {"label": "B", "text": "Unit root process"}, {"label": "C", "text": "Explosive process"}, {"label": "D", "text": "Stationary and unit root process"}], "answerKey": "B", "model_answer": "The correct answer is (D) Stationary and unit root process.\n\nThis AR(2) process can be represented as:\n\nyt = 1.5 yt-1 + 0.5 yt-2 + ut\n\nwhere yt represents the value of the time series at time t, yt-1 and yt-2 represent the values of the time series one period prior to time t, and ut represents the disturbance term.\n\nTo determine whether the process is stationary or not, we can check the correlation between the residuals (yt-1 and yt-2) and the mean of the process. If the correlation is close to zero, the process is stationary.\n\nIn this case, yt-1 and yt-2 are both one period prior to time t, so their correlation with yt should be close to zero. Therefore, the process is stationary.\n\nHowever, if we check the correlation between yt-1 and yt-2, it is not zero, so the process is not stationary.\n\nTherefore, the correct answer is (D) Stationary and unit root process."}
{"question": "Which of the following could be used as a test for autocorrelation up to third order?", "choices": [{"label": "A", "text": "The Durbin Watson test"}, {"label": "B", "text": "White's test"}, {"label": "C", "text": "The RESET test"}, {"label": "D", "text": "The Breusch-Godfrey test"}], "answerKey": "D", "model_answer": "The correct answer is (C) The RESET test.\nThe RESET test is a test for autocorrelation up to third order. The RESET test is used to test for the presence of autocorrelation in a time series data, where the lag order is not specified. The test statistic for the RESET test is the Durbin-Watson statistic, which is used to test for serial correlation in a regression model. The Durbin-Watson statistic is calculated as the ratio of the standard error of the residuals to the square root of the sample size.\nThe Durbin-Watson test is a test for autocorrelation up to second order, where the lag order is specified. The White test is a test for autocorrelation at any order, but it requires that the variance of the error term is constant over time. The Breusch-Godfrey test is a test for autocorrelation at any order, but it assumes that the variance of the error term is constant and that the serial correlation is homoscedastic.\nTherefore, the correct answer is (C) The RESET test."}
{"question": "The residual from a standard regression model is defined as", "choices": [{"label": "A", "text": "The difference between the actual value, y, and the mean, y-bar"}, {"label": "B", "text": "The difference between the fitted value, y-hat, and the mean, y-bar"}, {"label": "C", "text": "The difference between the actual value, y, and the fitted value, y-hat"}, {"label": "D", "text": "The square of the difference between the fitted value, y-hat, and the mean, y-bar"}], "answerKey": "C", "model_answer": "The correct answer is (B). The residual from a standard regression model is defined as the difference between the fitted value, y-hat, and the mean, y-bar. The fitted value, y-hat, is the predicted value of the response variable, y, based on the model parameters. The mean, y-bar, is the expected value of the response variable, based on the model assumptions and the sample data. The residual measures the discrepancy between the observed value of y and the predicted value, y-hat, based on the model. The residual can be used to check the model assumptions and to identify outliers or other anomalies in the data."}
{"question": "If OLS is applied separately to each equation that is part of a simultaneous system, the resulting estimates will be", "choices": [{"label": "A", "text": "Unbiased and consistent"}, {"label": "B", "text": "Biased but consistent"}, {"label": "C", "text": "Biased and inconsistent"}, {"label": "D", "text": "It is impossible to apply OLS to equations that are part of a simultaneous system"}], "answerKey": "C", "model_answer": "The correct answer is (B).\nOLS stands for Ordinary Least Squares, which is a method for estimating the parameters of a linear regression model. When OLS is applied separately to each equation that is part of a simultaneous system, the resulting estimates will be biased but consistent. This means that the estimates will be systematically biased, but will still be consistent with the assumptions of the OLS method, such as the linearity of the relationship and the normality of the errors.\nThe reason behind this bias is that the errors in the separate regressions are correlated with each other, which is not accounted for in the OLS method. However, the consistency of the estimates means that they will be consistent with the true parameters of the system, even if they are biased.\nIt's important to note that when applied to a simultaneous system, OLS can produce biased and inconsistent estimates, which means that the estimates are both biased and inconsistent with the assumptions of the method. Therefore, the correct answer is (B)."}
{"question": "Which one of the following is NOT an example of mis-specification of functional form?", "choices": [{"label": "A", "text": "Using a linear specification when y scales as a function of the squares of x"}, {"label": "B", "text": "Using a linear specification when a double-logarithmic model would be more appropriate"}, {"label": "C", "text": "Modelling y as a function of x when in fact it scales as a function of 1/x"}, {"label": "D", "text": "Excluding a relevant variable from a linear regression model"}], "answerKey": "D", "model_answer": "The correct answer is (D) Excluding a relevant variable from a linear regression model.\nMis-specification of functional form occurs when the functional form of a statistical model does not accurately represent the underlying relationship between the variables being studied. In the context of linear regression, mis-specification of functional form can occur when a non-linear relationship between the variables is assumed to be linear.\nExcluding a relevant variable from a linear regression model is not an example of mis-specification of functional form. It is a common mistake that can lead to biased results, but it is not a mis-specification of functional form.\nUsing a linear specification when a double-logarithmic model would be more appropriate is an example of mis-specification of functional form. It's when a linear model is used when a non-linear model would be more appropriate to describe the relationship between the variables.\nUsing a linear specification when y scales as a function of the squares of x is a mis-specification of functional form. It's a case of assuming a linear relationship between the variables when in fact a non-linear relationship exists."}
{"question": "Suppose that we wished to evaluate the factors that affected the probability that an investor would choose an equity fund rather than a bond fund or a cash investment. Which class of model would be most appropriate?", "choices": [{"label": "A", "text": "A logit model"}, {"label": "B", "text": "A multinomial logit"}, {"label": "C", "text": "A tobit model"}, {"label": "D", "text": "An ordered logit model"}], "answerKey": "B", "model_answer": "The correct answer is D: An ordered logit model.\nAn ordered logit model is a type of choice model that can be used to study decisions that involve multiple options, such as choosing between an equity fund, a bond fund, and a cash investment. The model allows for the analysis of how the probability of choosing an option depends on the characteristics of that option, as well as the characteristics of the other options.\nA logit model is a special case of an ordered logit model where the options are ordered in a specific way. A multinomial logit model is a more general model that can be used to study choices made among a set of options, such as choosing among three or more mutually exclusive options.\nA tobit model is a type of model that can be used to study decisions that are limited by a threshold or a bound, such as the minimum or maximum amount of money that an investor is willing to invest. The tobit model can be used to study decisions made about investment options, but it is not the most appropriate model for studying the probability of choosing an equity fund rather than a bond fund or a cash investment."}
{"question": "Which of the following statements will be true if the number of replications used in a Monte Carlo study is small?\n\ni) The statistic of interest may be estimated imprecisely\n\n\nii) The results may be affected by unrepresentative combinations of random draws\n\n\niii) The standard errors on the estimated quantities may be unacceptably large\n\n\niv) Variance reduction techniques can be used to reduce the standard errors", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nIf the number of replications used in a Monte Carlo study is small, then the results may be affected by unrepresentative combinations of random draws, the standard errors on the estimated quantities may be unacceptably large, and variances reduction techniques can be used to reduce the standard errors. Option (A) (ii) and (iv) only is incorrect because it does not include the effect of unrepresentative combinations of random draws. Option (B) (i) and (iii) only is incorrect because it does not include the effect of unacceptably large standard errors.\n\nOption (C) (i), (ii), (iii), and (iv) only is not the correct answer because it does not include the effect of unrepresentative combinations of random draws."}
{"question": "Which of the following is a disadvantage of the random effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "B", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "C", "text": "The random effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}, {"label": "D", "text": "All of (a) to (c) are potential disadvantages of the random effects approach."}], "answerKey": "A", "model_answer": "The correct answer is (B) The number of parameters to estimate may be large, resulting in a loss of degrees of freedom.\n\nThe random effects approach to estimating a panel model involves estimating a number of parameters, including the effects of the random effects, the fixed effects, and the error term. The number of parameters to estimate can be large, especially when there are many observations or explanatory variables in the panel. This can lead to a loss of degrees of freedom, which can impact the precision of the estimated parameters and the stability of the model.\n\nIt is important to note that the other choices (A) and (C) are not potential disadvantages of the random effects approach. The random effects approach can capture both cross-sectional and longitudinal heterogeneity, as well as temporal variation in the dependent variable. The validity of the approach is not affected by whether the composite error term is correlated with one or more of the explanatory variables."}
{"question": "Which of the following could result in autocorrelated residuals?\n\ni) Slowness of response of the dependent variable to changes in the values of the independent variables\n\nii) Over-reactions of the dependent variable to changes in the independent variables\n\niii) Omission of relevant explanatory variables that are autocorrelated\n\niv) Outliers in the data", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (A) (ii) and (iv) only.\n\nOption (i) describes the slowness of a dependent variable to respond to changes in independent variables. While it could result in autocorrelated residuals if the relationship between the variables is not stationary, it is not the cause of autocorrelated residuals in itself.\n\nOption (iii) describes the omission of relevant explanatory variables that are autocorrelated. These variables could result in autocorrelated residuals if they are not accounted for in the model, but they do not necessarily cause autocorrelated residuals themselves.\n\nOption (iv) describes outliers in the data, which can lead to autocorrelated residuals if they are not properly handled in the model.\n\nTherefore, the correct answer is (A) (ii) and (iv) only, which means that autocorrelated residuals can result from over-reactions of the dependent variable to changes in the independent variables and from outliers in the data."}
{"question": "Which of the following statements are true concerning the acf and pacf?\n\n(i) The acf and pacf are often hard to interpret in practice\n\n(ii) The acf and pacf can be difficult to calculate for some data sets\n\n(iii) Information criteria represent an alternative approach to model order determination\n\n(iv) If applied correctly, the acf and pacf will always deliver unique model selections", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nAll of the statements are true concerning the ACF and PACF, which are commonly used to analyze time series data.\n\n(i) The ACF and PACF are often hard to interpret in practice, as they can be difficult to understand and can provide conflicting information about the structure of the data.\n\n(ii) The ACF and PACF can be difficult to calculate for some data sets, particularly those with irregular or non-stationary time series.\n\n(iii) Information criteria, such as AIC and BIC, represent an alternative approach to model order determination, as they can help to identify the appropriate number of parameters in a time series model.\n\n(iv) If applied correctly, the ACF and PACF will always deliver unique model selections, as they can help to identify the appropriate model for a given data set."}
{"question": "Which of the following conditions are necessary for a series to be classifiable as a weakly stationary process?\n\n(i) It must have a constant mean\n\n(ii) It must have a constant variance\n\n(iii) It must have constant autocovariances for given lags\n\n(iv) It must have a constant probability distribution", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\nA process is said to be weakly stationary if its statistical properties remain the same over time, even if the time series itself is not stationary. In order for a process to be classified as weakly stationary, it must have a constant mean, constant variance, constant autocovariances for given lags, and a constant probability distribution.\n\nTherefore, options (A) (ii) and (iv) only, (B) (i) and (iii) only, and (C) (i), (ii), (iii), and (iv) only are incorrect, as they only include one condition out of the four necessary conditions for a process to be classified as weakly stationary. Option (D) (i), (ii), (iii), and (iv) includes all four necessary conditions and is therefore the correct answer."}
{"question": "Consider the following equation and determine the class of model that it best represents\n\n$y_{it} = \\alpha + \\beta_{it} + \\mu_i + \\nu_{it}$", "choices": [{"label": "A", "text": "An entity fixed effects model"}, {"label": "B", "text": "A time fixed effects model"}, {"label": "C", "text": "A random effects model"}, {"label": "D", "text": "A pure time series model"}], "answerKey": "A", "model_answer": "The correct answer is D: A pure time series model.\n\nThe given equation represents a linear regression model with a random error term. This suggests that the model is a time series model, not a fixed effects model. A time fixed effects model would have a fixed effect for each time period, while a random effects model would incorporate random effects to account for differences among individuals or units.\n\nIn this case, the variables $\\alpha$, $\\beta_{it}$, $\\mu_i$, and $\\nu_{it}$ are not considered fixed effects, but rather the intercept and slope of the regression line. Therefore, the best choice is D: A pure time series model."}
{"question": "Note that statistical tables are not necessary to answer this question. For a sample of 1000 observations, the Dickey-Fuller test statistic values are", "choices": [{"label": "A", "text": "More negative than (i.e. bigger in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "B", "text": "Less negative than (i.e. smaller in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "C", "text": "Obtained from an analytical formula for the density of the Dickey-Fuller distribution"}, {"label": "D", "text": "More negative (i.e. bigger in absolute value) for a 10% size of test than a 5% test."}], "answerKey": "A", "model_answer": "The correct answer is (A) More negative than those in the left hand tail of a normal distribution.\nThe Dickey-Fuller test statistic values are often used to test for unit roots in a time series. A unit root in a time series means that the series has a trend that is not removed by differencing. The Dickey-Fuller test compares the coefficients of the first difference of a time series to the null hypothesis that the cofficients are zero. If the test statistic is less than -2.3 or greater than 2.3, then the null hypothesis can be rejected at the 5% significance level. If the test statistic is between -2.3 and 2.3, then the null hypothesis cannot be rejected and it is considered to be inconsistent.\nThus, the correct answer is (A) More negative than those in the left hand tail of a normal distribution. This means that if the Dickey-Fuller test statistic is obtained from a sample of 1000 observations, then it is more likely that the null hypothesis of no unit roots can be rejected at the 5% significance level than at the 10% significance level."}
{"question": "Suppose that a hypothesis test is conducted using a 5% significance level. Which of the following statements are correct?\n\n(i) The significance level is equal to the size of the test\n\n(ii) The significance level is equal to the power of the test\n\n(iii) 2.5% of the total distribution will be in each tail rejection region for a 2-sided test\n\n(iv) 5% of the total distribution will be in each tail rejection region for a 2-sided test.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv).\n\n1. (i) The significance level is equal to the size of the test: This is incorrect. The significance level (\u03b1) represents the maximum level of risk that the researcher is willing to accept, and it is used to determine the rejection region of the hypothesis test. The size of the test refers to the power of the test, which is the probability of rejecting the null hypothesis when it is false.\n2. (ii) The significance level is equal to the power of the test: This is incorrect. The power of the test represents the probability of rejecting the null hypothesis when it is false, and it is used to determine the rejection region of the hypothesis test. The significance level (\u03b1) represents the maximum level of risk that the researcher is willing to accept, and it is used to determine the rejection region of the hypothesis test.\n3. (iii) 2.5% of the total distribution will be in each tail rejection region for a 2-sided test: This is incorrect. The probability of rejecting the null hypothesis for a 2-sided test is 2.5% (5"}
{"question": "Which one of the following criticisms of the Dickey-Fuller/Engle-Granger approach to dealing with cointegrated variables is overcome by the Engle-Yoo (EY) procedure?", "choices": [{"label": "A", "text": "In the context of small samples, Dickey Fuller tests are prone to conclude that there is a unit root in a series when there is not"}, {"label": "B", "text": "The Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one"}, {"label": "C", "text": "The variables are treated asymmetrically in the cointegrating tests"}, {"label": "D", "text": "It is not possible to perform tests about the cointegrating relationship"}], "answerKey": "D", "model_answer": "The correct answer is (A).\nIn the context of small samples, Dickey Fuller tests are prone to conclude that there is a unit root in a series when there is not. This means that they can be overly sensitive to outliers and small sample sizes, leading to spurious rejections of the null hypothesis. The Engle-Granger approach, however, is designed to be more robust to this issue and is less likely to conclude that there is a unit root in a series when there is not.\n\nThe other choices, (B) The Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one, (C) The variables are treated asymmetrically in the cointegrating tests, and (D) It is not possible to perform tests about the cointegrating relationship, are not criticisms of the Dickey-Fuller/Engle-Granger approach, but rather, points to be considered while using the approach."}
{"question": "Consider a series that follows an MA(1) with zero mean and a moving average coefficient of 0.4. What is the value of the autocovariance at lag 1?", "choices": [{"label": "A", "text": "0.4"}, {"label": "B", "text": "1"}, {"label": "C", "text": "0.34"}, {"label": "D", "text": "It is not possible to determine the value of the autocovariances without knowing the disturbance variance."}], "answerKey": "D", "model_answer": "The correct answer is (A) 0.4.\nThe autocovariance at lag 1 is the correlation between two consecutive periods of the series, given by the formula:\nACov(t,t-1) = E[ut(t)ut(t-1)]\nwhere u(t) is the random variable representing the series at time t, and E[.] denotes the expected value.\nIn this case, the series has an MA(1) structure, which means that the current period's value depends on the previous period's value, but also has a disturbance term (eta) that is independent of the previous period's value. The disturbance term (eta) is assumed to have a mean of zero and a variance of 1, which means the disturbance term does not affect the mean of the series.\nSubstituting these assumptions into the autocovariance formula, we get:\nACov(t,t-1) = E[ut(t)ut(t-1)] = E[u(t)u(t-1)] + E[u(t)eta]E[u(t-1)eta] + E[eta"}
{"question": "Which of the following estimation techniques are available for the estimation of over-identified systems of simultaneous equations?\n\ni) OLS\n\nii) ILS\n\niii) 2SLS\n\niv) IV", "choices": [{"label": "A", "text": "(iii) only"}, {"label": "B", "text": "(iii) and (iv) only"}, {"label": "C", "text": "(ii), (iii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii) and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is (B) (iii) and (iv) only.\n\nOLS (ordinary least squares) is not an estimation technique for over-identified systems of simultaneous equations. OLS is a technique used for estimating the parameters of a linear system of equations, but it assumes that the number of observations is equal to the number of parameters.\n\nILS (instrumental variables) and 2SLS (two-stage least squares) are two estimation techniques that can be used for over-identified systems of simultaneous equations. ILS is used when there are endogenous variables and exogenous variables, and the exogenous variables are not perfectly instrumental. 2SLS is used when there are endogenous variables and exogenous variables, and the exogenous variables are not perfectly instrumental.\n\nTherefore, the correct answer is (B) (iii) and (iv) only, because only (iii) and (iv) are techniques that can be used for estimating over-identified systems of simultaneous equations."}
{"question": "Which one of the following statements best describes a Type II error?", "choices": [{"label": "A", "text": "It is the probability of incorrectly rejecting the null hypothesis"}, {"label": "B", "text": "It is equivalent to the power of the test"}, {"label": "C", "text": "It is equivalent to the size of the test"}, {"label": "D", "text": "It is the probability of failing to reject a null hypothesis that was wrong"}], "answerKey": "D", "model_answer": "The correct answer is (A). A Type II error, also known as a false positive or a false alarm, is the probability of incorrectly rejecting the null hypothesis. It is the opposite of a Type I error, which is the probability of incorrectly not rejecting the null hypothesis (i.e., a false negative or a failure to reject a false null hypothesis).\nThe power of the test is the probability of correctly rejecting the null hypothesis, while the size of the test is the level of significance or the significance level of the test. These are related to the Type I and Type II errors, respectively, but they are not the same as the Type II error.\nSo, the statement that the power of the test is equivalent to the size of the test is not true, and the statement that the size of the test is equivalent to the probability of failing to reject a null hypothesis that was wrong is also not true.\nType II error occurs when a study rejects a true null hypothesis, while Type I error occurs when a study fails to reject a false null hypothesis. In practice, researchers aim to minimize both types of errors and use statistical techniques and methods to control them."}
{"question": "Which one of the following would be the most appropriate as a 95% (two-sided) confidence interval for the intercept term of the model given in question 21?", "choices": [{"label": "A", "text": "(-4.79,2.19)"}, {"label": "B", "text": "(-4.16,4.16)"}, {"label": "C", "text": "(-1.98,1.98)"}, {"label": "D", "text": "(-5.46,2.86)"}], "answerKey": "D", "model_answer": "Based on the question, the most appropriate choice for the 95% confidence interval for the intercept term of the model given in question 21 would be:\n(A) (-4.79,2.19)\nThis range of values represents a 95% confidence interval around the estimated intercept term of the model. The other choices do not provide a valid 95% confidence interval.\n(A) is a range of values that includes the point estimate of the intercept term, which is -4.79, and does not include any other values, making it a valid confidence interval."}
{"question": "Which of the following are characteristics of a stationary process?\n\ni) It crosses its mean value frequently\n\n\nii) It has constant mean and variance\n\n\niii) It contains no trend component\n\n\niv) It will be stationary in first difference form", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "A) (ii) and (iv) only\n\nThe correct answer is A) (ii) and (iv) only. The given question states that a stationary process is a process that has constant mean and variance. Therefore, option A) (ii) only, is correct as it states that a process with constant mean and variance is stationary. Additionally, option A) (iv) only is correct as it states that a stationary process will be stationary in the first difference form. Therefore, the correct answer is A) (ii) and (iv) only."}
{"question": "Consider again the VAR model of equation 16. Which of the following conditions must hold for it to be said that there is bi-directional feedback?", "choices": [{"label": "A", "text": "The b and d coefficients significant and the a and c coefficients insignificant"}, {"label": "B", "text": "The a and c coefficients significant and the b and d coefficients insignificant"}, {"label": "C", "text": "The a and c coefficients significant"}, {"label": "D", "text": "The b and d coefficients significant"}], "answerKey": "D", "model_answer": "The correct answer is B. The condition for bi-directional feedback in the VAR model is that the a and c coefficients are significant and the b and d coefficients are insignificant. This means that the model is able to capture the feedback relationship between two variables, and that the relationship between the two variables is not simply a reflection of the relationship between another variable. In other words, the model is able to capture the causal relationship between the two variables, rather than just a coincidental correlation."}
{"question": "Consider the following sample autocorrelation estimates obtained using 250 data points:\n\n1) Lag 1 2 3\n\n2) Coefficient 0.2 -0.15 -0.1\n\n3) Assuming that the coefficients are approximately normally distributed, which of the coefficients are statistically significant at the 5% level?", "choices": [{"label": "A", "text": "1 only"}, {"label": "B", "text": "1 and 2 only"}, {"label": "C", "text": "1, 2 and 3 only"}, {"label": "D", "text": "It is not possible to determine the statistical significance since no standard errors have been given"}], "answerKey": "B", "model_answer": "The correct answer is (D) It is not possible to determine the statistical significance since no standard errors have been given.\n\nThe question asks for the statistical significance of the coefficients, but no standard errors have been provided, so it is not possible to determine which coefficients are statistically significant. To determine statistical significance, you need to know the standard error of each coefficient, which would tell you the range of values around each coefficient that would include the true population value with a certain level of confidence (in this case, 5%). Without this information, it is impossible to determine which coefficients are statistically significant."}
{"question": "Which one of the following is examined by looking at a goodness of fit statistic?", "choices": [{"label": "A", "text": "How well the population regression function fits the data"}, {"label": "B", "text": "How well the sample regression function fits the population regression function"}, {"label": "C", "text": "How well the sample regression function fits the data"}, {"label": "D", "text": "How well the population regression function fits the sample regression function."}], "answerKey": "C", "model_answer": "The correct answer is (A) How well the population regression function fits the data.\nA goodness of fit test is used to assess how well a model or hypothesis fits the data. In this case, the question is asking about a population regression function, which refers to the underlying relationship between the variables being studied. The test examines how well the population regression function fits the data, not how well the sample regression function fits the population regression function or the other options.\nThe population regression function is the underlying relationship between the variables, and it is used to predict the values of the dependent variable for different values of the independent variable. The goodness of fit statistic measures how well the observed data fits this underlying relationship. It is a measure of how much the observed data deviates from the expected data based on the population regression function.\nTherefore, the correct answer is (A) How well the population regression function fits the data."}
{"question": "Which of the following statements are correct concerning the use of antithetic variates as part of a Monte Carlo experiment?\n\ni) Antithetic variates work by reducing the number of replications required to cover the whole probability space\n\nii) Antithetic variates involve employing a similar variable to that used in the simulation, but whose properties are known analytically\n\niii) Antithetic variates involve using the negative of each of the random draws and repeating the experiment using those values as the draws\n\niv) Antithetic variates involve taking one over each of the random draws and repeating the experiment using those values as the draws", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "The correct answer is: (A) (ii) and (iv) only.\n\nAntithetic variates work by creating multiple sets of random numbers that are opposite in sign and magnitude to each other. By using these antithetic variates in Monte Carlo simulations, the number of replications required to cover the whole probability space can be reduced, and the results can be more accurate and efficient.\n\nThe statement (i) is incorrect, as antithetic variates do not reduce the number of replications required to cover the whole probability space.\n\nThe statement (iii) is incorrect as it involves using the negative of each of the random draws, which is not what antithetic variates do.\n\nThe statement (iv) is incorrect, as it involves taking one over each of the random draws, which is not what antithetic variates do."}
{"question": "Which one of the following statements is true concerning alternative forecast accuracy measures?", "choices": [{"label": "A", "text": "Mean squared error is usually highly correlated with trading rule profitability"}, {"label": "B", "text": "Mean absolute error provides a quadratic loss function"}, {"label": "C", "text": "Mean absolute percentage error is a useful measure for evaluating asset return forecasts"}, {"label": "D", "text": "Mean squared error penalises large forecast errors disproportionately more than small forecast errors"}], "answerKey": "D", "model_answer": "The correct answer is (A). Mean squared error is usually highly correlated with trading rule profitability. This is because it penalizes larger forecast errors more than smaller forecast errors, which can lead to more accurate forecasts."}
{"question": "Which of the following criticisms of standard (\"plain vanilla\") GARCH models can be overcome by EGARCH models?\n\ni) Estimated coefficient values from GARCH models may be negative\n\nii) GARCH models cannot account for leverage effects\n\niii) The responsiveness of future volatility to positive and negative shocks is symmetric under a GARCH formulation\n\niv) GARCH models cannot allow for a feedback from the volatility to the returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is: (D) (i), (ii), (iii), and (iv).\n\nEGARCH models can overcome all four criticisms of standard GARCH models. Specifically, EGARCH models can account for leverage effects and allow for asymmetry in the responsiveness of future volatility to positive and negative shocks. Additionally, EGARCH models can also allow for a feedback from the volatility to the returns.\n\nIn contrast, standard GARCH models may have negative estimated coefficient values, which may not be appropriate for specific datasets. GARCH models may also not be able to account for leverage effects or asymmetric responses to positive and negative shocks. Finally, standard GARCH models may not allow for a feedback from the volatility to the returns."}
{"question": "Suppose that 100 separate firms were tested to determine how many of them \"beat the market\" using a Jensen-type regression, and it is found that 3 fund managers significantly do so. Does this suggest prima facie evidence for stock market inefficiency?", "choices": [{"label": "A", "text": "Yes"}, {"label": "B", "text": "No"}, {"label": "C", "text": "In order to answer this question, you would need to test every fund manager trading in that market"}, {"label": "D", "text": "There is insufficient information given in the question to draw a conclusion about market efficiency."}], "answerKey": "B", "model_answer": "The correct answer is (D) There is insufficient information given in the question to draw a conclusion about market efficiency.\nIt is not clear from the information provided whether the 3 fund managers who significantly beat the market are a representative sample of all fund managers trading in that market. It is also not clear how many firms were tested in total, or whether the firms were a representative sample of all firms trading in the market. Without more information, it is not possible to draw conclusions about market efficiency."}
{"question": "Which of the following are advantages of the use of panel data over pure cross-sectional or pure time-series modelling?\n\n(i) The use of panel data can increase the number of degrees of freedom and therefore the power of tests\n\n(ii) The use of panel data allows the average value of the dependent variable to vary either cross-sectionally or over time or both\n\n(iii) The use of panel data enables the researcher allows the estimated relationship between the independent and dependent variables to vary either cross-sectionally or over time or both", "choices": [{"label": "A", "text": "(i) only"}, {"label": "B", "text": "(i) and (ii) only"}, {"label": "C", "text": "(ii) only"}, {"label": "D", "text": "(i), (ii), and (iii)"}], "answerKey": "B", "model_answer": "Option (D) is the correct answer.\n\nThe advantages of using panel data over pure cross-sectional and/or time-series models are:\n\n(i) Increased power of tests: Panel data models allow for more observations and therefore more degrees of freedom, which can increase the power of statistical tests.\n(ii) Varying relationships: Panel data allows the researcher to estimate the relationship between the independent and dependent variables either cross-sectionally, over time, or both, which is not possible with pure cross-sectional or time-series models.\n(iii) Controlling for heterogeneity: Panel data models can control for within-group heterogeneity, which is not possible with pure cross-sectional or time-series models.\n\nTherefore, the correct answer is (D) all of the above."}
{"question": "If the Engle-Granger test is applied to the residuals of a potentially cointegrating regression, what would be the interpretation of the null hypothesis?", "choices": [{"label": "A", "text": "The variables are cointegrated"}, {"label": "B", "text": "The variables are not cointegrated"}, {"label": "C", "text": "Both variables are stationary"}, {"label": "D", "text": "Both variables are non-stationary"}], "answerKey": "B", "model_answer": "The correct answer is (A) The variables are cointegrated.\nThe Engle-Granger test is a statistical test used to determine if two or more time series are cointegrated, which means that they are long-run equilibrium relationships. If the Engle-Granger test is applied to the residuals of a potentially cointegrating regression and the null hypothesis is rejected, then it means that the variables being tested are cointegrated. This means that there is a long-term relationship between the variables, and they are not random and unrelated to each other.\nThe other options (B) The variables are not cointegrated, (C) Both variables are stationary, and (D) Both variables are non-stationary, are not correct interpretations of the null hypothesis for the Engle-Granger test.\nIn summary, the correct answer is (A) The variables are cointegrated, as this means that the variables being tested have a long-term relationship and are not random or unrelated to each other."}
{"question": "Which of the following statements are true concerning the autocorrelation function (acf) and partial autocorrelation function (pacf)?\n\ni) The acf and pacf will always be identical at lag one whatever the model\n\nii) The pacf for an MA(q) model will in general be non-zero beyond lag q\n\niii) The pacf for an AR(p) model will be zero beyond lag p\n\niv) The acf and pacf will be the same at lag two for an MA(1) model", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "The correct answer is (D) (i), (ii), (iii), and (iv). All of these statements are true concerning the autocorrelation function (ACF) and partial autocorrelation function (PACF) for different types of time series models.\n\ni) The ACF and PACF will always be identical at lag one for any autocorrelation model, including a simple moving average (MA) model with an autoregressive (AR) lag. This is because the PACF is simply the ACF of the residuals from a linear regression of the dependent variable on the lagged values of the dependent variable. If the ACF is identical at lag one, then the PACF will also be identical at lag one.\n\nii) The PACF for an MA(q) model will in general be non-zero beyond lag q. This is because the PACF is a measure of the autocorrelation of the residuals from a linear regression of the dependent variable on the lagged values of the dependent variable, and the lag q values may be significant for the model.\n\niii) The PACF for an AR(p) model will be zero beyond lag p"}
