{"question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is option (B) False, False.\nStatement 1 is false because linear regression is just one type of estimator and there are other unbiased estimators that may have smaller variance, depending on the specific context and problem being solved.\nStatement 2 is also false because the coefficients assigned to the classifiers in AdaBoost can be negative, not just non-negative. AdaBoost is a type of machine learning algorithm that improves the performance of classification models by iteratively adding new, weak classifiers to the ensemble. The coefficients assigned to the classifiers in the final ensemble are the weights assigned to each classifier, and they can be negative if certain classifiers have a negative impact on the overall performance of the ensemble."}
{"question": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "Statement 1: True\nStatement 2: False\n\nThe correct answer is (A) True, True, which means both statements are true.\n\nRoberta pretrains on a corpus that is approximately 10x larger than the corpus BERT pretrained on. Additionally, ResNets in 2018 usually used tanh activation functions."}
{"question": "Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "Statement 1 is true, and statement 2 is false.\nStatement 1 is true because support vector machines (SVMs) do give a probability distribution over the possible labels given an input example. SVMs use a hyperplane to separate the data into two groups, and the hyperplane can be thought of as a decision boundary that separates the data into two classes with different probabilities. This provides a probability distribution over the possible labels.\nStatement 2 is false because we would not expect the support vectors to remain the same when moving from a linear kernel to higher order polynomial kernels. Support vectors are the data points that are closest to the decision boundary, and the decision boundary is determined by the kernel function used. A linear kernel uses a linear decision boundary, while a polynomial kernel uses a non-linear decision boundary that has a higher degree polynomial term. This means that the support vectors will change as we move from a linear kernel to a higher order polynomial kernel, because the decision boundary will change.\nTherefore, the correct answer is (B) False, False."}
{"question": "A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?", "choices": [{"label": "A", "text": "12"}, {"label": "B", "text": "24"}, {"label": "C", "text": "48"}, {"label": "D", "text": "72"}], "answerKey": "D", "model_answer": "The correct answer is (D) 72.\nThe number of maximum possible different examples for a machine learning problem with four attributes and three classes is calculated as follows:\nNumber of possible combinations for each attribute:\n- Attribute 1: 3 x 2 x 2 x 2 = 24\n- Attribute 2: 3 x 2 x 2 x 2 = 24\n- Attribute 3: 3 x 2 x 2 x 2 = 24\n- Attribute 4: 3 x 2 x 2 x 2 = 24\nTotal number of possible combinations: 24 x 24 x 24 x 24 = 72\nTherefore, the maximum possible different examples for this machine learning problem would be 72."}
{"question": "As of 2020, which architecture is best for classifying high-resolution images?", "choices": [{"label": "A", "text": "convolutional networks"}, {"label": "B", "text": "graph networks"}, {"label": "C", "text": "fully connected networks"}, {"label": "D", "text": "RBF networks"}], "answerKey": "A", "model_answer": "The correct answer is (A) convolutional networks.\nConvolutional networks are a type of deep learning architecture that are commonly used for image classification tasks. They are designed to learn features directly from the data, and are particularly well-suited for high-resolution images. They consist of multiple layers of convolutional filters that are applied to the input image, which allows them to extract relevant features and learn complex patterns in the data. Convolutional networks have been shown to perform well on a wide range of image classification tasks, including high-resolution images.\n\nGraph networks, fully connected networks, and radial basis function (RBF) networks are other types of deep learning architectures that can be used for image classification tasks, but they are less commonly used than convolutional networks. Graph networks and fully connected networks are primarily used for tasks other than image classification, such as natural language processing and recommendation systems. RBF networks are a type of neural network that are used for regression tasks, but they are not as commonly used as other types of deep learning architectures for image classification."}
{"question": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is true: The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. This is because the E-step, which involves calculating the likelihood of the data given the current model, will always increase the log-likelihood of the data. The M-step, which involves updating the model based on the data, will also increase the log-likelihood of the data. Therefore, the log-likelihood of the data will always increase through successive iterations of the E-step and M-step.\nStatement 2 is false: One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment. Q-learning is an artificial intelligence method used for reinforcement learning, where the agent learns to make decisions based on the outcomes of its actions. It requires prior knowledge of the environment and the action-value function, which is a function that maps actions to rewards. Without this prior knowledge, Q-learning is not applicable.\nTherefore, the correct answer is (B) False, False."}
{"question": "Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?", "choices": [{"label": "A", "text": "O(D)"}, {"label": "B", "text": "O(N)"}, {"label": "C", "text": "O(ND)"}, {"label": "D", "text": "O(ND^2)"}], "answerKey": "A", "model_answer": "The correct answer is (D) O(ND^2), where N is the number of training examples and D is the number of dimensions in the cost function. The cost of one gradient descent update depends on the size of the gradient vector, which is proportional to the number of training examples and the number of dimensions in the cost function. Therefore, the cost of one gradient descent update is proportional to ND^2, which is the size of the gradient vector multiplied by the number of dimensions in the cost function. The other options are not correct because they do not describe the relationship between the size of the gradient vector and the number of training examples and dimensions in the cost function."}
{"question": "Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is D) False, True.\nStatement 1 is true, as it holds that for a continuous random variable x and its probability distribution function p(x), 0 \u2264 p(x) \u2264 1 for all x.\nStatement 2 is also true, as decision trees are learned by minimizing information gain.\nTherefore, the correct answer is D) False, True."}
{"question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?", "choices": [{"label": "A", "text": "2"}, {"label": "B", "text": "4"}, {"label": "C", "text": "8"}, {"label": "D", "text": "16"}], "answerKey": "C", "model_answer": "The correct answer is (A) 2, since the Bayesian network given below has two independent parameters:\nH -> U\nU -> P\nP -> W\nThere are two possible values for each parameter, which means that there are two independent parameters in total. The Bayesian network does not have eight or sixteen independent parameters, as the choices (B) 4 and (D) 16 are incorrect. The only correct choice is (A) 2, as there are two independent parameters in this Bayesian network."}
{"question": "As the number of training examples goes to infinity, your model trained on that data will have:", "choices": [{"label": "A", "text": "Lower variance"}, {"label": "B", "text": "Higher variance"}, {"label": "C", "text": "Same variance"}, {"label": "D", "text": "None of the above"}], "answerKey": "A", "model_answer": "The correct answer is (D) None of the above.\nAs the number of training examples goes to infinity, the variance of the model's predictions on new, unseen data will not change. This is because the variance of a model's predictions is determined by the size of the training set, which is fixed and cannot be changed. Therefore, it is not affected by the number of training examples.\nAdditionally, as the number of training examples increases, the model's mean will likely become more accurate, but the variance will not change. The mean is a measure of central tendency, which reflects the average performance of the model on the training set. However, the variance is a measure of spread or dispersion, which describes how the model's predictions deviate from the mean on new, unseen data. Increasing the number of training examples may improve the model's mean performance, but it will not change the variance.\nIt is important to note that the number of training examples is a critical factor in determining the accuracy and generalization ability of a machine learning model. However, increasing the number of training examples will not affect the variance of the model's predictions."}
{"question": "Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is true, but statement 2 is false. The VC-dimension of a k-Nearest Neighbor classifier is the minimum number of features needed to tell two points apart with probability > .5. When k = 1, this is also the dimension of the space, and it is finite. However, when k > 1, the VC-dimension can be infinite.\nTherefore, the correct answer is (D) False, True."}
{"question": "_ refers to a model that can neither model the training data nor generalize to new data.", "choices": [{"label": "A", "text": "good fitting"}, {"label": "B", "text": "overfitting"}, {"label": "C", "text": "underfitting"}, {"label": "D", "text": "all of the above"}], "answerKey": "C", "model_answer": "The correct answer is (D) all of the above. The options given are related to different aspects of model performance, but they do not necessarily refer to the model being unable to model the training data or generalize to new data.\nOption (A) \"good fitting\" refers to a model that is well-suited to the data it was trained on, but it does not guarantee that the model can generalize to new data. Similarly, option (B) \"overfitting\" refers to a model that is too complex and fits the training data too well, making it difficult to generalize to new data. Option (C) \"underfitting\" refers to a model that is too simple and does not fit the data well, making it difficult to use for prediction.\nTherefore, the correct answer is (D) all of the above, as any combination of poor performance (e.g., overfitting, underfitting, or bad generalization) could lead to a model that cannot model the training data or generalize to new data."}
{"question": "Statement 1| The F1 score can be especially useful for datasets with class high imbalance. Statement 2| The area under the ROC curve is one of the main metrics used to assess anomaly detectors.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1: The F1 score can be especially useful for datasets with class high imbalance.\nFalse: The F1 score is useful for both balanced and imbalanced datasets.\nStatement 2: The area under the ROC curve is one of the main metrics used to assess anomaly detectors.\nTrue: The area under the ROC curve (AUC) is a widely used metric for evaluating the performance of anomaly detectors.\nTherefore, the correct answer is (D) False, True."}
{"question": "Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is C) True, False.\nStatement 1: The back-propagation algorithm learns a globally optimal neural network with hidden layers.\nStatement 2: The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line.\nThe statement that the VC dimension of a line should be at most 2 is false. In fact, the VC dimension of a line can be arbitrarily large, and there are examples of 3 points that can be shattered by a line of any VC dimension.\nTherefore, the correct answer is C) True, False. Statement 1 is true, but statement 2 is false."}
{"question": "High entropy means that the partitions in classification are", "choices": [{"label": "A", "text": "pure"}, {"label": "B", "text": "not pure"}, {"label": "C", "text": "useful"}, {"label": "D", "text": "useless"}], "answerKey": "B", "model_answer": "The correct answer to the question \"High entropy means that the partitions in classification are\" is (A) pure.\nHigh entropy means that the partitions in a classification are highly diverse and do not fit into any one category. In other words, it means that the partitions are not pure, but rather are a mix of different categories. Therefore, the correct answer is (A) pure.\nNote: This is an example of a question that can be answered by understanding the meaning of the terms involved, and by using the proper context to interpret the question."}
{"question": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (C) True, False.\nStatement 1 is correct: Layer Normalization was introduced in a subsequent paper to the original ResNet paper, but not Batch Normalization.\nStatement 2 is correct: DCGANs use self-attention to stabilize training.\nTherefore, the correct answer is (C) True, False."}
{"question": "In building a linear regression model for a particular data set, you observe the coefficient of one of the features having a relatively high negative value. This suggests that", "choices": [{"label": "A", "text": "This feature has a strong effect on the model (should be retained)"}, {"label": "B", "text": "This feature does not have a strong effect on the model (should be ignored)"}, {"label": "C", "text": "It is not possible to comment on the importance of this feature without additional information"}, {"label": "D", "text": "Nothing can be determined."}], "answerKey": "C", "model_answer": "C. It is not possible to comment on the importance of this feature without additional information.\n\nThis is because a negative coefficient value does not necessarily indicate that the feature has a strong effect on the model. It could also indicate that the feature is negatively correlated with the response variable, meaning that as the value of the feature increases, the response variable decreases. In such a case, the feature might not be considered important to the model.\n\nWithout additional information about the data set and the relationship between the feature and the response variable, it is not possible to accurately determine the importance of the feature. Therefore, option C is the correct answer."}
{"question": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):", "choices": [{"label": "A", "text": "The number of hidden nodes"}, {"label": "B", "text": "The learning rate"}, {"label": "C", "text": "The initial choice of weights"}, {"label": "D", "text": "The use of a constant-term unit input"}], "answerKey": "A", "model_answer": "The correct answer is (C) The initial choice of weights.\nThe initial choice of weights for a neural network has a significant impact on the trade-off between underfitting and overfitting. The initial weights of a neural network are the starting point for the training process, and they can significantly affect the performance of the model. If the initial weights are poorly chosen, the model may overfit the training data and have poor generalization performance on new data. On the other hand, if the initial weights are too poorly chosen, the model may underfit the training data and have poor performance on the test set.\nThe number of hidden nodes, the learning rate, and the use of a constant-term unit input are other important factors that can affect the performance of a neural network, but the initial choice of weights is the most critical."}
{"question": "For polynomial regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:", "choices": [{"label": "A", "text": "The polynomial degree"}, {"label": "B", "text": "Whether we learn the weights by matrix inversion or gradient descent"}, {"label": "C", "text": "The assumed variance of the Gaussian noise"}, {"label": "D", "text": "The use of a constant-term unit input"}], "answerKey": "A", "model_answer": "The correct answer is (C) The assumed variance of the Gaussian noise.\nThe assumption of the variance of the Gaussian noise in polynomial regression is a structural assumption that affects the trade-off between underfitting and overfitting. If the assumed variance of the Gaussian noise is too low, it can lead to overfitting, which means that the model is too complex and over-fits the training data, leading to poor performance on new data. On the other hand, if the assumed variance of the Gaussian noise is too high, it can lead to underfitting, which means that the model is too simple and does not capture the complexity of the data, leading to poor performance on new data.\nTherefore, the assumption of the variance of the Gaussian noise is a crucial structural assumption that affects the trade-off between underfitting and overfitting in polynomial regression."}
{"question": "Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is: (D) False, True.\nStatement 1 is false, as some models may not have achieved greater than 98% accuracy on CIFAR-10 as of 2020. This statement is a generalization and may not be true for all models at all times.\nOn the other hand, Statement 2 is true. The original ResNets were not optimized with the Adam optimizer, which is a stochastic gradient descent (SGD) optimizer that is commonly used for training deep neural networks.\nTherefore, the correct answer is (D) False, True."}
{"question": "The K-means algorithm:", "choices": [{"label": "A", "text": "Requires the dimension of the feature space to be no bigger than the number of samples"}, {"label": "B", "text": "Has the smallest value of the objective function when K = 1"}, {"label": "C", "text": "Minimizes the within class variance for a given number of clusters"}, {"label": "D", "text": "Converges to the global optimum if and only if the initial means are chosen as some of the samples themselves"}], "answerKey": "C", "model_answer": "The correct answer is (C), Minimizes the within class variance for a given number of clusters.\nThe K-means algorithm is a clustering algorithm that partitions a set of data points into K clusters based on their similarity. The K in K-means stands for K-means, which is the name of the algorithm. The K-means algorithm is a type of unsupervised learning algorithm that does not require labeled data.\nThe objective function of the K-means algorithm is to minimize the within-cluster variance, which measures the spread of data points within each cluster. The dimension of the feature space does not necessarily have to be no bigger than the number of samples, and the K-means algorithm does not necessarily converge to the global optimum if and only if the initial means are chosen as some of the samples themselves."}
{"question": "Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (C) True, False.\nStatement 1 is false, as convolutional kernels in VGGNet have larger width and height than the first-layer kernels in AlexNet. This is because VGGNet was designed to be more efficient and to reduce computational complexity, which is achieved by increasing kernel size.\nStatement 2 is also false, as data-dependent weight initialization procedures were introduced after the introduction of Batch Normalization. Batch Normalization was introduced in 2015, and data-dependent weight initialization was introduced in 2019.\nTherefore, the correct answer is (C) True, False."}
{"question": "What is the rank of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]", "choices": [{"label": "A", "text": "0"}, {"label": "B", "text": "1"}, {"label": "C", "text": "2"}, {"label": "D", "text": "3"}], "answerKey": "B", "model_answer": "The correct answer is (B) 1.\nThe rank of a matrix is the number of linearly independent rows it has. In this case, the matrix has three rows and three columns, so it has a rank of 1. Since it has only one non-zero element in each row, it is also said to be a singular matrix.\nA rank of 0 would mean that the matrix is the zero matrix, which has no non-zero elements. A rank of 2 would mean that the matrix has two non-zero elements, and a rank of 3 would mean that the matrix has three non-zero elements.\nIt is important to note that the rank of a matrix depends on the field of scalars, so the rank of this matrix in the field of real numbers is 1, while the rank in the field of complex numbers is 0."}
{"question": "Statement 1| Density estimation (using say, the kernel density estimator) can be used to perform classification. Statement 2| The correspondence between logistic regression and Gaussian Naive Bayes (with identity class covariances) means that there is a one-to-one correspondence between the parameters of the two classifiers.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "Statement 1: False, Density estimation is used to estimate the probability distribution of a dataset, and it does not involve classification.\nStatement 2: True, The KDE (kernel density estimation) and Gaussian Naive Bayes (GNB) classifier share a one-to-one correspondence in their parameters.\nTherefore, the correct answer is option (B) False, False.\nThe first statement is false because density estimation is used to estimate the probability distribution of a dataset, and it does not involve classification. The second statement is true because the KDE and GNB classifier share a one-to-one correspondence in their parameters.\nOption (A) True, True is not correct because the first statement is false, and the second statement is true. Option (C) True, False is not correct because the first statement is false, and the second statement is true. Option (D) False, True is not correct because the first statement is false, and the second statement is true."}
{"question": "Suppose we would like to perform clustering on spatial data such as the geometrical locations of houses. We wish to produce clusters of many different sizes and shapes. Which of the following methods is the most appropriate?", "choices": [{"label": "A", "text": "Decision Trees"}, {"label": "B", "text": "Density-based clustering"}, {"label": "C", "text": "Model-based clustering"}, {"label": "D", "text": "K-means clustering"}], "answerKey": "B", "model_answer": "The correct answer to the question is choice (D) K-means clustering.\nK-means clustering is a widely used unsupervised learning algorithm that partitions a dataset into k clusters based on the similarity of its features. It is particularly well-suited for handling clusters of different sizes and shapes, as it is a hierarchical clustering algorithm that iteratively changes the center of each cluster to maximize the variance within each cluster. Density-based clustering, which is choice (B), is also a good choice for handling spatial data, but it is more sensitive to noise and outliers than K-means clustering, which can handle these issues more effectively. Decision Trees, which is choice (A), is also a good choice for handling spatial data, but it is a more complex and computationally intensive algorithm compared to K-means clustering."}
{"question": "Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false. In AdaBoost, the weights of the misclassified examples do not always go up by the same multiplicative factor. The weights can increase or decrease, depending on the performance of the weak classifier and the performance of the previous weak classifiers.\nStatement 2 is true. In AdaBoost, the weighted training error tends to increase as a function of the iteration number t. This is because the weights are increased for the misclassified examples to improve the performance of the weak classifier, and the weights are increased for the correctly classified examples to prevent overfitting. As a result, the weighted training error tends to increase as a function of t.\nTherefore, the correct answer is (D) False, True."}
{"question": "MLE estimates are often undesirable because", "choices": [{"label": "A", "text": "they are biased"}, {"label": "B", "text": "they have high variance"}, {"label": "C", "text": "they are not consistent estimators"}, {"label": "D", "text": "None of the above"}], "answerKey": "B", "model_answer": "The correct answer is (D) None of the above.\nIt is important to note that there is no definitive answer to this question as it depends on the context in which MLE is being used. In some cases, MLE may be undesirable due to bias, high variance, or lack of consistency, but in other cases, it may be the most appropriate estimator to use.\nOne potential issue with MLE is that it can be sensitive to the choice of prior distribution, which can lead to biased estimates. Additionally, if the likelihood function is not well-defined or if the data is not sufficient to accurately estimate the parameters, the MLE may have high variance or be inconsistent. However, in many cases, MLE is the most appropriate estimator to use and can be a useful tool for making inferences about a population based on a sample of data.\nTherefore, it is important to carefully consider the context and the specific characteristics of the data and likelihood function when deciding whether or not to use MLE."}
{"question": "Computational complexity of Gradient descent is,", "choices": [{"label": "A", "text": "linear in D"}, {"label": "B", "text": "linear in N"}, {"label": "C", "text": "polynomial in D"}, {"label": "D", "text": "dependent on the number of iterations"}], "answerKey": "C", "model_answer": "The correct answer is (C) polynomial in D.\nGradient descent is a type of algorithm used in machine learning to find the optimal values of the model parameters (weights or biases) by minimizing a cost function. The computational complexity of gradient descent depends on the number of data points (N) and the number of model parameters (D). The complexity of gradient descent is O(ND), which means that the algorithm has a time complexity of D operations for each data point, and an additional N operations for the entire dataset. This means that the computational complexity of gradient descent is polynomial in the number of data points and model parameters, rather than linear or dependent on the number of iterations.\n\ubc88\uc5ed\uacb0\uacfc  \n\ub9de\ucda4\ud615 \uad76\uc8fc \uc54c\uace0\ub9ac\uc998\uc740 \ubaa8\ub378 \ud328\ub7ec\ubbf8\ud130(\uc6e8\uc774\ud2b8 \ub610\ub294 \ubca0\uc774\uc544\uc6e8\uc774)\uc758 \ucd5c\uc801 \uac12\uc744 \ucc3e\uae30 \uc704\ud574 \ud568\uc218\ub97c \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud55c \uc54c\uace0\ub9ac\uc998"}
{"question": "Averaging the output of multiple decision trees helps _.", "choices": [{"label": "A", "text": "Increase bias"}, {"label": "B", "text": "Decrease bias"}, {"label": "C", "text": "Increase variance"}, {"label": "D", "text": "Decrease variance"}], "answerKey": "D", "model_answer": "The correct answer is (B) Decrease bias. Averaging the output of multiple decision trees helps to reduce the bias in the predictions made by the trees. Bias occurs when the predictions of the trees differ from the true values, and averaging the outputs can help to reduce this difference and improve the accuracy of the predictions. In contrast, increasing variance would increase the spread or dispersion of the predictions, which would not necessarily help to reduce bias. Choices (A) Increase bias and (D) Decrease variance are incorrect. Choice (C) Increase variance would be correct if the problem were related to variance rather than bias."}
{"question": "The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during", "choices": [{"label": "A", "text": "Best-subset selection"}, {"label": "B", "text": "Forward stepwise selection"}, {"label": "C", "text": "Forward stage wise selection"}, {"label": "D", "text": "All of the above"}], "answerKey": "C", "model_answer": "The correct answer is (A) Best-subset selection.\nBest-subset selection is a feature selection method that selects a subset of the most important features from a large number of features, based on their performance in a model. It is a non-parametric approach to feature selection that does not require the assumption of normality or independence of errors. Best-subset selection searches through all possible subsets of the features and selects the subset that yields the best performance in a model.\nThe other methods you mentioned, such as forward stepwise selection and forward stagewise selection, are also feature selection methods but they are more complex and often have the same performance as best subset selection.\nBest-subset selection is a simple and effective method to identify the relevant features for a model, but it may not be the best method for all situations, and it may not be able to handle high-dimensional data."}
{"question": "Neural networks:", "choices": [{"label": "A", "text": "Optimize a convex objective function"}, {"label": "B", "text": "Can only be trained with stochastic gradient descent"}, {"label": "C", "text": "Can use a mix of different activation functions"}, {"label": "D", "text": "None of the above"}], "answerKey": "C", "model_answer": "The correct answer is (C) Can use a mix of different activation functions. A neural network is a type of machine learning model that is modeled after the human brain. It is made up of layers of artificial neurons that are connected to each other and can process and analyze data. One of the key components of a neural network is the activation function, which is a mathematical function that is applied to each output of a neuron to determine whether the neuron fires or not. Different activation functions can be used in a neural network, which allows for a wider range of functionality and capabilities."}
{"question": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient \u201chas disease D\u201d and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive.", "choices": [{"label": "A", "text": "0.0368"}, {"label": "B", "text": "0.473"}, {"label": "C", "text": "0.078"}, {"label": "D", "text": "None of the above"}], "answerKey": "C", "model_answer": "The correct answer is (A) 0.0368.\nHere's why:\nIn this problem, we are given that the incidence of the disease is 5 cases per 100 people, which means that out of 100 people, on average, 5 of them have the disease. The probability of having the disease is thus 0.05.\nNow, we are asked to find the prior probability of testing positive, which is denoted as P(TP).\nThe prior probability is given as P(TP) = P(D) \\* P(TP | D) + P(~D) \\* P(TP | ~D).\nSince P(D) = 0.05 and P(~D) = 0.95, we need to find P(TP | D) and P(TP | ~D).\nP(TP | D) = 0.99, since the test is very accurate and the probability of testing positive when you have the disease is 0.99.\nP(TP | ~D) = 0.03, since the test is not very accurate and the probability of testing positive when you do"}
{"question": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false. After mapping into a feature space Q through a radial basis kernel function, the 1-NN classifier using unweighted Euclidean distance may not necessarily achieve better classification performance than in the original space. This is because the performance of the classifier depends on the quality of the features in the original space and the feature space, as well as the choice of the kernel function.\nStatement 2 is true. The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM. The VC dimension measures the number of features needed to distinguish between two classes in a classification problem. The VC dimension of a Perceptron is equal to the number of inputs, while the VC dimension of a simple linear SVM is equal to the number of inputs plus one. This means that a Perceptron requires fewer features to classify data than a simple linear SVM."}
{"question": "The disadvantage of Grid search is", "choices": [{"label": "A", "text": "It can not be applied to non-differentiable functions."}, {"label": "B", "text": "It can not be applied to non-continuous functions."}, {"label": "C", "text": "It is hard to implement."}, {"label": "D", "text": "It runs reasonably slow for multiple linear regression."}], "answerKey": "D", "model_answer": "The correct answer for the question \"The disadvantage of grid search is\" is (B) It can not be applied to non-continuous functions.\nGrid search is a method of optimization, which is typically used to find the best parameters for a function. It works by iteratively trying different values for the parameters and choosing the one that gives the best result. The method is applied by using a set of points, known as the grid, to search for the optimal parameters.\nHowever, grid search can not be applied to non-continuous functions, which are functions that are not defined at every point in their domain. Non-continuous functions can not be represented by a continuous function, which is the case in grid search. Therefore, grid search can not be applied to non-continuous functions.\nIt is important to note that the other options (A) It can not be applied to non-differentiable functions, (C) It is hard to implement, and (D) It runs reasonably slow for multiple linear regression, are also correct, but they are not the main disadvantage of grid search."}
{"question": "Predicting the amount of rainfall in a region based on various cues is a ______ problem.", "choices": [{"label": "A", "text": "Supervised learning"}, {"label": "B", "text": "Unsupervised learning"}, {"label": "C", "text": "Clustering"}, {"label": "D", "text": "None of the above"}], "answerKey": "A", "model_answer": "B. Unsupervised learning\n\nThe question describes a problem where the goal is to predict the amount of rainfall in a region based on various cues or inputs. Since the inputs are not labeled or known as true or false, the problem can be considered as unsupervised learning. The answer options given are:\n\nA. Supervised learning: This refers to a type of machine learning where the algorithm is trained on labeled data, where the input and output are known. In this case, the problem description does not fit the definition of a supervised learning problem.\n\nC. Clustering: Clustering is a type of unsupervised learning where the algorithm groups similar data points together based on their features. However, the goal of the problem described is to predict the amount of rainfall, which is a continuous variable.\n\nD. None of the above: This is not the correct answer as the question describes a problem that fits the definition of an unsupervised learning problem."}
{"question": "Which of the following sentence is FALSE regarding regression?", "choices": [{"label": "A", "text": "It relates inputs to outputs."}, {"label": "B", "text": "It is used for prediction."}, {"label": "C", "text": "It may be used for interpretation."}, {"label": "D", "text": "It discovers causal relationships"}], "answerKey": "D", "model_answer": "The correct answer is (C) It may be used for interpretation.\nRegression analysis is a statistical method that is used to analyze the relationship between a dependent variable (the variable we want to predict) and one or more independent variables (the variables we think affect the dependent variable). It is commonly used in data science and statistics to make predictions about the values of the dependent variable based on the values of the independent variables.\nWhile regression analysis can be used for prediction, it is not always used for interpretation. Interpretation refers to the process of gaining a better understanding of the relationship between the variables and the underlying causal mechanisms. However, regression analysis can be used to perform a type of interpretation called causal inference, where we use the estimates of the regression coefficients to make inferences about the effects of the independent variables on the dependent variable.\nTherefore, the sentence \"It discovers causal relationships\" is not entirely true. While regression can be used to make inferences about causal relationships, it does not \"discover\" causal relationships on its own. The interpretation of causal relationships requires additional analysis and assumptions about the underlying causal mechanisms."}
{"question": "Which one of the following is the main reason for pruning a Decision Tree?", "choices": [{"label": "A", "text": "To save computing time during testing"}, {"label": "B", "text": "To save space for storing the Decision Tree"}, {"label": "C", "text": "To make the training set error smaller"}, {"label": "D", "text": "To avoid overfitting the training set"}], "answerKey": "D", "model_answer": "The correct answer is A: To save computing time during testing. Pruning a Decision Tree is a technique to reduce the size of the tree while maintaining its accuracy, which can save computing time during testing. By removing nodes that do not contribute significantly to the accuracy of the tree, the tree becomes smaller and faster to process. This is especially important in applications where the cost of processing is high, such as in real-time systems or on resource-constrained devices."}
{"question": "Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "Statement 1 is true and statement 2 is false.\nStatement 1: The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set.\nThe kernel density estimator is a non-parametric method that estimates the density of a continuous random variable from a sample of its data points. It is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. The goal of the kernel density estimator is to estimate the probability density function of the underlying continuous random variable from the given sample.\nStatement 2: The depth of a learned decision tree can be larger than the number of training examples used to create the tree.\nA decision tree is a tree-like model that is used for classification or regression analysis. The depth of a decision tree refers to the number of levels of branches in the tree. The depth of a learned decision tree can be larger than the number of training examples used to create the tree. This means that the decision tree can have more branches than the number of training examples used to create it. The decision tree can continue to split the data into smaller and smaller subsets until it reaches the"}
{"question": "Suppose your model is overfitting. Which of the following is NOT a valid way to try and reduce the overfitting?", "choices": [{"label": "A", "text": "Increase the amount of training data."}, {"label": "B", "text": "Improve the optimisation algorithm being used for error minimisation."}, {"label": "C", "text": "Decrease the model complexity."}, {"label": "D", "text": "Reduce the noise in the training data."}], "answerKey": "B", "model_answer": "The correct answer is D) Reduce the noise in the training data.\nDuring overfitting, the model becomes too complex and starts to fit the noise in the training data instead of the underlying pattern. To reduce overfitting, one of the most effective ways is to reduce the noise in the training data. This can be done by using techniques such as regularization, data pre-processing, and feature selection.\nA) Increasing the amount of training data can also help reduce overfitting, but it does not address the issue of the model complexity.\nB) Improving the optimisation algorithm can also improve the performance of the model, but it does not directly address the issue of overfitting.\nC) Decreasing the model complexity can also help reduce overfitting, but it does not address the issue of the model becoming too complex."}
{"question": "Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "Statement 1 and Statement 2 are not related to each other.\nStatement 1: The softmax function is commonly used in multiclass logistic regression.\nStatement 2: The temperature of a nonuniform softmax distribution affects its entropy.\nTherefore, the correct answer is:\nThe correct answer is (A) True, True. Both statements are true. The softmax function is commonly used in multiclass logistic regression, and the temperature of a nonuniform softmax distribution does affect its entropy."}
{"question": "Which of the following is/are true regarding an SVM?", "choices": [{"label": "A", "text": "For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line."}, {"label": "B", "text": "In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane."}, {"label": "C", "text": "For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion."}, {"label": "D", "text": "Overfitting in an SVM is not a function of number of support vectors."}], "answerKey": "A", "model_answer": "Option (A) is true.\nA support vector machine (SVM) is a type of machine learning algorithm that can be used for classification or regression tasks. For two-dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line. This means that the data is linearly separable and can be classified using a linear boundary.\nOption (B) is false. A Gaussian kernel SVM can model complex separating hyperplanes, and it can be used to handle non-linearly separable data.\nOption (C) is false. Although kernel functions can be used to transform the data into a higher-dimensional space, this does not necessarily lead to an equivalent basis expansion.\nOption (D) is false. Overfitting in an SVM is not solely dependent on the number of support vectors. Other factors such as the choice of kernel function, the choice of hyperparameters, and the design of the SVM can also impact overfitting."}
{"question": "Which of the following is the joint probability of H, U, P, and W described by the given Bayesian Network H -> U <- P <- W? [note: as the product of the conditional probabilities]", "choices": [{"label": "A", "text": "P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)"}, {"label": "B", "text": "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)"}, {"label": "C", "text": "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)"}, {"label": "D", "text": "None of the above"}], "answerKey": "C", "model_answer": "The correct answer is (A) P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P).\nLet's break down the answer:\nP(H, U, P, W) = P(Event H) * P(Event W | Event H) * P(Event P | Event W, Event H) * P(Event U | Event H, Event P)\nP(H) represents the probability of the event \"H\" happening (i.e., heads)\nP(W | H) represents the probability of the event \"W\" happening given that \"H\" has already occurred (i.e., the probability of getting a \"head\" on a fair coin)\nP(P | W, H) represents the probability of the event \"P\" happening given that \"W\" and \"H\" have already occurred (i.e., the probability of getting the picture \"p\" when \"heads\" and \"tails\" have already occurred)\nP(U | H, P) represents the probability of the event \"U\" happening given that \"H\" and \"P\" have already occurred (i"}
{"question": "Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is false, as the VC dimension for an SVM with a radial base kernel is not infinite, it is finite. The VC dimension of an SVM is determined by the number of hyperplanes used in the model, which can be finite.\nStatement 2 is also false, a two layer neural network with linear activation functions is not essentially a weighted combination of linear separators. The boosting algorithm is a method to find a combination of weak learners, which are linear separators, but it does not necessarily build on linear separators.\nTherefore, the correct answer is (B) False, False."}
{"question": "Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false, as the ID3 algorithm is not guaranteed to find the optimal decision tree. It is a heuristic algorithm that can produce a suboptimal tree.\nStatement 2 is false, as a continuous probability distribution with density f(x) that is nonzero everywhere does not necessarily mean that the probability of a value x is equal to f(x). For example, in the case of a uniform distribution, the probability of any value x is equal to 1/n, where n is the number of possible values.\nTherefore, the correct answer is (D) False, True."}
{"question": "Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum?", "choices": [{"label": "A", "text": "Stochastic Gradient Descent"}, {"label": "B", "text": "Mini-Batch Gradient Descent"}, {"label": "C", "text": "Batch Gradient Descent"}, {"label": "D", "text": "All of the above"}], "answerKey": "D", "model_answer": "The correct answer is (D) All of the above. All of these algorithms can be used to find the global optimum of the neural network with the specified characteristics, but the choice of algorithm will depend on the specific problem and the characteristics of the data."}
{"question": "Adding more basis functions in a linear model, pick the most probably option:", "choices": [{"label": "A", "text": "Decreases model bias"}, {"label": "B", "text": "Decreases estimation bias"}, {"label": "C", "text": "Decreases variance"}, {"label": "D", "text": "Doesn\u2019t affect bias and variance"}], "answerKey": "A", "model_answer": "The correct answer is (A). In a linear model, adding more basis functions can decrease model bias.\nModel bias refers to the difference between the true population parameter and the estimated parameter obtained from the model. Adding more basis functions can improve the accuracy of the model by providing a better fit to the data, which can decrease model bias. However, adding more basis functions can also increase the variance of the model, so it\u2019s important to carefully consider the trade-off between bias and variance when adding more basis functions to a linear model."}
{"question": "Consider the Bayesian network given below. How many independent parameters would we need if we made no assumptions about independence or conditional independence H -> U <- P <- W?", "choices": [{"label": "A", "text": "3"}, {"label": "B", "text": "4"}, {"label": "C", "text": "7"}, {"label": "D", "text": "15"}], "answerKey": "D", "model_answer": "The correct answer is (A) 3.\nIn this Bayesian network, we have H -> U <- P <- W, which means that H is dependent on U, U is dependent on P, and P is dependent on W. The number of independent parameters would be the number of nodes in the network minus one, which is 4 in this case. Therefore, the correct answer is (A) 3.\n\nIf we make no assumptions about independence or conditional independence, then we would not have any information about the relationships between the variables, and we would have to gather more data to determine the relationships. Therefore, the correct answer is (A) 3."}
{"question": "Another term for out-of-distribution detection is?", "choices": [{"label": "A", "text": "anomaly detection"}, {"label": "B", "text": "one-class detection"}, {"label": "C", "text": "train-test mismatch robustness"}, {"label": "D", "text": "background detection"}], "answerKey": "A", "model_answer": "The correct answer is (A) anomaly detection. Anomaly detection is a type of out-of-distribution detection where a model is trained to identify outliers or unusual patterns, also known as anomalies, in the data. This is different from one-class detection, which is a method used to classify data into a single class, such as a background class, rather than a binary class of anomalies versus normal data. Train-test mismatch robustness is related to the difference between the training data and testing data, which can affect the performance of a model. Background detection is not related to out-of-distribution detection and is not a commonly used term."}
{"question": "Statement 1| We learn a classifier f by boosting weak learners h. The functional form of f\u2019s decision boundary is the same as h\u2019s, but with different parameters. (e.g., if h was a linear classifier, then f is also a linear classifier). Statement 2| Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is true. When we use boosting to learn a classifier, we start with a set of weak learners h, and then we use boosting to learn a stronger classifier f using the weights of the weak learners. The functional form of f's decision boundary is the same as h's, but with different parameters, which means that the decision boundaries of f and h will be the same, but with different coefficients.\nStatement 2 is also true. Cross-validation can be used to select the number of iterations in boosting, which can help reduce overfitting. The idea behind cross-validation is to split the data into a training set and a validation set, and then use the training set to train the classifier and use the validation set to evaluate the performance of the classifier. By repeating this process with different numbers of iterations, we can find the number of iterations that results in the best balance between performance and overfitting.\nSo, the correct answer is (D) False, True."}
{"question": "Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (D) False, True, because:\n\nStatement 1: Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions.\nTrue: Highway networks were introduced after ResNets, which are a type of deep neural network.\nTrue: Highway networks eschew max pooling in favor of convolutions, which are a type of operation used in neural networks.\n\nStatement 2: DenseNets usually cost more memory than ResNets.\nFalse: DenseNets usually require more memory than ResNets, but they are not always more memory-intensive.\nTrue: DenseNets are a type of deep neural network that are typically more complex and more memory-intensive than ResNets.\n\nTherefore, the correct answer is (D) False, True."}
{"question": "If N is the number of instances in the training dataset, nearest neighbors has a classification run time of", "choices": [{"label": "A", "text": "O(1)"}, {"label": "B", "text": "O( N )"}, {"label": "C", "text": "O(log N )"}, {"label": "D", "text": "O( N^2 )"}], "answerKey": "B", "model_answer": "The correct answer is (B) O(N).\nThe time complexity of nearest neighbors algorithm depends on the number of instances in the training dataset, N. If N is small, then the algorithm can be implemented in O(1) time, as it only involves searching for the nearest neighbor among a small number of instances. However, as N increases, the time complexity of the algorithm increases rapidly, and it becomes more computationally expensive. For large datasets, the time complexity can be O(N), which is what is described in the choices."}
{"question": "Statement 1| The original ResNets and Transformers are feedforward neural networks. Statement 2| The original Transformers use self-attention, but the original ResNet does not.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1: The original ResNets and Transformers are feedforward neural networks.\nStatement 2: The original Transformers use self-attention, but the original ResNet does not.\nBoth statements are true.\nResNet (Residual Neural Networks) is a type of feedforward neural network, while Transformers are a type of neural network that uses self-attention."}
{"question": "Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false, as RELUs are non-monotonic, meaning they can have multiple local optima.\nStatement 2 is true, as neural networks trained with gradient descent often converge to the global optimum.\nTherefore, the correct answer is (D) False, True."}
{"question": "The numerical output of a sigmoid node in a neural network:", "choices": [{"label": "A", "text": "Is unbounded, encompassing all real numbers."}, {"label": "B", "text": "Is unbounded, encompassing all integers."}, {"label": "C", "text": "Is bounded between 0 and 1."}, {"label": "D", "text": "Is bounded between -1 and 1."}], "answerKey": "C", "model_answer": "The correct answer is (A) Is unbounded, encompassing all real numbers. A sigmoid node in a neural network is a mathematical function that maps input values to output values. The output of a sigmoid node is a probability value, which is generally in the range of 0 to 1. However, the output of a sigmoid node is not bounded between any specific values, such as integers or -1 to 1. It is unbounded and can take on any real number value between 0 and 1."}
{"question": "Which of the following can only be used when training data are linearly separable?", "choices": [{"label": "A", "text": "Linear hard-margin SVM."}, {"label": "B", "text": "Linear Logistic Regression."}, {"label": "C", "text": "Linear Soft margin SVM."}, {"label": "D", "text": "The centroid method."}], "answerKey": "A", "model_answer": "The correct answer is (D) The centroid method.\nLinear SVM can be used to classify linearly separable data, as it uses a linear decision boundary to separate the data into two classes. However, it is not limited to linearly separable data.\nLinear Logistic Regression can also be used to classify linearly separable data, but it is not limited to such data.\nLinear Soft Margin SVM can also be used to classify linearly separable data, but it is not limited to such data.\nThe centroid method, on the other hand, is a clustering technique that is used to group data points that are close to each other in a high-dimensional space. It is not limited to linearly separable data."}
{"question": "Which of the following are the spatial clustering algorithms?", "choices": [{"label": "A", "text": "Partitioning based clustering"}, {"label": "B", "text": "K-means clustering"}, {"label": "C", "text": "Grid based clustering"}, {"label": "D", "text": "All of the above"}], "answerKey": "D", "model_answer": "The correct answer is (D) All of the above.\nSpatial clustering algorithms are a group of algorithms that are used to group geographical data points into clusters based on their spatial proximity. These algorithms can be divided into two main categories: partitioning-based clustering algorithms and density-based clustering algorithms.\nPartitioning-based clustering algorithms, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise), use a grid-based approach to divide the data space into smaller regions, and then group the data points within each region into a cluster. These algorithms require a predefined number of clusters and do not take into account the density of the data points.\nDensity-based clustering algorithms, such as hierarchical clustering, DBSCAN, and OPTICS (Object-Based Image Analysis for Clustering), do not require a predefined number of clusters and instead group data points based on their density. These algorithms can handle different shapes and densities of data points and can be used in different applications.\nTherefore, the answer \"All of the above\" is correct because both partitioning-based clustering algorithms and density-based clustering algorithms can"}
{"question": "Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (A) True, True.\nStatement 1 is correct. SVMs use decision boundaries constructed from a set of hyperplanes that are orthogonal to a set of margin hyperplanes. These decision boundaries have the lowest generalization error among all linear classifiers.\nStatement 2 is correct. Any decision boundary that we get from a generative model with class-conditional Gaussian distributions could potentially be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.\nTherefore, the correct answer is (A) True, True."}
{"question": "Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is false: L2 regularization can make a model more sparse than L1 regularization, but it also can make a model less sparse. L2 regularization tends to encourage small values for the coefficients, which can lead to overfitting.\nStatement 2 is also false: Residual connections cannot be found in ResNets or Transformers. Residual connections involve adding the input data to the output of the previous layer, while Transformers and ResNets are both different architectures for neural networks."}
{"question": "Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?", "choices": [{"label": "A", "text": "P(E, F), P(H), P(E|H), P(F|H)"}, {"label": "B", "text": "P(E, F), P(H), P(E, F|H)"}, {"label": "C", "text": "P(H), P(E|H), P(F|H)"}, {"label": "D", "text": "P(E, F), P(E|H), P(F|H)"}], "answerKey": "B", "model_answer": "The correct answer is (A) P(E, F), P(H), P(E|H), P(F|H).\nThis is because the given information is not sufficient to determine P(H|E, F), which is the probability of H given E and F. We need more information to determine this probability.\nThe missing information is the conditional probabilities P(H|E) and P(F|H), which cannot be determined solely from the given information."}
{"question": "Which among the following prevents overfitting when we perform bagging?", "choices": [{"label": "A", "text": "The use of sampling with replacement as the sampling technique"}, {"label": "B", "text": "The use of weak classifiers"}, {"label": "C", "text": "The use of classification algorithms which are not prone to overfitting"}, {"label": "D", "text": "The practice of validation performed on every classifier trained"}], "answerKey": "B", "model_answer": "The correct answer is (D) The practice of validation performed on every classifier trained."}
{"question": "Statement 1| PCA and Spectral Clustering (such as Andrew Ng\u2019s) perform eigendecomposition on two different matrices. However, the size of these two matrices are the same. Statement 2| Since classification is a special case of regression, logistic regression is a special case of linear regression.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (C) True, False.\nStatement 1 and Statement 2 are both true but they are not related to each other. Eigendecomposition is a mathematical operation that is used in both PCA and Spectral Clustering to perform dimensionality reduction and clustering. However, the size of the matrices used in PCA and Spectral Clustering can be different.\nPCA is a dimensionality reduction technique that reduces the number of features in a dataset by projecting it onto a new lower-dimensional space. It does this by computing the leading eigenvectors of the data matrix and selecting the ones with the largest eigenvalues. The new data matrix will have only the leading eigenvectors and the corresponding eigenvalues.\nSpectral Clustering is a clustering technique that uses eigenvectors and eigenvalues of the similarity matrix to divide the data into clusters. It does this by recursively applying the power iteration algorithm to the similarity matrix to obtain the eigenvalues and eigenvectors.\nThe second statement is also true. Logistic Regression is a special case of Linear Regression, which is a statistical technique used for modeling the relationship between a dependent variable and one or more independent variables.\nLogistic Regression is"}
{"question": "Statement 1| The Stanford Sentiment Treebank contained movie reviews, not book reviews. Statement 2| The Penn Treebank has been used for language modeling.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is true: The Stanford Sentiment Treebank contained movie reviews, not book reviews.\nStatement 2 is also true: The Penn Treebank has been used for language modeling.\nTherefore, the correct answer is (B) False, False."}
{"question": "What is the dimensionality of the null space of the following matrix? A = [[3, 2, \u22129], [\u22126, \u22124, 18], [12, 8, \u221236]]", "choices": [{"label": "A", "text": "0"}, {"label": "B", "text": "1"}, {"label": "C", "text": "2"}, {"label": "D", "text": "3"}], "answerKey": "C", "model_answer": "The correct answer is option (A): 0.\nThe null space of a matrix is the set of vectors that are not in the column space of the matrix. Since the matrix A is not singular, its column space is one-dimensional and spanned by the first column of A. Therefore, the null space of A is the set of all vectors that are orthogonal to the first column of A, which is the zero vector. Since the matrix A is not singular, it does not have a null space. Therefore, the correct answer is option (A): 0."}
{"question": "What are support vectors?", "choices": [{"label": "A", "text": "The examples farthest from the decision boundary."}, {"label": "B", "text": "The only examples necessary to compute f(x) in an SVM."}, {"label": "C", "text": "The data centroid."}, {"label": "D", "text": "All the examples that have a non-zero weight \u03b1k in a SVM."}], "answerKey": "B", "model_answer": "Answer: (C) The data centroid.\nSupport vectors are the examples in a dataset that have a non-zero weight, or coefficient, in a support vector machine (SVM) model. These are the examples that the model considers to be the most important for making predictions. In an SVM model, the support vectors are typically the data points that are closest to the decision boundary, but they can also be further away from the boundary if they have a high weight. The support vectors are used to classify new data points by finding the closest support vector to each new example, and then predicting the class that corresponds to that support vector."}
{"question": "Statement 1| Word2Vec parameters were not initialized using a Restricted Boltzman Machine. Statement 2| The tanh function is a nonlinear activation function.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (A) True, True.\nStatement 1: Word2Vec parameters were not initialized using a Restricted Boltzman Machine.\nFalse: Word2Vec is a type of word embedding model, which uses a neural network to represent words as dense vectors in a high-dimensional space. It does not use a Restricted Boltzmann Machine (RBM) to initialize its parameters.\nStatement 2: The tanh function is a nonlinear activation function.\nTrue: The tanh function is a nonlinear activation function, which is often used in neural networks to introduce nonlinearity to the model. It is commonly used in the hidden layers of neural networks to introduce more complex relationships between the input and output.\nTherefore, the correct answer is (A) True, True."}
{"question": "If your training loss increases with number of epochs, which of the following could be a possible issue with the learning process?", "choices": [{"label": "A", "text": "Regularization is too low and model is overfitting"}, {"label": "B", "text": "Regularization is too high and model is underfitting"}, {"label": "C", "text": "Step size is too large"}, {"label": "D", "text": "Step size is too small"}], "answerKey": "C", "model_answer": "The correct answer is (A) Regularization is too low and model is overfitting.\nRegularization is a technique used to prevent overfitting, and if the regularization is too low or not applied at all, the model may overfit the training data and perform poorly on new, unseen data. In this case, the training loss may increase with the number of epochs, indicating that the model is not able to generalize well to new inputs.\n\nOn the other hand, if regularization is too high or set improperly, the model may underfit the training data and have a low loss, but it may not perform well on new, unseen data. In this case, the training loss may decrease with the number of epochs, indicating that the model is not able to generalize well to new inputs."}
{"question": "Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient \u201chas disease D\u201d and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(D | TP), the posterior probability that you have disease D when the test is positive?", "choices": [{"label": "A", "text": "0.0495"}, {"label": "B", "text": "0.078"}, {"label": "C", "text": "0.635"}, {"label": "D", "text": "0.97"}], "answerKey": "C", "model_answer": "The correct answer is (D) 0.97.\nTo calculate the posterior probability of having disease D when the test is positive, we can use Bayes' theorem, which states:\nP(D | TP) = P(TP | D) \\* P(D) / P(TP)\nP(D | TP) = P(TP | D) \\* P(D) / P(TP)\nP(D | TP) = 0.99 \\* 0.05 / 0.05\nP(D | TP) = 0.985\nSince the correct answer choice is D, the answer must be D.\nTherefore, the correct answer is (D) 0.97."}
{"question": "Statement 1| Traditional machine learning results assume that the train and test sets are independent and identically distributed. Statement 2| In 2017, COCO models were usually pretrained on ImageNet.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false because traditional machine learning models do not necessarily assume the train and test sets are independent and identically distributed (IID). It depends on the specific task and model being used.\nStatement 2 is true because in 2017, many COCO models were pretrained on ImageNet, which is a large-scale dataset of annotated images used for training deep learning models.\nTherefore, the correct answer is (D) False, True."}
{"question": "Statement 1| The values of the margins obtained by two different kernels K1(x, x0) and K2(x, x0) on the same training set do not tell us which classifier will perform better on the test set. Statement 2| The activation function of BERT is the GELU.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (A) True, True.\nStatement 1 is true because the values of the margins obtained by two different kernels K1(x, x0) and K2(x, x0) on the same training set do not necessarily tell us which classifier will perform better on the test set. Different kernels may have different performance on the test set, and the values of the margins obtained by those kernels may not necessarily reflect the performance of the classifier.\nStatement 2 is also true because the activation function of BERT is the GELU."}
{"question": "Which of the following is a clustering algorithm in machine learning?", "choices": [{"label": "A", "text": "Expectation Maximization"}, {"label": "B", "text": "CART"}, {"label": "C", "text": "Gaussian Na\u00efve Bayes"}, {"label": "D", "text": "Apriori"}], "answerKey": "A", "model_answer": "The correct answer is (D) Apriori.\nApriori is a popular algorithm used in association rule mining, which is a technique used in machine learning to find patterns and relationships in large data sets. The Apriori algorithm is a type of clustering algorithm that groups similar items together based on their attributes and frequency of occurrence. It is commonly used in data mining and market basket analysis to identify patterns in customer purchasing behavior.\nExpectation Maximization (EM) is a general technique used in machine learning to estimate the parameters of a model from data. It is not specifically a clustering algorithm. CART (Classification and Regression Trees) is a popular algorithm used in decision tree learning, which is a type of supervised learning algorithm. Gaussian Na\u00efve Bayes is a classification algorithm that uses the Bayes' Theorem to make predictions based on the probabilities of different classes. It is not specifically a clustering algorithm either.\nSo, the correct answer is (D) Apriori."}
{"question": "You've just finished training a decision tree for spam classification, and it is getting abnormally bad performance on both your training and test sets. You know that your implementation has no bugs, so what could be causing the problem?", "choices": [{"label": "A", "text": "Your decision trees are too shallow."}, {"label": "B", "text": "You need to increase the learning rate."}, {"label": "C", "text": "You are overfitting."}, {"label": "D", "text": "None of the above."}], "answerKey": "A", "model_answer": "The correct answer is (C) You are overfitting.\nOverfitting occurs when a model is too complex and fits the training data too well, but does not generalize well to new data. In this case, the decision tree is getting abnormal performance on both the training and test sets, which suggests that it is overfitting. The other options, such as the decision trees being too shallow, do not address the problem of overfitting.\nIt is important to note that overfitting can be caused by a variety of factors, including having too many decision trees in a ensemble model, using too many features, or using a complex algorithm. In this case, the decision tree is the only model used, so the problem is likely caused by overfitting."}
{"question": "K-fold cross-validation is", "choices": [{"label": "A", "text": "linear in K"}, {"label": "B", "text": "quadratic in K"}, {"label": "C", "text": "cubic in K"}, {"label": "D", "text": "exponential in K"}], "answerKey": "A", "model_answer": "The correct answer is (A) linear in K.\nK-fold cross-validation is a technique used in machine learning and statistics to evaluate the performance of a model. It involves splitting the data into K subsets (called \"folds\"), training the model on K-1 of the folds and testing it on the remaining fold, then repeating this process K times with different splits of the data. The average performance of the model can be estimated based on the results of all K splits. The choice of K has an impact on the computation time and the accuracy of the results, but the general idea of the technique remains the same regardless of the choice of K.\nIn this case, the choice of K does not affect the complexity of the computation in a quadratic, cubic, or exponential way. The linear complexity of the computation comes from the fact that the same data is used in all K splits, so the number of operations required is directly proportional to the size of the data. The choice of K does not affect the complexity of the computation in a quadratic, cubic, or exponential way."}
{"question": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (C) True, False.\nStatement 1 is true, because industrial-scale neural networks are typically trained on CPUs, not GPUs. GPUs are more powerful and can handle the large amount of data and computations required for training these networks.\nStatement 2 is false, because while the ResNet-50 model does have over 1 billion parameters, it is typically trained on GPUs, not CPUs. GPUs are specifically designed for training deep neural networks, which require a large amount of memory and computing power.\nTherefore, the correct answer is (C) True, False."}
{"question": "Given two Boolean random variables, A and B, where P(A) = 1/2, P(B) = 1/3, and P(A | \u00acB) = 1/4, what is P(A | B)?", "choices": [{"label": "A", "text": "1/6"}, {"label": "B", "text": "1/4"}, {"label": "C", "text": "3/4"}, {"label": "D", "text": "1"}], "answerKey": "D", "model_answer": "The correct answer is (B) 1/4.\nThe given information is as follows:\nP(A) = 1/2\nP(B) = 1/3\nP(A | \u00acB) = 1/4\nWe can use the formula for conditional probability, which is:\nP(A | B) = P(A \u2229 B) / P(B)\nWe know that P(A) = 1/2, P(B) = 1/3 and P(A | \u00acB) = 1/4, so we can substitute those values into the formula to find P(A | B):\nP(A | B) = P(A \u2229 B) / P(B)\nP(A \u2229 B) = P(A) \u00d7 P(B | A)\nP(A \u2229 B) = 1/2 \u00d7 1/3\nP(A \u2229 B) = 1/6\nP(A | B) = P(A \u2229 B) / P(B)\nP(A | B) = 1/6 / 1/3\nP(A | B) ="}
{"question": "Existential risks posed by AI are most commonly associated with which of the following professors?", "choices": [{"label": "A", "text": "Nando de Frietas"}, {"label": "B", "text": "Yann LeCun"}, {"label": "C", "text": "Stuart Russell"}, {"label": "D", "text": "Jitendra Malik"}], "answerKey": "C", "model_answer": "The correct answer is D) Jitendra Malik.\nMalik is a computer vision researcher and is known for his work in computer vision, especially in the field of machine learning and deep learning. He has written extensively on the topic of existential risks posed by AI and has warned about the potential dangers of developing advanced artificial intelligence systems.\nIt's important to note that while there may be other professors who have also written about the existential risks posed by AI, Jitendra Malik is the one who is most commonly associated with this topic.\nPlease let me know if you have any other question."}
{"question": "Statement 1| Maximizing the likelihood of logistic regression model yields multiple local optimums. Statement 2| No classifier can do better than a naive Bayes classifier if the distribution of the data is known.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false. The likelihood ratio test, which is used in logistic regression, does not guarantee the existence of a unique global optimum.\nStatement 2 is true. Naive Bayes is a simple and effective classifier that performs well on many datasets, but it is not always the best choice.\nTherefore, the correct answer is (D) False, True."}
{"question": "For Kernel Regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:", "choices": [{"label": "A", "text": "Whether kernel function is Gaussian versus triangular versus box-shaped"}, {"label": "B", "text": "Whether we use Euclidian versus L1 versus L\u221e metrics"}, {"label": "C", "text": "The kernel width"}, {"label": "D", "text": "The maximum height of the kernel function"}], "answerKey": "C", "model_answer": "The correct answer is (D) The maximum height of the kernel function.\nThe maximum height of the kernel function refers to the upper limit of the range of values that the kernel function can take. In Kernel Regression, the maximum height of the kernel function affects the trade-off between underfitting and overfitting by determining how much noise can be introduced into the model. If the maximum height of the kernel function is too low, the model will not be able to fit the noise in the data, resulting in underfitting. If the maximum height of the kernel function is too high, the model will be able to fit too much noise, resulting in overfitting.\nTherefore, the maximum height of the kernel function is a critical structural assumption that affects the trade-off between underfitting and overfitting in Kernel Regression."}
{"question": "Statement 1| The SVM learning algorithm is guaranteed to find the globally optimal hypothesis with respect to its object function. Statement 2| After being mapped into feature space Q through a radial basis kernel function, a Perceptron may be able to achieve better classification performance than in its original space (though we can\u2019t guarantee this).", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false, as the SVM learning algorithm is not guaranteed to find the globally optimal hypothesis with respect to its object function. In fact, SVMs can find local optima, which are not necessarily globally optimal.\nStatement 2 is true. After being mapped into feature space Q through a radial basis kernel function, a Perceptron can achieve better classification performance than in its original space. This is because the kernel function can reduce the dimensionality of the input space and improve the generalization ability of the classifier. However, we cannot guarantee that this improvement will occur, as it depends on the specific problem and the size of the feature space.\nTherefore, the correct answer is (D) False, True."}
{"question": "For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:", "choices": [{"label": "A", "text": "Whether we learn the class centers by Maximum Likelihood or Gradient Descent"}, {"label": "B", "text": "Whether we assume full class covariance matrices or diagonal class covariance matrices"}, {"label": "C", "text": "Whether we have equal class priors or priors estimated from the data."}, {"label": "D", "text": "Whether we allow classes to have different mean vectors or we force them to share the same mean vector"}], "answerKey": "B", "model_answer": "The answer is (C), Whether we have equal class priors or priors estimated from the data.\nThe choice of prior distribution for the class priors is a key structural assumption in Bayesian classification, and it can significantly affect the trade-off between underfitting and overfitting. If we have equal class priors, this means we are assuming that all classes have the same prior probability, which can lead to overfitting and difficulty in making accurate predictions. On the other hand, if we use priors estimated from the data, this means we are using the actual distribution of the data to estimate the prior distribution of the classes, which can lead to better generalization and less overfitting.\nIn the context of a Gaussian Bayes classifier, using equal class priors is equivalent to assuming a diagonal class covariance matrix, which is the same as option (B). Option (A) and (D) do not directly relate to the choice of prior distribution for the class priors."}
{"question": "Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "D", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is true, as overfitting is more likely when the set of training data is small. This is because with limited training data, the model has less information to learn from and can become too complex, resulting in overfitting.\nStatement 2 is also true, as overfitting is more likely when the hypothesis space is small. This is because with a limited number of possible hypotheses to choose from, the model has less flexibility to choose from, making it more likely to overfit to the training data.\nTherefore, the correct answer is (D) False, True, which means that both statements are correct.\n\ubc88\uc5ed\uacb0\uacfc  \n\uc62c\ubc14\ub978 \ub2f5\ubcc0\uc740 (D) False, True\uc785\ub2c8\ub2e4.\nStatement 1\uc740 \uc791\uc740 \uc124\uacc4 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc624\ub958 \ud504\ub9ac\ud53d\uc2a4\uac00 \ub354 \ub9ce\uc774 \ubc1c\uc0dd\ud558\ub294 \uac83\uc774 \ufffd"}
{"question": "Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (A) True, True.\nStatement 1 is true: Gradient descent can be used to perform inference or learning on Gaussian mixture models.\nStatement 2 is also true: Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.\nSo, the correct answer is (A) True, True.\nStatement 1 is false: Gradient descent cannot be used to perform inference or learning on Gaussian mixture models.\nStatement 2 is false: A Gaussian-based Bayes optimal classifier can be learned in time that is not linear in the number of records in the dataset.\nSo, the correct answer is (C) True, False.\nStatement 1 is true: Gradient descent can be used to perform inference or learning on Gaussian mixture models.\nStatement 2 is false: A Gaussian-based Bayes optimal classifier cannot be learned in time that is linear in the number of records in the dataset.\nSo, the correct answer is (D) False, True."}
{"question": "Statement 1| In a Bayesian network, the inference results of the junction tree algorithm are the same as the inference results of variable elimination. Statement 2| If two random variable X and Y are conditionally independent given another random variable Z, then in the corresponding Bayesian network, the nodes for X and Y are d-separated given Z.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "The correct answer is (A) True, True.\nStatement 1 is true, as the inference results of the junction tree algorithm are based on the conditional independence assumptions and the structure of the Bayesian network. The junction tree algorithm uses the conditional independence assumptions to prune the search space and find the most probable answer.\nStatement 2 is also true, as in the Bayesian network, two random variables X and Y are conditionally independent given another random variable Z if their joint probability distribution factorizes as P(X, Y|Z)=P(X|Z)P(Y|Z) and P(X, Y)=P(X)P(Y). If X and Y are conditionally independent given Z, then their nodes in the Bayesian network are d-separated from each other and from Z.\nStatements 1 and 2 together imply that the inference results of the junction tree algorithm are the same as the inference results of variable elimination in a Bayesian network."}
{"question": "Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?", "choices": [{"label": "A", "text": "Supervised learning"}, {"label": "B", "text": "Unsupervised learning"}, {"label": "C", "text": "Both (a) and (b)"}, {"label": "D", "text": "Neither (a) nor (b)"}], "answerKey": "B", "model_answer": "The correct answer is (C) Both (a) and (b). This is an example of a hybrid learning problem, which involves both supervised and unsupervised learning. In this case, the dataset of medical records serves as the labeled input, which can be used for supervised learning to identify patterns and make predictions. However, the unsupervised learning aspect comes into play when trying to identify different clusters of heart disease patients, as this requires analyzing the data to identify similarities and differences among the patients. By combining both types of learning, it is possible to gain a more comprehensive understanding of the data and make more accurate predictions and decisions."}
{"question": "What would you do in PCA to get the same projection as SVD?", "choices": [{"label": "A", "text": "Transform data to zero mean"}, {"label": "B", "text": "Transform data to zero median"}, {"label": "C", "text": "Not possible"}, {"label": "D", "text": "None of these"}], "answerKey": "A", "model_answer": "The correct answer is (A) Transform data to zero mean.\nPCA (Principal Component Analysis) is a linear dimensionality reduction method that is commonly used to reduce the dimensionality of a dataset by finding the directions that best capture the underlying variance or structure in the data. One way to do this is to calculate the eigenvectors and eigenvalues of the covariance matrix of the data, which can be thought of as the directions of maximum variance or direction of maximum variance, respectively.\nTo get the same projection as SVD (Singular Value Decomposition), which is another widely used dimensionality reduction method, you can use the same steps as PCA, but with a different set of eigenvectors and eigenvalues. Specifically, you can use the left singular vectors from the SVD to project the data onto the new coordinate system.\nOption A, transforming the data to zero mean, is one way to achieve this, as it is equivalent to scaling the data so that the mean is zero, which is the same as the SVD method. Option B, transforming the data to zero median, would not achieve the same result because the median is not a meaningful zero value, and Option C, \"not possible,\" is incorrect because PCA"}
{"question": "Statement 1| The training error of 1-nearest neighbor classifier is 0. Statement 2| As the number of data points grows to infinity, the MAP estimate approaches the MLE estimate for all possible priors. In other words, given enough data, the choice of prior is irrelevant.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "Statement 1 is true, statement 2 is false.\nWhen it comes to the training error of a 1-nearest neighbor classifier, it is always non-zero because the classifier will always make some mistakes when it predicts the class of a new example based on its neighbors.\nRegarding statement 2, it is false. The maximum a posteriori (MAP) estimate and the maximum likelihood (ML) estimate are not the same when it comes to the choice of prior. The choice of prior can affect the MAP estimate and the ML estimate, and it can be shown that the two estimates can diverge when the prior is not proper.\nTherefore, the correct answer is D) False, True."}
{"question": "When doing least-squares regression with regularisation (assuming that the optimisation can be done exactly), increasing the value of the regularisation parameter \u03bb the testing error.", "choices": [{"label": "A", "text": "will never decrease the training error."}, {"label": "B", "text": "will never increase the training error."}, {"label": "C", "text": "will never decrease the testing error."}, {"label": "D", "text": "will never increase"}], "answerKey": "A", "model_answer": "The correct answer is option (C): Will never decrease the testing error.\nThis statement is true because in regularized least squares regression, the regularization term is added to the loss function during the optimization process. The regularization parameter \u03bb controls the amount of regularization, and increasing \u03bb will lead to a larger penalty term being added to the loss function, which will tend to reduce the magnitude of the coefficients in the model. This can result in better generalization performance, which means the model will perform well on unseen data, including the testing set. As a result, the testing error will decrease as the regularization parameter \u03bb increases.\nNote that this statement does not necessarily imply that the training error will always increase with \u03bb. The training error can increase if the regularization term becomes too large and the model becomes overly complex, which may lead to worse performance on the testing set. The statement is only true when the regularization term is used appropriately and the model is appropriately regularized."}
{"question": "Which of the following best describes what discriminative approaches try to model? (w are the parameters in the model)", "choices": [{"label": "A", "text": "p(y|x, w)"}, {"label": "B", "text": "p(y, x)"}, {"label": "C", "text": "p(w|x, w)"}, {"label": "D", "text": "None of the above"}], "answerKey": "A", "model_answer": "The correct answer is (A) p(y|x, w). Discriminative approaches try to model the probability distribution of the target variable (y) given the input variable (x) and the parameters (w) in the model. This is done by maximizing the likelihood of the observed data given the model parameters. The probability distribution p(y|x, w) is estimated by the maximum likelihood estimator (MLE) which is the parameter set w that maximizes the likelihood of the observed data given the model."}
{"question": "Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "The correct answer is (D) False, True.\nStatement 1: CIFAR-10 classification performance for convolutional neural networks (CNN) can exceed 95%.\nStatement 2: Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.\nThe statement (A) True, True is incorrect because it is a possibility that both statements are true.\nThe statement (B) False, False is incorrect because it is a possibility that both statements are false.\nThe statement (C) True, False is incorrect because it contradicts the statement (A) and (B) cannot be inferred from the given statements.\nThe statement (D) False, True is correct because it contradicts the statement (A) and CNN can indeed achieve high classification performance."}
{"question": "Which of the following points would Bayesians and frequentists disagree on?", "choices": [{"label": "A", "text": "The use of a non-Gaussian noise model in probabilistic regression."}, {"label": "B", "text": "The use of probabilistic modelling for regression."}, {"label": "C", "text": "The use of prior distributions on the parameters in a probabilistic model."}, {"label": "D", "text": "The use of class priors in Gaussian Discriminant Analysis."}], "answerKey": "C", "model_answer": "The correct answer is (A) The use of a non-Gaussian noise model in probabilistic regression.\n\nBayesian and frequentist approaches to statistics can differ on how they approach problems and make inferences, but they are not inherently opposed to each other. In fact, they can be used together in the same problem to obtain complementary results. The choice (A) is an example of a point where Bayesian and frequentist approaches might disagree, as frequentists typically use Gaussian noise models while Bayesians are more likely to use non-Gaussian noise models. This is because non-Gaussian noise models can better capture the complexities of real-world data, while Gaussian noise models can be overly simplistic in certain situations."}
{"question": "Statement 1| The BLEU metric uses precision, while the ROGUE metric uses recall. Statement 2| Hidden markov models were frequently used to model English sentences.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is correct: BLEU is a metric that uses precision, while ROUGE is a metric that uses recall.\nStatement 2 is also correct: Hidden markov models were frequently used to model English sentences.\nTherefore, the correct answer is (B) False, False."}
{"question": "Statement 1| ImageNet has images of various resolutions. Statement 2| Caltech-101 has more images than ImageNet.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "The correct answer is (A) True, True. This is because both statements are true.\nStatement 1: ImageNet has images of various resolutions.\nStatement 2: Caltech-101 has more images than ImageNet.\nSince ImageNet has images of various resolutions, it can be inferred that ImageNet has images of different sizes and resolutions. Additionally, since Caltech-101 has more images than ImageNet, it can be inferred that Caltech-101 has more images than ImageNet. Therefore, both statements are true."}
{"question": "Which of the following is more appropriate to do feature selection?", "choices": [{"label": "A", "text": "Ridge"}, {"label": "B", "text": "Lasso"}, {"label": "C", "text": "both (a) and (b)"}, {"label": "D", "text": "neither (a) nor (b)"}], "answerKey": "B", "model_answer": "The correct answer is (D) neither (a) nor (b). Neither Lasso nor Ridge is appropriate for feature selection.\nLasso is a linear model which is used for regression problems, and it is not suitable for feature selection. Feature selection is a non-linear process, and it is not within the scope of Lasso.\nRidge is a regularization technique used for both regression and classification, and it is also not suitable for feature selection. Feature selection is a different process from regularization, and it is not within the scope of Ridge.\nBoth Lasso and Ridge are used for model selection and parameter tuning, and they are not appropriate for feature selection. Feature selection is a separate process from model selection and parameter tuning, and it is not within the scope of Lasso or Ridge.\nTherefore, the correct answer is neither (a) nor (b), which means that neither Lasso nor Ridge is appropriate for feature selection."}
{"question": "Suppose you are given an EM algorithm that finds maximum likelihood estimates for a model with latent variables. You are asked to modify the algorithm so that it finds MAP estimates instead. Which step or steps do you need to modify?", "choices": [{"label": "A", "text": "Expectation"}, {"label": "B", "text": "Maximization"}, {"label": "C", "text": "No modification necessary"}, {"label": "D", "text": "Both"}], "answerKey": "B", "model_answer": "The correct answer is (C) No modification necessary. The EM algorithm is designed to find maximum likelihood estimates for a model with latent variables. By definition, maximum likelihood estimates are the same as maximum a posteriori (MAP) estimates. Therefore, no modification is necessary to the algorithm to find MAP estimates."}
{"question": "For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:", "choices": [{"label": "A", "text": "Whether we learn the class centers by Maximum Likelihood or Gradient Descent"}, {"label": "B", "text": "Whether we assume full class covariance matrices or diagonal class covariance matrices"}, {"label": "C", "text": "Whether we have equal class priors or priors estimated from the data"}, {"label": "D", "text": "Whether we allow classes to have different mean vectors or we force them to share the same mean vector"}], "answerKey": "B", "model_answer": "The correct answer is (D) Whether we allow classes to have different mean vectors or we force them to share the same mean vector.\nThe structural assumption of a Gaussian Bayes classifier that most affects the trade-off between underfitting and overfitting is whether the classes have the same mean vector or not. If the classes have the same mean vector, the model is said to be linearly separable, and it will be easier to overfit the data. If the classes have different mean vectors, the model is said to be linearly non-separable, and it will be harder to overfit the data.\nThe other choices (A) whether we learn the class centers by Maximum Likelihood or Gradient Descent, (B) whether we assume full class covariance matrices or diagonal class covariance matrices, and (C) whether we have equal class priors or priors estimated from the data, do not directly impact the trade-off between underfitting and overfitting."}
{"question": "Statement 1| For any two variables x and y having joint distribution p(x, y), we always have H[x, y] \u2265 H[x] + H[y] where H is entropy function. Statement 2| For some directed graphs, moralization decreases the number of edges present in the graph.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "B", "model_answer": "Statement 1 is true.\nStatement 2 is false.\nThe moralization of a directed graph is the process of replacing each edge with two parallel edges, such that the number of edges in the moralized graph is always at least as large as the number of edges in the original graph. This statement is false because there are directed graphs with fewer edges in the moralized graph than in the original graph.\nTherefore, the correct answer is:\n(D) False, True."}
{"question": "Which of the following is NOT supervised learning?", "choices": [{"label": "A", "text": "PCA"}, {"label": "B", "text": "Decision Tree"}, {"label": "C", "text": "Linear Regression"}, {"label": "D", "text": "Naive Bayesian"}], "answerKey": "A", "model_answer": "The correct answer is (C) Linear Regression.\nLinear regression is a type of supervised learning, which means that it is used to learn from labeled data. In supervised learning, the algorithm is trained on a dataset with labeled examples, and it is able to predict the output value for new, unseen data. Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It is a common technique used in data analysis and is often used in machine learning models.\nThe other options you listed (A) PCA, (B) Decision Tree, and (D) Naive Bayesian) are also types of supervised learning, which means that they are used to learn from labeled data and can be used to predict the output value for new, unseen data.\nPCA (Principal Component Analysis) is a method used to reduce the dimensionality of a dataset by projecting it onto a lower-dimensional subspace. It is often used as a pre-processing step in data analysis and is not typically used to make predictions.\nDecision Trees are a type of machine learning algorithm that is used to make predictions by building a tree-like model of possible outputs based on the input"}
{"question": "Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is false. While the learning rate can affect the convergence of a neural network, it is not the only factor that influences convergence. There are many other variables that can affect convergence, such as the size and complexity of the network, the choice of activation functions, and the choice of optimization algorithm.\nStatement 2 is true. Dropout is a technique used in deep learning to prevent overfitting by randomly setting a certain percentage of neurons' activation values to zero during training. This helps to prevent the network from memorizing the training data and instead focuses on generalization.\nTherefore, the correct answer is (D) False, True."}
{"question": "Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?", "choices": [{"label": "A", "text": "P(A | B) * P(B | C) * P(C | A)"}, {"label": "B", "text": "P(C | A, B) * P(A) * P(B)"}, {"label": "C", "text": "P(A, B | C) * P(C)"}, {"label": "D", "text": "P(A | B, C) * P(B | A, C) * P(C | A, B)"}], "answerKey": "C", "model_answer": "The correct answer is (B) P(C | A, B) * P(A) * P(B).\n\nThe probability of A, B, and C jointly, given the Boolean random variables A, B, and C, is:\n\nP(A, B, C)\n\n= P(A | B, C) * P(B | A, C) * P(C | A, B)\n\n= P(A | B, C) * P(B) * P(C)\n\n= P(A | B, C) * P(B) * P(C)\n\n= P(A | B, C) * P(B)\n\n= P(A | B) * P(B | C) * P(C | A)\n\n= P(A | B) * P(B) * P(C)\n\n= P(A | B) * P(B)\n\n= P(A | B)\n\nP(A) = P(A | B) * P(B) + P(A | \u00acB) * P(\u00acB)\n\n= P(A | B)"}
{"question": "Which of the following tasks can be best solved using Clustering.", "choices": [{"label": "A", "text": "Predicting the amount of rainfall based on various cues"}, {"label": "B", "text": "Detecting fraudulent credit card transactions"}, {"label": "C", "text": "Training a robot to solve a maze"}, {"label": "D", "text": "All of the above"}], "answerKey": "B", "model_answer": "The correct answer is (A) Predicting the amount of rainfall based on various cues. Clustering is a technique used for grouping similar items together, but it is not necessarily the best approach for all tasks. Clustering can be used for tasks such as detecting fraudulent credit card transactions, but it may not be the most effective way to predict the amount of rainfall based on various cues. It may be more appropriate to use other techniques, such as regression analysis or time series analysis, to predict rainfall."}
{"question": "After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?", "choices": [{"label": "A", "text": "L0 norm"}, {"label": "B", "text": "L1 norm"}, {"label": "C", "text": "L2 norm"}, {"label": "D", "text": "either (a) or (b)"}], "answerKey": "D", "model_answer": "The correct answer is (B) L1 norm, also known as the absolute value penalty. The L1 norm is used to penalize large coefficients by forcing them to be smaller. This is in contrast to the L2 norm, which penalizes all coefficients equally, and the L0 norm, which penalizes all coefficients. The L2 norm is not a choice in this question, and the L0 norm would be used to force all coefficients to be zeroed out."}
{"question": "A and B are two events. If P(A, B) decreases while P(A) increases, which of the following is true?", "choices": [{"label": "A", "text": "P(A|B) decreases"}, {"label": "B", "text": "P(B|A) decreases"}, {"label": "C", "text": "P(B) decreases"}, {"label": "D", "text": "All of above"}], "answerKey": "B", "model_answer": "The correct answer is (D) All of the above.\nWhen P(A, B) decreases while P(A) increases, it means that the joint probability of both events decreases, while the individual probability of A increases. This means that all of the following are true:\n1. P(A, B) decreases.\n2. P(A|B) decreases.\n3. P(B|A) decreases.\n4. P(B) decreases.\n\nTherefore, the correct answer is (D) All of the above."}
{"question": "Statement 1| When learning an HMM for a fixed set of observations, assume we do not know the true number of hidden states (which is often the case), we can always increase the training data likelihood by permitting more hidden states. Statement 2| Collaborative filtering is often a useful model for modeling users' movie preference.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (B) False, False.\nStatement 1 is correct and states that if you are learning an HMM for a fixed set of observations and do not know the true number of hidden states, increasing the number of hidden states can improve the training data likelihood.\nStatement 2 is also correct and states that collaborative filtering is often a useful model for modeling users' movie preferences.\nTherefore, the correct combination of the two statements is (B) False, False.\nStatement 1 is correct and states that if you are learning an HMM for a fixed set of observations and do not know the true number of hidden states, increasing the number of hidden states can improve the training data likelihood.\nStatement 2 is also correct and states that collaborative filtering is often a useful model for modeling users' movie preferences.\nTherefore, the correct combination of the two statements is (B) False, False."}
{"question": "You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\\ell_2$ regularization to penalize the weights. As you increase the $\\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?", "choices": [{"label": "A", "text": "Bias increase ; Variance increase"}, {"label": "B", "text": "Bias increase ; Variance decrease"}, {"label": "C", "text": "Bias decrease ; Variance increase"}, {"label": "D", "text": "Bias decrease ; Variance decrease"}], "answerKey": "B", "model_answer": "The correct answer is (C) Bias decrease ; Variance decrease.\nWhen adding $\\ell_2$ regularization to a linear regression model, the regularization term penalizes the weights and encourages them to be small. This means that the model will tend to over-fit the training data less and make predictions that are less biased. However, as the regularization coefficient increases, the model will also tend to generalize less well and make more errors on new, unseen data. This is because the regularization term also encourages the weights to be small, which means that the model will not use all of the available information in the training data to make predictions.\nTherefore, as the regularization coefficient increases, the bias of the model will decrease, while the variance will also decrease. This means that the model will be less biased overall, but will also be less accurate on new, unseen data. This trade-off is a common issue in model selection and is often referred to as the \"regularization penalty trade-off.\""}
{"question": "Which PyTorch 1.8 command(s) produce $10\\times 5$ Gaussian matrix with each entry i.i.d. sampled from $\\mathcal{N}(\\mu=5,\\sigma^2=16)$ and a $10\\times 10$ uniform matrix with each entry i.i.d. sampled from $U[-1,1)$?", "choices": [{"label": "A", "text": "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{torch.rand(10,10,low=-1,high=1)}"}, {"label": "B", "text": "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{(torch.rand(10,10) - 0.5) / 0.5}"}, {"label": "C", "text": "\\texttt{5 + torch.randn(10,5) * 4} ; \\texttt{2 * torch.rand(10,10) - 1}"}, {"label": "D", "text": "\\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \\texttt{2 * torch.rand(10,10) - 1}"}], "answerKey": "C", "model_answer": "The correct answer is (A) \\texttt{5 + torch.randn(10,5) * 16}.\nThis command creates a matrix with shape (10,5) and each entry i.i.d. sampled from $\\mathcal{N}(\\mu=5,\\sigma^2=16)$, which means the expected value of each entry is 5 and the standard deviation is 16. The command uses the \\texttt{randn} function from PyTorch to generate a random matrix with normal distribution, and multiplies it by 16 to set the mean and standard deviation of the matrix.\nPlease note that the other options (B), (C) and (D) produce different matrices, and none of them create the matrix with shape (10,5) and mean 5 and standard deviation 16 as specified in the question."}
{"question": "Statement 1| The ReLU's gradient is zero for $x<0$, and the sigmoid gradient $\\sigma(x)(1-\\sigma(x))\\le \\frac{1}{4}$ for all $x$. Statement 2| The sigmoid has a continuous gradient and the ReLU has a discontinuous gradient.", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "A", "model_answer": "The correct answer is (D) False, True.\nStatement 1 is true because the ReLU function satisfies $f(x) = \\max(0, x)$ for all $x$. This means that the gradient of the ReLU function is zero for all $x < 0$, as the input must be non-negative for the output to be positive.\n\nStatement 2 is false because the sigmoid function $\\sigma(x) = 1 / (1 + e^{-x})$ has a continuous gradient, while the ReLU function has a discontinuous gradient.\n\nTherefore, the correct answer is D) False, True."}
{"question": "Which is true about Batch Normalization?", "choices": [{"label": "A", "text": "After applying batch normalization, the layer\u2019s activations will follow a standard Gaussian distribution."}, {"label": "B", "text": "The bias parameter of affine layers becomes redundant if a batch normalization layer follows immediately afterward."}, {"label": "C", "text": "The standard weight initialization must be changed when using Batch Normalization."}, {"label": "D", "text": "Batch Normalization is equivalent to Layer Normalization for convolutional neural networks."}], "answerKey": "B", "model_answer": "The correct answer is (A). After applying batch normalization, the layer's activations will follow a standard Gaussian distribution. This means that the activations of the layer will be centered around zero and have a normal distribution, which is a desirable property for most machine learning tasks."}
{"question": "Suppose we have the following objective function: $\\argmin_{w} \\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\gamma \\norm{w}^2_2$ What is the gradient of $\\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\lambda \\norm{w}^2_2$ with respect to $w$?", "choices": [{"label": "A", "text": "$\\nabla_w f(w) = (X^\\top X + \\lambda I)w - X^\\top y + \\lambda w$"}, {"label": "B", "text": "$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda$"}, {"label": "C", "text": "$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda w$"}, {"label": "D", "text": "$\\nabla_w f(w) = X^\\top X w - X^\\top y + (\\lambda+1) w$"}], "answerKey": "C", "model_answer": "The correct answer is (C) $\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda w$.\n\nThe objective function is given by:\n\n$\\argmin_{w} \\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\gamma \\norm{w}^2_2$\n\nWe need to find the gradient of this objective function with respect to $w$.\n\nThe derivative of the first term can be calculated as:\n\n$\\frac{\\partial \\frac{1}{2} \\norm{Xw-y}^2_2}{\\partial w}$\n= $\\frac{1}{2} \\norm{Xw-y}^2_2 \\frac{\\partial X^\\top X}{\\partial w}$\n= $\\frac{1}{2} X^\\top X w - X^\\top y$\n\nThe derivative of the second term can be calculated as:\n\n$\\frac{\\partial \\frac{1}{2}\\gamma \\norm{w}^2_2}{\\partial w}$\n= $\\gamma \\norm{w}^2_2 \\frac{\\partial \\norm{w}^2"}
{"question": "Which of the following is true of a convolution kernel?", "choices": [{"label": "A", "text": "Convolving an image with $\\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ would not change the image"}, {"label": "B", "text": "Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image"}, {"label": "C", "text": "Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image"}, {"label": "D", "text": "Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image"}], "answerKey": "B", "model_answer": "C) Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image.\nA convolution kernel is a matrix that is used in the convolution operation in image processing. The convolution operation is the process of multiplying the kernel matrix by the pixel values of an image to produce a new image that represents the result of the convolution. The output image will be a function of the kernel matrix and the input image, and the kernel matrix determines the shape of the output image.\nIn the given question, the choices given are all valid kernel matrices that could be used to convolve an image. However, only choice C, $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$, is a kernel matrix that would not change the image. A kernel matrix that has all ones in the first column (which corresponds to the output channel) will produce output pixel values that are equal to the input pixel values, and therefore will not change the image. On the other hand, if the kernel matrix has all"}
{"question": "Which of the following is false?", "choices": [{"label": "A", "text": "Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image."}, {"label": "B", "text": "A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive."}, {"label": "C", "text": "When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive."}, {"label": "D", "text": "A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative."}], "answerKey": "D", "model_answer": "B is the false statement. A bounding box with an IoU equal to 96% would likely be considered a true positive, not false negative."}
{"question": "Which of the following is false?", "choices": [{"label": "A", "text": "The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices."}, {"label": "B", "text": "Leaky ReLU $\\max\\{0.01x,x\\}$ is convex."}, {"label": "C", "text": "A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex."}, {"label": "D", "text": "The loss $\\log \\sigma(x)= -\\log(1+e^{-x})$ is concave"}], "answerKey": "C", "model_answer": "The correct answer is (C).\nA combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is not convex.\nA ReLU function is a non-linear activation function that maps each input $x$ to either $0$ or $x$ depending on whether $x$ is positive or negative. A combination of two ReLU functions will not be convex, because the output of one function will not affect the input of the other function. However, a combination of ReLU functions with a sum or product operator could be convex, depending on the specific form of the operator."}
{"question": "We are training fully connected network with two hidden layers to predict housing prices. Inputs are $100$-dimensional, and have several features such as the number of square feet, the median family income, etc. The first hidden layer has $1000$ activations. The second hidden layer has $10$ activations. The output is a scalar representing the house price. Assuming a vanilla network with affine transformations and with no batch normalization and no learnable parameters in the activation function, how many parameters does this network have?", "choices": [{"label": "A", "text": "111021"}, {"label": "B", "text": "110010"}, {"label": "C", "text": "111110"}, {"label": "D", "text": "110011"}], "answerKey": "A", "model_answer": "The correct answer is:\nThe number of parameters in this fully connected network with two hidden layers is:\nC(100, 1000) + C(100, 10) + C(100, 10) + C(100, 1) = 111021"}
{"question": "Statement 1| The derivative of the sigmoid $\\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\\text{Var}(B)$ where $B\\sim \\text{Bern}(\\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases", "choices": [{"label": "A", "text": "True, True"}, {"label": "B", "text": "False, False"}, {"label": "C", "text": "True, False"}, {"label": "D", "text": "False, True"}], "answerKey": "C", "model_answer": "The correct answer is (C) True, False.\nStatement 1 is true because the derivative of the sigmoid function with respect to $x$ is equal to the variance of the Bernoulli random variable $B\\sim \\text{Bern}(\\sigma(x))$.\nStatement 2 is false because setting the bias parameters in each layer of a neural network to 0 does not necessarily change the bias-variance trade-off; it only changes the bias and not the variance.\nTherefore, the correct answer is (C) True, False."}
