{"question": "Which one of the following is the most appropriate definition of a 99% confidence interval?", "choices": [{"label": "A", "text": "99% of the time in repeated samples, the interval would contain the true value of the parameter"}, {"label": "B", "text": "99% of the time in repeated samples, the interval would contain the estimated value of the parameter"}, {"label": "C", "text": "99% of the time in repeated samples, the null hypothesis will be rejected"}, {"label": "D", "text": "99% of the time in repeated samples, the null hypothesis will not be rejected when it was false"}], "answerKey": "A", "model_answer": "LECTURE: A confidence interval is a range of values that is calculated to contain the true value of a parameter.\nFor example, suppose you want to estimate the average height of dogs in a breed. You take multiple measurements and calculate the average height, and then calculate a confidence interval for the average height. The confidence interval will contain the true average height 95% of the time.\nOne way to define a confidence interval is in terms of the confidence level. A 99% confidence interval, for example, would contain the true value of the parameter 99% of the time.\nHere are three ways a 99% confidence interval could be defined:\nA. A 99% confidence interval is a range of values that, in 99% of the samples, will contain the true height of the breed.\nB. A 99% confidence interval is a range of values that, in 99% of the samples, will contain the estimated height of the breed.\nC. A 99% confidence interval is a range of values for which, if the null hypothesis is true, the interval will contain the population proportion 99% of the time.\nD. A 99% confidence interval is a range of values that, in 99% of the samples, will contain the null hypothesis when it is false."}
{"question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?", "choices": [{"label": "A", "text": "ADF is a single equation approach to unit root testing while PP is a systems approach"}, {"label": "B", "text": "PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test"}, {"label": "C", "text": "The PP test incorporates an automatic correction for autocorrelated residuals in the test regression"}, {"label": "D", "text": "PP tests have good power in small samples whereas DF tests do not."}], "answerKey": "C", "model_answer": "SOLUTION: The main difference between the Dickey-Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing is that the PP test incorporates an automatic correction for autocorrelated residuals in the test regression.\nThe DF test relies on the assumption that the residuals in the test regression are not autocorrelated. This assumption is fulfilled if the series is stationary. However, the series may not be stationary if it is highly autocorrelated. In this case, the DF test may not detect a unit root.\nThe PP test addresses this limitation of the DF test by incorporating an automatic correction for autocorrelated residuals in the test regression. This correction ensures that the residuals in the test regression are not autocorrelated, even if the series is highly autocorrelated."}
{"question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?", "choices": [{"label": "A", "text": "It would rise more quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "B", "text": "It would be symmetrical about zero"}, {"label": "C", "text": "It would rise less quickly for negative disturbances than for positive ones of the same magnitude"}, {"label": "D", "text": "It would be zero for all positive disturbances"}], "answerKey": "A", "model_answer": "LECTURE: The shape of the news impact curve reflects the degree to which the news affects the stock market. The shape of the curve can be affected by the nature of the news.\nIf there were a leCTURE: The shape of the news impact curve reflects the degree to which the news affects the stock market. The shape of the curve can be affected by the nature of the news.\nIf there were a negative news article, the stock might not fall immediately. It could take some time for investors to react to the news. So, the news impact curve for negative news might rise more slowly at first and then rise more quickly later on.\nIf there were a positive news article, the stock might not rise immediately. It could take some time for investors to react to the news. So, the news impact curve for positive news might rise more slowly at first and then rise more quickly later on.\nIf there were a news article that was both positive and negative, the stock might not react much at first. But as investors absorbed the news, the stock might rise or fall. So, the news impact curve for mixed news might be symmetrical.\nThe news impact curve is zero for all positive disturbances. This means that the stock does not move until the positive news is absorbed by the market. At that point, the stock might rise sharply."}
{"question": "Which of the following statements is false concerning the linear probability model?", "choices": [{"label": "A", "text": "There is nothing in the model to ensure that the estimated probabilities lie between zero and one"}, {"label": "B", "text": "Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one"}, {"label": "C", "text": "The error terms will be heteroscedastic and not normally distributed"}, {"label": "D", "text": "The model is much harder to estimate than a standard regression model with a continuous dependent variable"}], "answerKey": "D", "model_answer": "LECTURE: A linear probability model is a model of the form:\nP(Y=1|X=x) = f(x)\nwhere P is the probability of a binary outcome, X is an n-dimensional vector of predictors, and f is a link function. In this model, the predicted probability of a successful outcome is a linear function of the predictors.\nOne common predictor in binary response models is the covariate itself. This is an example of a linear probability model:\nP(Y=1|X=x) = f(x)\nwhere X is an n-dimensional vector of predictors, and f is a link function.\nIn a heteroscedastic linear probability model, the uncertainty (or error) around the predicted probability is constant. This is different from a homoscedastic linear probability model, in which the uncertainty is proportional to the distance of the predictor from its mean.\nSOLUTION: The error terms in a linear probability model are not normally distributed. Instead, the error terms are constrained to have values between 0 and 1. As a result, the error terms are not normally distributed.\nA common choice for the link function in a linear probability model is the logit function. For this choice, the error terms are normally distributed. But, if you use a different link function, the error terms will not be normally distributed.\nEven if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one. This is because the error terms are not normally distributed. So, many of the estimated probabilities will be at the boundaries of the allowable values.\nThe model is much harder to estimate than a standard regression model with a continuous dependent variable. This is because the predicted probabilities are constrained to have values between 0 and 1. As a result, the model includes more constraints.\nThere is nothing in the model to ensure that the estimated probabilities lie between zero and one. This means that the estimated probabilities could be outside of the allowable range."}
{"question": "Which of the following statements concerning the regression population and sample is FALSE?", "choices": [{"label": "A", "text": "The population is the total collection of all items of interest"}, {"label": "B", "text": "The population can be infinite"}, {"label": "C", "text": "In theory, the sample could be larger than the population"}, {"label": "D", "text": "A random sample is one where each individual item from the population is equally likely to be drawn."}], "answerKey": "C", "model_answer": "SOLUTION: The population is the total collection of all items of interest.\nThe sample is a subset of the population.\nIn theory, the sample could be larger than the population.\nA random sample is one where each individual item from the population is equally likely to be drawn.\nThe following statement concerns the sample, not the population: \"In theory, the sample could be larger than the population.\"."}
{"question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?", "choices": [{"label": "A", "text": "Asymptotically, the values of the two test statistics will be equal"}, {"label": "B", "text": "The Q test has better small-sample properties than the Q*"}, {"label": "C", "text": "The Q test is sometimes over-sized for small samples"}, {"label": "D", "text": "As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients."}], "answerKey": "B", "model_answer": "SOLUTION: The Q test and the Q* test are used to test the null hypothesis that a time series has zero autocorrelations. The Q test is sometimes over-sized for small samples. This means that the Q test may reject the null hypothesis even when the true null hypothesis is not rejected. As a result, the Q test may indicate that there is a higher degree of linear dependence in a time series than actually exists.\nThe Ljung-Box Q* test is a modification of the Box-Pierce Q test that is intended to correct for this problem. The Ljung-Box Q* test is not subject to this problem and is considered to be more powerful than the Box-Pierce Q test for small sample sizes.\nIn some situations, the values of the two test statistics will be equal. This is possible when the true null hypothesis is that there is zero autocorrelation in the time series. In this case, both tests will show a tendency to always reject the null hypothesis as the sample size tends towards infinity."}
{"question": "A parsimonious model is one that", "choices": [{"label": "A", "text": "Includes too many variables"}, {"label": "B", "text": "Includes as few variables as possible to explain the data"}, {"label": "C", "text": "Is a well-specified model"}, {"label": "D", "text": "Is a mis-specified model"}], "answerKey": "A", "model_answer": "LECTURE: A parsimonious model is one that includes as few variables as possible to explain the data.\nMany variables can be added to a model to get a good fit. However, including more variables can make the model more complicated and harder to understand. Including too many variables can also make the model more susceptible to overfitting, which occurs when a model is overly tailored to the data it is analyzing.\nSo, a parsimonious model is a simple model that uses as few variables as possible to explain the data.\nSOLUTION: A well-specified model is a model that uses clear and unambiguous variables. This means that each variable in the model is clearly defined and measured.\nA mis-specified model is a model that uses ambiguous, unclear, or unmeasureable variables. This can include variables that are not clearly defined, or variables that are measured in a way that is not consistent with their definition.\nExamples of ambiguous variables include things like \"rich\" or \"beautiful.\" It is not clear how to measure these variables, so they cannot be included in a model.\nModels can also be mis-specified when variables are measured in ways that are not consistent with their definition. For example, if a variable is defined as the age of a car, but the age is measured in years and months, the model is mis-specified."}
{"question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?", "choices": [{"label": "A", "text": "They can be interpreted as continuously compounded changes in the prices"}, {"label": "B", "text": "They can be added over time to give returns for longer time periods"}, {"label": "C", "text": "They can be added across a portfolio of assets to give portfolio returns"}, {"label": "D", "text": "They are usually fat-tailed"}], "answerKey": "C", "model_answer": "SOLUTION: Continuously compounded returns are changes in log-returns. Log-returns are calculated as the difference between the logarithm of an asset's price at two different points in time. The calculation of continuously compounded returns involves taking the difference between the logarithm of an asset's price at different points in time, and then taking the exponential of that difference. As a result, continuously compounded returns can be interpreted as continuously compounded changes in the prices of assets.\nContinuously compounded returns can be added over time to give returns for longer time periods. For example, if an asset has a continuously compounded return of 10% for a year, and then a continuously compounded return of 5% for the next year, the total continuously compounded return for the two-year period is 10% + 5% = 15%.\nContinuously compounded returns can be added across a portfolio of assets to give portfolio returns. For example, if a portfolio of assets has a continuously compounded return of 10% for a year, and then a continuously compounded return of 5% for the next year, the total continuously compounded return for the two-year period is 10% + 5% = 15%.\nFat-tailed distributions have heavier tails than normal distributions. In other words, there are more extreme outcomes in fat-tailed distributions than in normal distributions. So, fat-tailed distributions are more likely to have returns that are much higher or much lower than the average return than normal distributions. Log-returns are fat-tailed. So, continuously compounded returns are usually fat-tailed."}
{"question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "LECTURE: A financial asset's return time-series is a sequence of random variables representing the asset's returns over time. The features of a financial asset's return time-series can be used to describe the asset's behavior.\nFinancial asset return time-series have the following features:\nSOLUTION: A GARCH model can capture the following features of a financial asset's return time-series:\nvolatility clustering, which describes the tendency of returns to cluster around their mean.\nA GARCH model with a value of 1 for the ARCH term can capture volatility affecting returns, which describes the tendency of returns to be more volatile after large changes.\nA GARCH model can capture leCTURE: A financial asset's return time-series is a sequence of random variables representing the asset's returns over time. The features of a financial asset's return time-series can be used to describe the asset's behavior.\nFinancial asset return time-series have the following features:\nSOLUTION: A GARCH model can capture the following features of a financial asset's return time-series:\nvolatility clustering, which describes the tendency of returns to cluster around their mean.\nA GARCH model with a value of 1 for the ARCH term can capture volatility affecting returns, which describes the tendency of returns to be more volatile after large changes.\nA GARCH model can capture leCTURE: A financial asset's return time-series is a sequence of random variables representing the asset's returns over time. The features of a financial asset's return time-series can be used to describe the asset's behavior.\nFinancial asset return time-series have the following features:\nSOLUTION: A GARCH model can capture the following features of a financial asset's return time-series:\nvolatility clustering, which describes the tendency of returns to cluster around their mean.\nA GARCH model with a value of 1 for the ARCH term can capture volatility affecting returns, which describes the tendency of returns to be more volatile after large changes.\nA GARCH model can capture the following:\nthe volatility of returns affects how much the returns deviate from their mean."}
{"question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?", "choices": [{"label": "A", "text": "Less than -1"}, {"label": "B", "text": "Between -1 and 0"}, {"label": "C", "text": "Between 0 and 1"}, {"label": "D", "text": "Bigger than 1"}], "answerKey": "C", "model_answer": "GARCH models have three main components: a linear component, a cyclical component, and a constant component. The constant component is represented by the GARCH-in-mean parameter. The GARCH-in-mean parameter estimates the size of the constant component.\nThe constant component is typically less than 1. So, the value of the GARCH-in-mean parameter estimate should be between -1 and 0."}
{"question": "Under which of the following situations would bootstrapping be preferred to pure simulation?\n\ni) If it is desired that the distributional properties of the data in the experiment\n\nare the same as those of some actual data\n\n\nii) If it is desired that the distributional properties of the data in the experiment\n\nare known exactly\n\n\niii) If the distributional properties of the actual data are unknown\n\n\niv) If the sample of actual data available is very small", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: Bootstrapping can be used to estimate the distribution of a statistic when the data are small or when the sample is not representative of the population. In these cases, using pure simulation may not be practical.\nPure simulation can be used when the data are large and a computer can be used to simulate the experiment.\nIn the first three situations, it is desired that the distributional properties of the data in the experiment are known exactly, and bootstrapping can be used to estimate the distribution of a statistic.\nIn the fourth situation, the sample of actual data available is very small, and bootstrapping can be used to estimate the distribution of a statistic."}
{"question": "Which of the following may be consequences of one or more of the CLRM assumptions being violated?\n\ni) The coefficient estimates are not optimal\n\n\nii) The standard error estimates are not optimal\n\n\niii) The distributions assumed for the test statistics are inappropriate\n\n\niv) Conclusions regarding the strength of relationships between the dependent\n\nand independent variables may be invalid.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": ""}
{"question": "Which of the following statements is true concerning forecasting in econometrics?", "choices": [{"label": "A", "text": "Forecasts can only be made for time-series data"}, {"label": "B", "text": "Mis-specified models are certain to produce inaccurate forecasts"}, {"label": "C", "text": "Structural forecasts are simpler to produce than those from time series models"}, {"label": "D", "text": "In-sample forecasting ability is a poor test of model adequacy"}], "answerKey": "D", "model_answer": "SOLUTION: Forecasting is an important part of econometric analysis. Here are some common steps in the forecasting process:\n1. Specify a model: Choose a model that has a certain degree of simplicity and a certain degree of complexity. The model should be specified such that it can be estimated using the available data.\n2. Estimate the model: Use the available data to estimate the parameters of the model.\n3. Assistant: Test the model: Use statistical tests to assess the adequacy of the model.\n4. Use the model to forecast: Use the estimated model to make predictions about future outcomes.\n5. Evaluate the forecasts: Assistantistant: LECTURE: In-sample forecasting ability is a poor test of model adequacy.\nWhen econometricians evaluate a model, they often use in-sample forecasting ability as a test of the model's adequacy. This is a poor test for several reasons.\nFirst, in-sample forecasting ability does not necessarily indicate how well the model will forecast out-of-sample.\nSecond, in-sample forecasting ability does not take into account the fact that the model was designed for the data. A model that is designed for the data will have high in-sample forecasting ability, but may not be flexible enough to forecast well for other data.\nThird, in-sample forecasting ability does not take into account the fact that the model is a point estimate. A model may have high in-sample forecasting ability, but will likely have a high degree of uncertainty.\nThere are several ways to improve forecasting ability.\nOne way is to use out-of-sample testing. Out-of-sample testing involves using the model to make forecasts about data that were not used to estimate the model. Out-of-sample testing is a better test of forecasting ability than in-sample forecasting ability.\nAnother way is to use a model that is internally valid. An internally valid model is a model that is well-specified for the data. A model that is well-specified for the data will have high forecasting ability."}
{"question": "The pacf is necessary for distinguishing between", "choices": [{"label": "A", "text": "An AR and an MA model"}, {"label": "B", "text": "An AR and an ARMA model"}, {"label": "C", "text": "An MA and an ARMA model"}, {"label": "D", "text": "Different models from within the ARMA family"}], "answerKey": "B", "model_answer": "SOLUTION: Models from within the ARMA family can use both autocorrelated and moving-averaged errors. An AR and an MA model are two separate models. An ARMA model is a combination of an AR and an MA model."}
{"question": "Negative residual autocorrelation is indicated by which one of the following?", "choices": [{"label": "A", "text": "A cyclical pattern in the residuals"}, {"label": "B", "text": "An alternating pattern in the residuals"}, {"label": "C", "text": "A complete randomness in the residuals"}, {"label": "D", "text": "Residuals that are all close to zero"}], "answerKey": "B", "model_answer": "LECTURE: Residuals are the differences between the observed values and the predicted values of a variable. Residuals can show how well the model fits the data.\nOne type of pattern that can show up in the residuals is autocorrelation. Autocorrelation is a statistical term that describes the relationship between a variable and its own lags. In other words, autocorrelation tells you how one variable is related to other values of the same variable at other points in time.\nThere are two main types of autocorrelation: positive autocorrelation and negative autocorrelation. Positive autocorrelation indicates that the variable is higher in the past than it is in the present. This type of autocorrelation is also called a trend.\nNegative autocorrelation indicates that the variable is lower in the past than it is in the present. This type of autocorrelation is also called a cycle.\nSOLUTION: Negative residual autocorrelation is indicated by a cyclical pattern in the residuals. The residuals show a pattern of getting smaller, staying small for a while, and then getting larger again."}
{"question": "Which of the following statements are true concerning a comparison between ARCH(q) and GARCH(1,1) models?\n\ni) The ARCH(q) model is likely to be the more parsimonious\n\n\nii) The ARCH(q) model is the more likely to violate non-negativity constraints\n\n\niii) The ARCH(q) model can allow for an infinite number of previous lags of squared\n\nreturns to affect the current conditional variance\n\n\niv) The GARCH(1,1) model will usually be sufficient to capture all of the dependence\n\nin the conditional variance", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: The GARCH(1,1) model includes the ARCH(1) model as a special case. So, the ARCH(1) model is likely to be true.\nThe ARCH(1) model can allow for an infinite number of previous lags of squared returns to affect the current conditional variance.\nThe GARCH(1,1) model will usually be sufficient to capture all of the dependence in the conditional variance.\nThe ARCH(q) model is the more likely to violate non-negativity constraints.\nThe ARCH(q) model is likely to be the more parsimonious."}
{"question": "Which of the following statements are true concerning a triangular or recursive system?\n\ni) The parameters can be validly estimated using separate applications of OLS to\n\neach equation\n\n\nii) The independent variables may be correlated with the error terms in other\n\nequations\n\n\niii) An application of 2SLS would lead to unbiased but inefficient parameter estimates\n\n\niv) The independent variables may be correlated with the error terms in the equations\n\nin which they appear as independent variables", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: A triangular or recursive system is a set of equations where one or more equations depend on the parameters estimated in other equations.\nConsider the following system of equations:\nyi = \u03b20i + \u03b21x1i + \u03b22x2i + \u03b5i\nyi = \u03b20i + \u03b21x1i + \u03b5i\nyi = \u03b20i + \u03b21x1i + \u03b22x2i + \u03b5i\nwhere yi, x1i, and x2i are the dependent variables, the independent variables, and the error terms, respectively. The parameter \u03b22x2i is estimated in the second equation, and all three parameters are estimated in the third equation.\nSo, the parameters can be validly estimated using separate applications of OLS to each equation.\nThe independent variables may be correlated with the error terms in other equations. So, the independent variables may be correlated with the error terms in the equations in which they appear as independent variables.\nAn application of 2SLS would lead to unbiased but inefficient parameter estimates. So, it is true."}
{"question": "Which one of the following statements best describes the algebraic representation of the fitted regression line?", "choices": [{"label": "A", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}, {"label": "B", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t"}, {"label": "C", "text": "\\hat{y}_t = \\hat{\\alpha} + \\hat{\\beta}x_t + u_t"}, {"label": "D", "text": "y_t = \\hat{\\alpha} + \\hat{\\beta}x_t + \\hat{u}_t"}], "answerKey": "B", "model_answer": "LECTURE: The algebraic representation of a linear regression model is given by the equation\ny\\_t = \\beta_0 + \\beta_1x\\_t + u\\_t,\nwhere y\\_t is the output, x\\_t is the input, \\beta\\_0 is the intercept, \\beta\\_1 is the slope, and u\\_t is the error at time t.\nThe \"fitted\" regression line is the line that passes as closely as possible through the points of the data.\nThe equation of the fitted regression line is given by the function\n\\hat{y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\_t + \\hat{u}_t.\nSubstituting the equation for the error, \\hat{u}_t = \\beta\\_0 + \\beta\\_1x\\_t - \\hat{y}_t, into the equation for the fitted regression line, we get\n\\hat{y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\_t + (\\beta\\_0 + \\beta\\_1x\\_t - \\hat{y}_t).\nSimplifying, we get\n\\hat{y}_t = \\hat{\\beta}_0 + 2\\hat{\\beta}_1x\\_t - \\hat{y}_t.\nSolving for \\hat{y}\\_t, we get\n\\hat{y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\_t.\nThis is the same as the equation of the original regression line. It does not contain the term \\hat{u}\\_t.\nTherefore, the algebraic representation of the fitted regression line is \\hat{y}\\_t = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\_t."}
{"question": "What are the dimensions of $\\hat{u}^t \\hat{u}?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "D", "model_answer": "SOLUTION: The matrix \ud835\udf0c is a 10 x 10 matrix and \ud835\udf0c^T is a 10 x 10 matrix. Since \ud835\udf0c^T \ud835\udf0c is a 10 x 10 matrix, the dimensions of \ud835\udf0c^T \ud835\udf0c are T x T.\nThis means that the dimensions of \ud835\udf0c^T \ud835\udf0c are k x k.\nSo, the dimensions of \ud835\udf0c^T \ud835\udf0c are k x k, not T x T or k x 1 or 1 x 1."}
{"question": "The characteristic roots of the MA process\n\n$y_t = -3u_{t-1} + u_{t-2} + u_t$\n\nare", "choices": [{"label": "A", "text": "1 and 2"}, {"label": "B", "text": "1 and 0.5"}, {"label": "C", "text": "2 and -0.5"}, {"label": "D", "text": "1 and -3"}], "answerKey": "B", "model_answer": "SOLUTION: The characteristic roots of the MA process are the values of \u03bb that make the polynomial 0. For the MA process, the polynomial is:\n$(\\lambda - 1)(\\lambda - 2)(\\lambda - 3) = \\lambda^3 - 3\u03bb^2 + 2\u03bb - 3 = 0$\nSo, the characteristic roots are 1, 2, and 3."}
{"question": "Which of the following is an equivalent expression for saying that the explanatory variable is \"non-stochastic\"?", "choices": [{"label": "A", "text": "The explanatory variable is partly random"}, {"label": "B", "text": "The explanatory variable is fixed in repeated samples"}, {"label": "C", "text": "The explanatory variable is correlated with the errors"}, {"label": "D", "text": "The explanatory variable always has a value of one"}], "answerKey": "B", "model_answer": "LECTURE: In a regression problem, the goal is to predict the output variable y. The explanatory variables are x1, x2, ..., xn. One of the assumptions of regression is that the explanatory variables are not independent of the output variable. This is called the exclusion principle.\nOne way to understand the exclusion principle is that it implies that the explanatory variables can help predict the output variable. In other words, knowing the values of the explanatory variables can help predict the value of the output variable.\nAnother way to understand the exclusion principle is that it implies that the explanatory variables are not random. If the explanatory variables were random, they would be unpredictable. This would prevent them from being able to help predict the output variable.\nSOLUTION: An expression is partly random if the value of the explanatory variable is determined partly by chance. So, the explanatory variable is partly random if its value is not totally determined by the other variables.\nThe explanatory variable is fixed in repeated samples if its value is the same in all similar situations. So, the explanatory variable is not partly random if it is fixed in repeated samples.\nThe explanatory variable is correlated with the errors if the value of the explanatory variable tends to increase as the value of the errors increases. So, the explanatory variable is not non-stochastic if it is correlated with the errors.\nThe explanatory variable is always of a certain value (e.g., always equal to one) if its value is the same for all observations. So, the explanatory variable is not non-stochastic if it is always of a certain value."}
{"question": "Suppose that the Durbin Watson test is applied to a regression containing two explanatory variables plus a constant with 50 data points. The test statistic takes a value of 1.53. What is the appropriate conclusion?", "choices": [{"label": "A", "text": "Residuals appear to be positively autocorrelated"}, {"label": "B", "text": "Residuals appear to be negatively autocorrelated"}, {"label": "C", "text": "Residuals appear not to be autocorrelated"}, {"label": "D", "text": "The test result is inconclusive"}], "answerKey": "D", "model_answer": "LECTURE: The Durbin Watson test is a statistical test for testing for the presence of autocorrelation in the residuals of a regression. Autocorrelation occurs when a variable's residual value is correlated with its own previous values. This can indicate that there are problems with the model, such as that it is not correctly specifying the correct number of parameters.\nThe Durbin Watson test statistic is a test statistic that looks at the differences between adjacent residuals. If the test statistic is less than 0, then the residuals appear to be negatively autocorrelated. If the test statistic is between 0 and 4, then the residuals appear to be not autocorrelated. And if the test statistic is greater than 4, then the residuals appear to be positively autocorrelated.\nSOLUTION: The test statistic is 1.53, which is greater than 4. So, the residuals appear to be positively autocorrelated."}
{"question": "If OLS is used in the presence of autocorrelation, which of the following will be likely consequences?\n\ni) Coefficient estimates may be misleading\n\n\nii) Hypothesis tests could reach the wrong conclusions\n\n\niii) Forecasts made from the model could be biased\n\n\niv) Standard errors may inappropriate", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "LECTURE: Autocorrelation is a type of correlation that occurs when the value of a variable depends on its own past values. This means that the variable is not independent of its own past.\nOne example of a variable that is autocorrelated is the variable Y in the model Y = \u03b20 + \u03b21X + \u03b5, where \u03b5 is white noise. In this case, the value of Y at any given time t depends on the value of Y at previous times.\nWhen an ordinary least squares (OLS) regression is used in the presence of autocorrelation, the coefficient estimates may be biased, which means that they may not reflect the true relationship between the variables. The standard errors may also be inappropriate, which means that the hypothesis tests and forecasts may be affected.\nSOLUTION: The consequences of using OLS in the presence of autocorrelation include:\n* Coefficient estimates may be biased.\n* Standard errors may be inappropriate.\n* Forecasts made from the model could be biased.\n* Hypothesis tests could reach the wrong conclusions.\nSo, the consequences of using OLS in the presence of autocorrelation include all of the following: i). iv)."}
{"question": "What will be the properties of the OLS estimator in the presence of multicollinearity?", "choices": [{"label": "A", "text": "It will be consistent, unbiased and efficient"}, {"label": "B", "text": "It will be consistent and unbiased but not efficient"}, {"label": "C", "text": "It will be consistent but not unbiased"}, {"label": "D", "text": "It will not be consistent"}], "answerKey": "A", "model_answer": "SOLUTION: When there is multicollinearity, the ordinary least squares (OLS) estimator can still be consistent, but it is no longer guaranteed to be unbiased or efficient.\nConsistency: A consistent estimator is one that, as the size of the data set increases, gets closer and closer to the true population parameter. The OLS estimator is consistent when there is multicollinearity.\nUnbiasedness: A biased estimator is one that systematically over- or underestimates the true population parameter. The OLS estimator can still be unbiased when there is multicollinearity.\nEfficiency: An efficient estimator is one that has the lowest possible variance for a given level of bias. The OLS estimator is not efficient when there is multicollinearity."}
{"question": "Which one of the following would NOT be a consequence of using non-stationary data in levels form?", "choices": [{"label": "A", "text": "The regression $R^2$ may be spuriously high"}, {"label": "B", "text": "Test statistics may not follow standard distributions"}, {"label": "C", "text": "Statistical inferences may be invalid"}, {"label": "D", "text": "Parameter estimates may be biased"}], "answerKey": "D", "model_answer": "LECTURE: When data are not normally distributed, the usual assumptions of statistical tests may be violated. For example, if the dependent variable is a categorical variable, a non-stationary data in levels form may make the error terms in a regression model not strictly independent. This can affect the validity of inferences drawn from the model.\nSimilarly, if the independent variable is a count data, a non-stationary data in levels form may make the residuals in a Poisson regression model not normally distributed. This can affect the validity of inferences drawn from the model.\nIn addition, when data are not stationary, the parameter estimates from a time series model may be biased. This can affect the accuracy of predictions made from the model.\nFinally, when data are not stationary, the regression $R^2$ may be spuriously high. This can occur because a non-stationary time series has a higher variance than a stationary time series. As a result, a spurious relationship may appear to exist between the independent and dependent variables, when in fact, there is no true relationship.\nSOLUTION: A non-stationary data in levels form does not affect the validity of statistical inferences. So, statistical inferences may still be invalid.\nA non-stationary data in levels form does not affect the bias of parameter estimates. So, parameter estimates may still be biased.\nA non-stationary data in levels form can affect the spuriously high value of the regression $R^2$. So, the regression $R^2$ may be spuriously high.\nTest statistics may not follow standard distributions when data are not stationary. So, test statistics may not follow standard distributions."}
{"question": "If a series, y, follows a random walk, what is the optimal one-step ahead forecast of y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of y over the in-sample period"}], "answerKey": "A", "model_answer": "SOLUTION: A random walk is a series of numbers that are not predictable from past numbers. So, for a series that follows a random walk, the optimal one-step ahead forecast is zero."}
{"question": "The order condition is", "choices": [{"label": "A", "text": "A necessary and sufficient condition for identification"}, {"label": "B", "text": "A necessary but not sufficient condition for identification"}, {"label": "C", "text": "A sufficient but not necessary condition for identification"}, {"label": "D", "text": "A condition that is nether necessary nor sufficient for identification"}], "answerKey": "B", "model_answer": "LECTURE: A condition is a necessary condition if it is true and if the condition would be enough to identify the value of the variable. A condition is a sufficient condition if it is true and if the condition would guarantee the identifiability of the value of the variable.\nA condition is a necessary and sufficient condition for identification if it is both a necessary condition and a sufficient condition. This means that if the condition is true, the condition would be enough to identify the value of the variable.\nA condition is a necessary but not sufficient condition for identification if it is a necessary condition, but not all sufficient conditions are listed. This means that if the condition is true, the condition would be enough to identify the value of the variable. However, there may be other sufficient conditions in addition to the ones listed.\nA condition is a sufficient but not necessary condition for identification if it is a sufficient condition, but not all necessary conditions are listed. This means that if the condition is true, the condition would guarantee the identifiability of the value of the variable. However, there may be other necessary conditions in addition to the ones listed.\nA condition is a condition that is neither necessary nor sufficient for identification if it is not a necessary condition and it is not a sufficient condition. This means that the condition does not help you identify the value of the variable.\nSOLUTION: An identification condition is a condition that is neither necessary nor sufficient for identification. So, the order condition is not a necessary and sufficient condition for identification."}
{"question": "If an estimator is said to have minimum variance, which of the following statements is NOT implied?", "choices": [{"label": "A", "text": "The probability that the estimate is a long way away from its true value is minimised"}, {"label": "B", "text": "The estimator is efficient"}, {"label": "C", "text": "Such an estimator would be termed \"best\""}, {"label": "D", "text": "Such an estimator will always be unbiased"}], "answerKey": "D", "model_answer": "LECTURE: The term \"minimum variance\" refers to the fact that the estimator has a spread, or variance, that is as small as possible. This means that the variance of the estimator is minimized.\nOne implication of minimum-variance unbiased estimation is that the estimator has a smaller spread, or variance, than any other unbiased estimator. This means that the probability that the estimate is a long way away from its true value is minimized.\nHowever, the fact that an estimator has a minimum variance does not mean that it is efficient. An efficient estimator has a smaller variance than any other unbiased estimator.\nAn unbiased estimator that is also efficient will be the minimum-variance unbiased estimator. But just because an estimator is minimum-variance unbiased does not mean that it is efficient.\nFinally, just because an estimator is efficient does not mean that it is the \"best\" estimator. The best estimator is the one that meets your specific needs.\nSOLUTION: A minimum-variance unbiased estimator has a smaller spread, or variance, than any other unbiased estimator. So, the probability that the estimate is a long way away from its true value is minimized.\nBut just because an estimator has a minimum variance does not mean that it is efficient. So, the fact that an estimator is minimum-variance unbiased does not imply that the estimator is efficient.\nEfficiency is a separate criterion."}
{"question": "Which of the following are disadvantages of the Dickey-Fuller / Engle-Granger approach to testing for cointegration and modelling cointegrating relationships?\n\ni) Only one cointegrating relationship can be estimated\n\nii) Particularly for small samples. There is a high chance of the tests suggestingthat variables are not cointegrated when they are\n\niii) It is not possible to make inferences on the cointegrating regression\n\niv) The procedure forces the researcher to specify which is the dependent variable and which are the independent variables.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": ""}
{"question": "Which of the following statements is true concerning the population regression function (PRF) and sample regression function (SRF)?", "choices": [{"label": "A", "text": "The PRF is the estimated model"}, {"label": "B", "text": "The PRF is used to infer likely values of the SRF"}, {"label": "C", "text": "Whether the model is good can be determined by comparing the SRF and the PRF"}, {"label": "D", "text": "The PRF is a description of the process thought to be generating the data."}], "answerKey": "D", "model_answer": "SOLUTION: The population regression function (PRF) is a description of the process thought to be generating the data. The sample regression function (SRF) is the estimated model.\nThe PRF is not the estimated model.\nThe PRF is not used to infer likely values of the SRF.\nWhether a model is good can be determined by comparing the SRF and the PRF."}
{"question": "Which one of the following is a disadvantage of the general to specific or \"LSE\" (\"Hendry\") approach to building econometric models, relative to the specific to general approach?", "choices": [{"label": "A", "text": "Some variables may be excluded at the first stage leading to coefficient biases"}, {"label": "B", "text": "The final model may lack theoretical interpretation"}, {"label": "C", "text": "The final model may be statistically inadequate"}, {"label": "D", "text": "If the initial model is mis-specified, all subsequent steps will be invalid."}], "answerKey": "B", "model_answer": "LECTURE: Econometric models are built using both theoretical economics and statistical techniques. Theories from economics can be used to identify the variables that should be included in an econometric model, and statistical techniques can be used to estimate the parameters of the model.\nThere are two approaches to building an econometric model: the general to specific approach, or \"LSE\" approach, and the specific to general approach. The LSE approach starts with a model that includes all the variables that might potentially be important. These variables are then dropped one by one, starting with the least important, until a final model is arrived at. The specific to general approach starts with a model that includes only a few variables. These variables are then added to, one at a time, until a final model is arrived at.\nSOLUTION: The disadvantage of the LSE approach is that it may exclude variables at the first stage that are important for the final model. This can lead to biases in the estimates of the model's coefficients."}
{"question": "Which of the following statements are true concerning maximum likelihood (ML) estimation in the context of GARCH models?\n\ni) Maximum likelihood estimation selects the parameter values that maximise the\n\nprobability that we would have actually observed the values of the series y that we\n\nactually did.\n\n\nii) GARCH models can only be estimated by ML and not by OLS\n\n\niii) For estimation of a standard linear model (with no GARCH), the OLS and ML\n\nestimates for the slope and intercept parameters will be identical but the estimator\n\nfor the variance of the disturbances is slightly different\n\n\niv) Most computer packages use numerical procedures to estimate GARCH models\n\nrather than a set of analytical formulae", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: i. Maximum likelihood estimation selects the parameter values that maximise the probability that we would have actually observed the values of the series y that we actually did.\nii. GARCH models can only be estimated by ML and not by OLS.\niii. For estimation of a standard linear model (with no GARCH), the OLS and ML estimates for the slope and intercept parameters will be identical but the estimator for the variance of the disturbances is slightly different.\niv. Most computer packages use numerical procedures to estimate GARCH models rather than a set of analytical formulae.\nThese statements are true.\nML estimation is used in GARCH models. It is the only method of estimation.\nThe likelihood function is used in ML estimation. This function tells you the probability that your model generated the data you observed, given the parameter values.\nFor a GARCH model, the likelihood function is different from the likelihood function for a standard linear model. So, the OLS and ML estimates for the parameters of a GARCH model will be different from the OLS and ML estimates for the parameters of a standard linear model.\nThe only way to estimate a GARCH model is by ML. This is because the likelihood function for a GARCH model is not analytical. It is not possible to find an exact, explicit expression for the likelihood function. So, the only way to estimate the parameters is by numerical optimization.\nMost computer packages use numerical procedures to estimate GARCH models. They do not use a set of analytical formulae.\nThese statements are also true. So, the claim istrue."}
{"question": "Including relevant lagged values of the dependent variable on the right hand side of a regression equation could lead to which one of the following?", "choices": [{"label": "A", "text": "Biased but consistent coefficient estimates"}, {"label": "B", "text": "Biased and inconsistent coefficient estimates"}, {"label": "C", "text": "Unbiased but inconsistent coefficient estimates"}, {"label": "D", "text": "Unbiased and consistent but inefficient coefficient estimates."}], "answerKey": "A", "model_answer": "LECTURE: Including relevant lagged values of the dependent variable on the right hand side of a regression equation can help improve the accuracy of the coefficient estimates. However, it can also introduce bias into the estimates.\nRegression models can be influenced by how the independent variables are constructed, and including lagged values of the dependent variable on the right hand side of the equation can be one such influence. This can make the estimates of the coefficients inconsistent, which means that they do not produce the same results each time the model is estimated.\nConsistent but biased coefficient estimates are estimates that produce the same results each time the model is estimated. These estimates are biased, which means that they do not represent the true relationship between the independent and dependent variables.\nInconsistent but unbiased coefficient estimates are estimates that produce different results each time the model is estimated. These estimates are unbiased, which means that they represent the true relationship between the independent and dependent variables.\nEfficient coefficient estimates are estimates that capture all of the information in the data. Including relevant lagged values of the dependent variable on the right hand side of the equation can help improve the accuracy of the coefficient estimates, but it does not guarantee that the estimates will be efficient.\nSOLUTION: Including relevant lagged values of the dependent variable on the right hand side of a regression equation can introduce bias into the coefficient estimates. This can result in biased and inconsistent coefficient estimates."}
{"question": "Which one of the following factors is likely to lead to a relatively high degree of out-of-sample forecast accuracy?", "choices": [{"label": "A", "text": "A model that is based on financial theory"}, {"label": "B", "text": "A model that contains many variables"}, {"label": "C", "text": "A model whose dependent variable has recently exhibited a structural change"}, {"label": "D", "text": "A model that is entirely statistical in nature with no room for judgmental modification of forecasts"}], "answerKey": "A", "model_answer": "SOLUTION: A model that is based on financial theory is likely to lead to a relatively high degree of out-of-sample forecast accuracy. Financial theory provides a foundation for forecasting financial variables. For example, financial theory can be used to determine how changes in interest rates or inflation affect stock prices.\nA model that contains many variables is not necessarily more accurate. Adding variables to a model can improve its fit, but it can also increase the probability that the model is merely a statistical artifact.\nA model whose dependent variable has recently exhibited a structural change is likely to require a different type of model. For example, if a variable's mean has recently changed, a model that uses the old mean to make forecasts is likely to have low accuracy. But a model that uses the new mean is likely to have high accuracy.\nA model that is entirely statistical in nature with no room for judgmental modification of forecasts is not likely to lead to a relatively high degree of out-of-sample forecast accuracy. Such a model cannot incorporate information that is not in the data. For example, if a company is acquired, a statistical model cannot incorporate this information, while a model based on financial theory can."}
{"question": "Which of the following are plausible approaches to dealing with residual autocorrelation?\n\ni) Take logarithms of each of the variables\n\nii) Add lagged values of the variables to the regression equation\n\niii) Use dummy variables to remove outlying observations\n\niv) Try a model in first differenced form rather than in levels.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: Residual autocorrelation is a type of autocorrelation. As mentioned in the question, there are several approaches that can be taken to deal with autocorrelation.\nOne approach is to use lagged values of the variables in the regression equation. This is equivalent to including a constant term for each variable in the model, which can be effective at correcting for autocorrelation.\nAnother approach is to use dummy variables to remove outlying observations. This can be effective at correcting for autocorrelation if there are specific observations that are contributing to the autocorrelation.\nTaking logarithms of each of the variables is not an effective approach for dealing with autocorrelation.\nUsing a model in first differenced form (as opposed to using levels) can be effective at dealing with autocorrelation."}
{"question": "For an autoregressive process to be considered stationary", "choices": [{"label": "A", "text": "The roots of the characteristic equation must all lie inside the unit circle"}, {"label": "B", "text": "The roots of the characteristic equation must all lie on the unit circle"}, {"label": "C", "text": "The roots of the characteristic equation must all lie outside the unit circle"}, {"label": "D", "text": "The roots of the characteristic equation must all be less than one in absolute value"}], "answerKey": "C", "model_answer": "LECTURE: A stationary process is one in which the statistical properties do not change over time. One way to define a stationary process is through its autocorrelation function (ACF). The ACF of a stationary process is called its characteristic function.\nThe characteristic function is a complex-valued function. Its magnitude (or absolute value) is equal to the sum of the magnitudes of the coefficients of the polynomial.\nThe roots of the characteristic equation are the values of the autoregressive parameter (\u03b1) for which the characteristic function has a magnitude greater than 1.\nFor an autoregressive process to be stationary, the magnitude of the characteristic function must be equal to 1. This means that the roots of the characteristic equation must all lie on the unit circle.\nThe unit circle is the set of all points in the complex plane with magnitude 1.\nThe roots of the characteristic equation must all lie on the unit circle."}
{"question": "Which of the following statements are true concerning information criteria?\n\n(i) Adjusted R-squared is an information criterion\n\n(ii) If the residual sum of squares falls when an additional term is added, the value of the information criterion will fall\n\n(iii) Akaike's information criterion always leads to model orders that are at least as large as those of Schwarz's information criterion\n\n(iv) Akaike's information criterion is consistent", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: An information criterion is a measure of the goodness of fit of a model. Akaike's information criterion (AIC) and Schwarz's information criterion (SIC) are two common information criteria.\nBoth AIC and SIC can be used to compare models. If a lower value of AIC (or a lower value of SIC) indicates a better model, then a lower value of AIC (or a lower value of SIC) indicates that a model with a smaller order is a better model than a model with a larger order.\nBoth AIC and SIC are consistent, which means that they always lead to model orders that are at least as large as the order of the best linear model.\nIf the residual sum of squares falls when an additional term is added, the value of the information criterion will fall. This means that AIC and SIC will always lead to model orders that are at least as small as the order of the best linear model.\nThere are times when AIC and SIC will lead to different model orders. In these cases, the model with the lower value of the information criterion has a smaller order."}
{"question": "The \"within transform\" involves", "choices": [{"label": "A", "text": "Taking the average values of the variables"}, {"label": "B", "text": "Subtracting the mean of each entity away from each observation on that entity"}, {"label": "C", "text": "Estimating a panel data model using least squares dummy variables"}, {"label": "D", "text": "Using both time dummies and cross-sectional dummies in a fixed effects panel model"}], "answerKey": "B", "model_answer": "SOLUTION: The \"within transform\" involves using both time dummies and cross-sectional dummies in a fixed effects panel model.\nThe within transform in a fixed effects model includes two types of variables: time dummies and cross-sectional dummies.\nTime dummies are used to indicate changes in the within-group mean over time. For example, if you are comparing a country's gasoline consumption in 2001 to its consumption in 2002, you would include a time dummy for 2001 and a time dummy for 2002 in the model.\nCross-sectional dummies are used to indicate differences between groups. For example, if you are comparing gasoline consumption in winter to gasoline consumption in summer, you would include a cross-sectional dummy for winter and a cross-sectional dummy for summer in the model.\nThe within transform can include other variables, such as a fixed effect for a country, but the within transform always includes the time dummies and cross-sectional dummies."}
{"question": "The purpose of \"augmenting\" the Dickey-Fuller test regression is to", "choices": [{"label": "A", "text": "Ensure that there is no heteroscedasticity in the test regression residuals."}, {"label": "B", "text": "Ensure that the test regression residuals are normally distributed"}, {"label": "C", "text": "Ensure that there is no autocorrelation in the test regression residuals"}, {"label": "D", "text": "Ensure that all of the non-stationarity is taken into account."}], "answerKey": "C", "model_answer": "SOLUTION: The Dickey-Fuller test is used to determine if a time series is stationary. Stationarity is an assumption of many time series models. If a time series is not stationary, the parameters of a time series model may change over time.\nOne of the ways to determine if a time series is stationary is to use a test such as the Augmented Dickey-Fuller (ADF) test. The ADF test examines the residuals from a regression of the time series on its own lags. The purpose of \"augmenting\" the ADF test regression is to ensure that all of the non-stationarity is taken into account.\nIf a time series is not stationary, it may be becoming stationary. In this case, the ADF test can still be used to determine whether the time series is stationary. The test can examine the residuals from a regression of the time series on its own lags. The purpose of \"augmenting\" the ADF test regression is to ensure that all of the non-stationarity is taken into account."}
{"question": "If a series, y, follows a random walk with drift b, what is the optimal one-step ahead forecast of the change in y?", "choices": [{"label": "A", "text": "The current value of y"}, {"label": "B", "text": "Zero"}, {"label": "C", "text": "One"}, {"label": "D", "text": "The average value of the change in y over the in-sample period"}], "answerKey": "D", "model_answer": "LECTURE: A random walk is a model of a series where the change between consecutive observations is random. In a random walk with drift, the change is predicated on the current value of the series.\nSo, the optimal one-step ahead forecast for a random walk with drift is the current value of the series."}
{"question": "Which of the following are plausible approaches to dealing with a model that exhibits heteroscedasticity?\n\ni) Take logarithms of each of the variables\n\nii) Use suitably modified standard errors\n\niii) Use a generalised least squares procedure\n\niv) Add lagged values of the variables to the regression equation.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": ""}
{"question": "Which of the following statements are true concerning the standardised residuals (residuals divided by their respective conditional standard deviations) from an estimated GARCH model?\n\ni) They are assumed to be normally distributed\n\n\nii) Their squares will be related to their lagged squared values if the GARCH model is\n\nappropriate\n\n\niii) In practice, they are likely to have fat tails\n\n\niv) If the GARCH model is adequate, the standardised residuals and the raw residuals\n\nwill be identical", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": ""}
{"question": "Which one of the following statements is true concerning VARs?", "choices": [{"label": "A", "text": "The coefficient estimates have intuitive theoretical interpretations"}, {"label": "B", "text": "The coefficient estimates usually have the same sign for all of the lags of a given variable in a given equation"}, {"label": "C", "text": "VARs often produce better forecasts than simultaneous equation structural models"}, {"label": "D", "text": "All of the components of a VAR must be stationary before it can be used for forecasting"}], "answerKey": "C", "model_answer": "SOLUTION: A VAR (vector autoregression) is a type of econometric model that can be used to analyze the relationships among multiple time series. The coefficient estimates in a VAR have the following properties:\nThe estimated coefficients have intuitive theoretical interpretations.\nThe coefficient estimates usually have the same sign for all of the lags of a given variable in a given equation.\nVARs often produce better forecasts than simultaneous equation structural models.\nAll of the components of a VAR must be stationary before it can be used for forecasting.\nThe first three properties are part of the definition of a VAR. The fourth property is a requirement for using a VAR for forecasting. So, all of the components of a VAR must be stationary before it can be used for forecasting."}
{"question": "Which of the following statements is INCORRECT concerning the classical hypothesis testing framework?", "choices": [{"label": "A", "text": "If the null hypothesis is rejected, the alternative is accepted"}, {"label": "B", "text": "The null hypothesis is the statement being tested while the alternative encompasses the remaining outcomes of interest"}, {"label": "C", "text": "The test of significance and confidence interval approaches will always give the same conclusions"}, {"label": "D", "text": "Hypothesis tests are used to make inferences about the population parameters."}], "answerKey": "A", "model_answer": "LECTURE: One of the key steps in the classical hypothesis testing framework is to determine the conclusion that can be drawn from the test. This step involves considering the test statistic and the significance level.\nIf the test statistic is greater than the significance level, then the null hypothesis is rejected. This means that the results of the study are consistent with the alternative hypothesis.\nIf the test statistic is less than the significance level, then the null hypothesis is not rejected. This means that the results of the study are consistent with the null hypothesis.\nIn either case, the conclusion is drawn from the evidence. The conclusion is not based on the null hypothesis or the alternative hypothesis per se.\nIf the null hypothesis is rejected, then the alternative hypothesis is accepted. This means that the results of the study are consistent with the conclusion that the alternative hypothesis encapsulates.\nIf the null hypothesis is not rejected, then the alternative hypothesis is not accepted. This means that the results of the study are consistent with the conclusion that the null hypothesis encapsulates.\nThere are two common approaches to conducting a hypothesis test: the test of significance and the confidence interval approach. These two approaches will sometimes give different conclusions. For example, a confidence interval may indicate that the true effect is outside of the range being tested, which would not be detected by a test of significance.\nHypothesis tests are used to make inferences about population parameters. The word \"infer\" is the key. Infer means to make a deduction. Hypothesis tests use data from a sample to make conclusions about a population parameter.\nSOLUTION: The correct answer is A. If the null hypothesis is rejected, the alternative is accepted. This means that the conclusion is drawn from the evidence. The conclusion is not based on the null hypothesis or the alternative hypothesis."}
{"question": "An ARMA(p,q) (p, q are integers bigger than zero) model will have", "choices": [{"label": "A", "text": "An acf and pacf that both decline geometrically"}, {"label": "B", "text": "An acf that declines geometrically and a pacf that is zero after p lags"}, {"label": "C", "text": "An acf that declines geometrically and a pacf that is zero after q lags"}, {"label": "D", "text": "An acf that is zero after p lags and a pacf that is zero after q lags"}], "answerKey": "A", "model_answer": "LECTURE: A time series is a sequence of data measured over time. One way to analyze time series data is to use autoregressive integrated moving average (ARIMA) models. An ARMA model is an ARIMA model with only the autoregressive and moving average components.\nAn ARMA(p,q) model will have an autocorrelation function (ACF) that is nonzero for lags 0 to q and zero for larger lags, and a partial autocorrelation function (PACF) that is zero for lags larger than p.\nThe ACF and PACF can be used to determine the values of p and q. The ACF will decline geometrically as the lag increases from 0 to q. The PACF will be zero after the peak at lag p.\nSOLUTION: An ARMA(p,q) model will have an acf that declines geometrically and a pacf that is zero after q lags. The acf will be zero after p lags, and the pacf will be zero after q lags."}
{"question": "Suppose that the following regression is estimated using 27 quarterly observations:\n\n$y_t = \\beta_1 + \\beta_2 x_2 + \\beta_3 x_{3t} + u_t$\n\nWhat is the appropriate critical value for a 2-sided 5% size of test of $H_0: \\beta_3 = 1$?", "choices": [{"label": "A", "text": "1.64"}, {"label": "B", "text": "1.71"}, {"label": "C", "text": "2.06"}, {"label": "D", "text": "1.96"}], "answerKey": "C", "model_answer": "LECTURE: In a regression model with three variables, the joint significance of the parameters at the 5% level can be tested using a 2-sided test. The critical value for this test is based on a t-distribution with 27-2=25 degrees of freedom.\nThe t-value can be calculated as follows:\nt = (b3 / SE(b3)) / sqrt(n-2)\nwhere b3 is the estimated value of the parameter of interest (in this case, 1), and SE(b3) is the standard error of b3.\nSE(b3) = b3 / sqrt(n)\nSubstituting the values for b3 and SE(b3), we get:\nt = (1 - 0.05) / 2.06\nt = 1.645\nThe critical value for a 2-sided 5% test is 1.645."}
{"question": "Suppose that two researchers, using the same 3 variables and the same 250 observations on each variable, estimate a VAR. One estimates a VAR(6), while the other estimates a VAR(4). The determinants of the variance-covariance matrices of the residuals for each VAR are 0.0036 and 0.0049 respectively. What is the values of the test statistic for performing a test of whether the VAR(6) can be restricted to a VAR(4)?", "choices": [{"label": "A", "text": "77.07"}, {"label": "B", "text": "0.31"}, {"label": "C", "text": "0.33"}, {"label": "D", "text": "4.87"}], "answerKey": "A", "model_answer": "LECTURE: A test can be performed to determine whether a VAR(6) can be reduced to a VAR(4) by restricting the number of variables. One way to perform this test is to estimate a VAR(4) and then use the residuals to calculate the test statistic.\nThe test statistic is calculated from the residuals of the VAR(6) and the determinants of the variance-covariance matrix of the residuals for the VAR(4). The formula for the test statistic is:\ntest statistic = (trace(R6) - trace(R4)) / (determinant(R6) - determinant(R4))\nwhere R6 and R4 are the variance-covariance matrices of the residuals for the VAR(6) and the VAR(4), respectively.\nThe value of the test statistic will be greater than 0. This test statistic can be compared to a critical value to determine whether the VAR(6) can be restricted to a VAR(4).\nSOLUTION: The values of the trace and the determinant of the variance-covariance matrix of the residuals for each VAR are:\nFor the VAR(6):\ntrace of R6 = 0.0036 \u00b7 6 = 0.0224\ndeterminant of R6 = 0.0036 \u00b7 36 - 0.0224 \u00b7 250 = 0.0036 \u00b7 36 - 0.0224 \u00b7 250\nFor the VAR(4):\ntrace of R4 = 0.0049 \u00b7 4 = 0.0246\ndeterminant of R4 = 0.0049 \u00b7 24 - 0.0246 \u00b7 250 = 0.0049 \u00b7 24 - 0.0246 \u00b7 250\nSo, the test statistic is:\ntest statistic = (0.0224 - 0.0246) / (0.0036 \u00b7 36 - 0.0049 \u00b7 24) = 0.33 / 0.0036 \u00b7 36 = 0.33 / 0.0234 = 1.42.\nThis test statistic can be compared to a critical value to determine whether the VAR(6) can be restricted to a VAR(4). The critical value for a test with 250 observations and 4 variables is 77.07.\nSo, the test statistic of 1.42 is less than the critical value of 77.07. This test can fail to reject the null hypothesis that the VAR(6) can be restricted to a VAR(4)."}
{"question": "Which of the following is a DISADVANTAGE of using pure time-series models (relative to structural models)?", "choices": [{"label": "A", "text": "They are not theoretically motivated"}, {"label": "B", "text": "They cannot produce forecasts easily"}, {"label": "C", "text": "They cannot be used for very high frequency data"}, {"label": "D", "text": "It is difficult to determine the appropriate explanatory variables for use in pure time-series models"}], "answerKey": "A", "model_answer": "SOLUTION: Pure time-series models use only historical data from the series you are trying to forecast. They do not use any external information, such as the value of economic indicators.\nOne disadvantage of using pure time-series models is that they cannot produce forecasts easily. In contrast, structural models can use external information to produce more accurate forecasts.\nAnother disadvantage is that pure time-series models cannot be used for very high frequency data. This is because pure time-series models use only the last few observations and the average of the previous observations. This means that they do not use observations that are too old. For very high frequency data, this can be a problem.\nIt is difficult to determine the appropriate explanatory variables for use in pure time-series models. Explanatory variables are variables that help to explain the series. For pure time-series models, the appropriate explanatory variables are the lagged values of the series itself. It can be difficult to determine which lagged values to use."}
{"question": "Which of the following are alternative names for the dependent variable (usually denoted by y) in linear regression analysis?\n\n(i) The regressand\n\n(ii) The regressor\n\n(iii) The explained variable\n\n(iv) The explanatory variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The explanatory variable is a synonym for the independent variable. The dependent variable is a synonym for the output variable.\nThe regressand is a synonym for the dependent variable.\nThe regressor is a synonym for the independent variable.\nThe explained variable is a synonym for the dependent variable."}
{"question": "Which of the following are advantages of the VAR approach to modelling the relationship between variables relative to the estimation of full structural models?\n\ni) VARs receive strong motivation from financial and economic theory\n\n\nii) VARs in their reduced forms can be used easily to produce time-series forecasts\n\n\niii) VAR models are typically highly parsimonious\n\n\niv) OLS can be applied separately to each equation in a reduced form VAR", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: TheVAR approach can be used to model the relationship between variables in a time series. One of the key advantages of the VAR approach is that it allows for the estimation of the effects of each variable on each other variable in the model. This makes the VAR approach well suited for detecting the long-run, or equilibrium, relationships among the variables in the model.\nAnother advantage of the VAR approach is that the reduced form VAR can be used to produce time-series forecasts. This is because the reduced form VAR can be used to identify the patterns in the data that are likely to continue into the future.\nA parsimonious model is a model that uses a small number of variables to explain a large amount of the variance in the data. One of the advantages of the VAR approach is that VAR models are typically highly parsimonious. This means that VAR models can explain a large amount of the variance in the data using a relatively small number of variables.\nOLS can be applied separately to each equation in a reduced form VAR. This means that OLS can be used to estimate the parameters of each equation in the VAR model."}
{"question": "Which of the following statements is TRUE concerning the standard regression model?", "choices": [{"label": "A", "text": "y has a probability distribution"}, {"label": "B", "text": "x has a probability distribution"}, {"label": "C", "text": "The disturbance term is assumed to be correlated with x"}, {"label": "D", "text": "For an adequate model, the residual (u-hat) will be zero for all sample data points"}], "answerKey": "A", "model_answer": "SOLUTION: The standard regression model is y = b0 + b1x + u.\nThe disturbance term is assumed to be correlated with x. This is called the linear regression model.\nFor this model, the residual (u-hat) will be zero for all sample data points but one. The rest of the answer choices are incorrect.\nThe correct answer is D."}
{"question": "Consider the following model for $y_t$:\n\n$y_t = \\mu + \\lambda t + u_t$\n\nWhich one of the following most accurately describes the process for $y_t$?", "choices": [{"label": "A", "text": "A unit root process"}, {"label": "B", "text": "A stationary process"}, {"label": "C", "text": "A deterministic trend process"}, {"label": "D", "text": "A random walk with drift"}], "answerKey": "C", "model_answer": "LECTURE: A unit root process is a process that has a unit root.\nA stationary process is a process with a constant mean.\nA deterministic trend process is a process with a fixed slope.\nA random walk with drift is a process with a random slope and a fixed mean.\nSOLUTION: A unit root process is a process that has a unit root. This means that the process has a non-zero mean and a non-constant mean value over time.\nA stationary process is a process with a constant mean. This means that the mean of the process does not change over time.\nA deterministic trend process is a process with a fixed slope. This means that the process has a non-zero mean and a constant mean value over time.\nA random walk with drift is a process with a random slope and a fixed mean. This means that the mean of the process does not change over time, and the mean is not equal to zero."}
{"question": "Which of the following is correct concerning logit and probit models?", "choices": [{"label": "A", "text": "They use a different method of transforming the model so that the probabilities lie between zero and one"}, {"label": "B", "text": "The logit model can result in too many observations falling at exactly zero or exactly one"}, {"label": "C", "text": "For the logit model, the marginal effect of a change in one of the explanatory variables is simply the estimate of the parameter attached to that variable, whereas this is not the case for the probit model"}, {"label": "D", "text": "The probit model is based on a cumulative logistic function"}], "answerKey": "A", "model_answer": ""}
{"question": "What is the most important disadvantage of the diagonal VECH approach to building multivariate GARCH models that is overcome by the BEKK formulation?", "choices": [{"label": "A", "text": "The diagonal VECH model is hard to interpret intuitively"}, {"label": "B", "text": "The diagonal VECH model contains too many parameters"}, {"label": "C", "text": "The diagonal VECH model does not ensure a positive-definite variance-covariance matrix"}, {"label": "D", "text": "The BEKK model reduces the dimensionality problem that arises when a number of series are modelled together."}], "answerKey": "C", "model_answer": "SOLUTION: The most important disadvantage of the diagonal VECH approach to building multivariate GARCH models is that it does not ensure a positive-definite variance-covariance matrix. This can lead to unstable parameter estimates. The BEKK formulation overcomes this disadvantage by ensuring a positive-definite variance-covariance matrix."}
{"question": "If a relevant variable is omitted from a regression equation, the consequences would be that:\n\ni) The standard errors would be biased\n\n\nii) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope coefficients will be inconsistent.\n\n\niii) If the excluded variable is uncorrelated with all of the included variables, the\n\nintercept coefficient will be inconsistent.\n\n\niv) If the excluded variable is uncorrelated with all of the included variables, all of\n\nthe slope and intercept coefficients will be consistent and unbiased but inefficient.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: If a relevant variable is omitted from a regression equation, the consequences are:\nThe standard errors are biased.\nIf the excluded variable is uncorrelated with all of the included variables, the coefficients will be consistent.\nIf the excluded variable is uncorrelated with all of the included variables, the intercept coefficient will be biased.\nIf the excluded variable is uncorrelated with all of the included variables, the slope coefficients will be inconsistent.\nIf the excluded variable is uncorrelated with all of the included variables, the intercept coefficient will be inconsistent.\nIf the excluded variable is uncorrelated with all of the included variables, all of the slope and intercept coefficients will be consistent and unbiased but inefficient."}
{"question": "Which of the following are alternative names for the independent variable (usually denoted by x) in linear regression analysis?\n\n(i) The regressor\n\n(ii) The regressand\n\n(iii) The causal variable\n\n(iv) The effect variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The independent variable is called the regressor because it is the variable whose values are used to predict the values of the dependent variable.\nThe dependent variable is called the regressand because it is the variable that is 'regressed\u2019 onto (predicted from) the independent variable.\nThe causal variable is another term for the independent variable. This term is no longer used in statistics, but it is still used in other fields, such as economics.\nThe effect variable is another term for the dependent variable. This term is no longer used in statistics, but it is still used in other fields, such as economics."}
{"question": "Consider the OLS estimator for the standard error of the slope coefficient. Which of the following statement(s) is (are) true?\n\n(i) The standard error will be positively related to the residual variance\n\n(ii) The standard error will be negatively related to the dispersion of the observations on the explanatory variable about their mean value\n\n(iii) The standard error will be negatively related to the sample size\n\n(iv) The standard error gives a measure of the precision of the coefficient estimate.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: The standard error gives a measure of the precision of the coefficient estimate.\nThe OLS estimator is the slope coefficient estimate that minimizes the residual sum of squares.\nThe residual sum of squares is equal to the sum of the squared residuals, which in turn is equal to the sum of the squared differences between the observed values of the dependent variable and the predicted values.\nSo, the standard error is equal to the slope coefficient estimate divided by the reciprocal of its t-value.\nThe t-value is equal to the ratio of the standard error to the square root of the sample size.\nSo, the standard error is equal to the slope coefficient estimate divided by the reciprocal of its t-value, which is equal to the slope coefficient estimate divided by the sample size.\nThe standard error is negatively related to the sample size.\nThis means that the standard error is higher when the sample size is smaller."}
{"question": "What is the meaning of the term \"heteroscedasticity\"?", "choices": [{"label": "A", "text": "The variance of the errors is not constant"}, {"label": "B", "text": "The variance of the dependent variable is not constant"}, {"label": "C", "text": "The errors are not linearly independent of one another"}, {"label": "D", "text": "The errors have non-zero mean"}], "answerKey": "A", "model_answer": "LECTURE: Heteroscedasticity refers to a situation in which the variance of the errors is not constant. In other words, the size of the errors in the model is not the same for all values of the independent variable.\nHeteroscedasticity can be problematic because it can lead to biased estimates of the regression coefficients. This happens because the variable whose variance is changing is usually included in the regression equation. When the variance of the errors is not constant, the estimates of the coefficients are no longer based on the same calculations for each point on the independent variable. This can lead to inconsistent estimates of the coefficients, and therefore biased predictions.\nSOLUTION: The variance of the errors is not constant.\nHeteroscedasticity occurs when the variance of the errors is not constant. That is, the size of the errors in the model is not the same for all values of the independent variable.\nThe other answer choices are incorrect.\nThe errors are not linearly independent of one another.\nLinear independence is a concept from linear algebra. It applies to sets of linear equations, not to regression models.\nThe errors have a non-zero mean.\nA heteroscedastic model may have a non-zero mean for the errors. However, the mean of the errors is not equal to the variance of the errors. The mean of the errors is a different property from the variance of the errors.\nThe variance of the dependent variable is not constant.\nThe variance of the dependent variable is not equal to the variance of the errors. The dependent variable is not the same as the errors."}
{"question": "If a Durbin Watson statistic takes a value close to zero, what will be the value of the first order autocorrelation coefficient?", "choices": [{"label": "A", "text": "Close to zero"}, {"label": "B", "text": "Close to plus one"}, {"label": "C", "text": "Close to minus one"}, {"label": "D", "text": "Close to either minus one or plus one"}], "answerKey": "C", "model_answer": "LECTURE: A Durbin Watson statistic takes values close to zero when there is an autocorrelation problem in the data. An autocorrelation problem means that the value of a variable depends on its own value in the previous time period. This can cause the residuals from a statistical model to be correlated.\nThe first order autocorrelation coefficient is a measure of the strength of the autocorrelation. It takes values between -1 and 1.\nIf the Durbin Watson statistic takes a value close to zero, the first order autocorrelation coefficient will also take a value close to zero.\nSOLUTION: The Durbin Watson statistic takes a value close to zero when there is an autocorrelation problem in the data. So, if the Durbin Watson statistic takes a value close to zero, the first order autocorrelation coefficient will also take a value close to zero."}
{"question": "Under the null hypothesis of a Bera-Jarque test, the distribution has", "choices": [{"label": "A", "text": "Zero skewness and zero kurtosis"}, {"label": "B", "text": "Zero skewness and a kurtosis of three"}, {"label": "C", "text": "Skewness of one and zero kurtosis"}, {"label": "D", "text": "Skewness of one and kurtosis of three."}], "answerKey": "B", "model_answer": "LECTURE: The Bera-Jarque test is a test of normality for a univariate dataset. The null hypothesis of the test is that the dataset comes from a normal distribution. The alternative hypothesis is that the dataset does not come from a normal distribution.\nOne of the results of the Bera-Jarque test is that the distribution has zero skewness and zero kurtosis. This result means that the dataset is normally distributed.\nAnother result of the Bera-Jarque test is that the distribution has skewness of one and kurtosis of three. This result indicates that the dataset is not normally distributed.\nSOLUTION: Under the null hypothesis of a Bera-Jarque test, the distribution has zero skewness and zero kurtosis."}
{"question": "If an estimator is said to be consistent, it is implied that", "choices": [{"label": "A", "text": "On average, the estimated coefficient values will equal the true values"}, {"label": "B", "text": "The OLS estimator is unbiased and no other unbiased estimator has a smaller variance"}, {"label": "C", "text": "The estimates will converge upon the true values as the sample size increases"}, {"label": "D", "text": "The coefficient estimates will be as close to their true values as possible for small and large samples."}], "answerKey": "C", "model_answer": "SOLUTION: Consistent estimators converge upon the true values as the sample size increases.\nThis means that as the size of the sample grows, the estimator gets closer and closer to the true value of the coefficient."}
{"question": "Which of the following is a typical characteristic of financial asset return time-series?", "choices": [{"label": "A", "text": "Their distributions are thin-tailed"}, {"label": "B", "text": "They are not weakly stationary"}, {"label": "C", "text": "They are highly autocorrelated"}, {"label": "D", "text": "They have no trend"}], "answerKey": "D", "model_answer": "LECTURE: A financial asset's return time-series is a series of returns that the asset has generated in the past. The return series can be used to estimate the expected return and the risk of the asset.\nA financial asset's return time-series has the following characteristics:\nIt is a random walk.\nIt is weakly stationary.\nIt has a trend.\nIt is highly autocorrelated.\nIt has a thin tail.\nA weakly stationary time series is one in which the means, medians, and modes are constant over time. Financial asset return time-series are not weakly stationary. Instead, they are characterized by a trend, or a long-term increase or decrease in the average return.\nA highly autocorrelated time series is one in which the current return is highly correlated with the return from a single period ago. Financial asset return time-series are highly autocorrelated. This means that the return in one period is a good predictor of the return in the next period.\nA thin-tailed time series has a few extreme values. Financial asset return time-series are thin-tailed. This means that there are few periods with extremely high or low returns.\nA time series has a trend if the average return is increasing or decreasing over time. Financial asset return time-series have a trend.\nSOLUTION: A financial asset's return time-series is highly autocorrelated, which means that the return in one period is a good predictor of the return in the next period. The return series has a trend, which means that the average return is increasing or decreasing over time. The return series is not weakly stationary, which means that the means, medians, and modes are not constant over time. The return series is a thin-tailed time series, which means that there are few periods with extremely high or low returns."}
{"question": "Which of the following assumptions are required to show the consistency, unbiasedness and efficiency of the OLS estimator?\n\ni) $E(u_t) = 0$\n\n\nii) $\\text{Var}(u_t) = \\sigma^2$\n\n\niii) $\\text{Cov}(u_t, u_{t-j}) = 0 \\forall j$\n\n\niv) $u_t \\sim N(0, \\sigma^2)$", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": ""}
{"question": "Which of the following is a disadvantage of the fixed effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The model is likely to be technical to estimate"}, {"label": "B", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "C", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "D", "text": "The fixed effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}], "answerKey": "C", "model_answer": "LECTURE: In economics, a panel data set is a data set that includes data for multiple units over time. A panel model is a type of econometric model that is estimated on a panel data set.\nOne way to estimate a panel model is to use a fixed effects approach. In this approach, the model is estimated separately for each unit, while accounting for the fact that the units are not independent. This approach is called \"fixed effects\" because it estimates a separate effect for each unit.\nOne disadvantage of the fixed effects approach is that the model is likely to be technical to estimate. This can make it difficult for practitioners to understand the results of the model.\nAnother disadvantage is that the approach may not be valid if the composite error term is correlated with one or more of the explanatory variables. If this is the case, the estimates of the parameters may be biased.\nThe number of parameters to estimate may be large, resulting in a loss of degrees of freedom. This can make it difficult to obtain stable estimates of the parameters.\nThe fixed effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable. This means that the approach can identify which units have different outcomes, but it cannot identify how the outcomes change over time for each unit.\nSOLUTION: D."}
{"question": "Consider an identical situation to that of question 21, except that now a 2-sided alternative is used. What would now be the appropriate conclusion?", "choices": [{"label": "A", "text": "H0 is rejected"}, {"label": "B", "text": "H0 is not rejected"}, {"label": "C", "text": "H1 is rejected"}, {"label": "D", "text": "There is insufficient information given in the question to reach a conclusion"}], "answerKey": "A", "model_answer": "LECTURE: When a hypothesis test uses a two-sided alternative, the test can examine two different populations to determine if they are significantly different. In an independent samples test, the test can examine the difference between two populations to determine if it is statistically significant. In a paired samples test, the test can examine the difference between two related populations to determine if it is statistically significant.\nWhen a hypothesis test uses a two-sided alternative, the conclusions that can be drawn depend on the direction of the test statistic.\nIf the test statistic is greater than the critical value, the result is significant. In this case, you reject H0.\nIf the test statistic is less than the critical value, the result is not significant. In this case, you do not reject H0.\nSOLUTION: To determine the appropriate conclusion, compare the test statistic to the critical value.\nIf the test statistic is greater than the critical value, the result is significant. In this case, you reject H0.\nIf the test statistic is less than the critical value, the result is not significant. In this case, you do not reject H0.\nThere is insufficient information given in the question to reach a conclusion."}
{"question": "The price of a house is best described as what type of number?", "choices": [{"label": "A", "text": "Discrete"}, {"label": "B", "text": "Cardinal"}, {"label": "C", "text": "Ordinal"}, {"label": "D", "text": "Nominal"}], "answerKey": "B", "model_answer": "SOLUTION: The price of a house is a cardinal number, which is a number that describes a quantity. The price of a house tells you how much the house costs."}
{"question": "If a Johansen \"trace\" test for a null hypothesis of 2 cointegrating vectors is applied to a system containing 4 variables is conducted, which eigenvalues would be used in the test?", "choices": [{"label": "A", "text": "All of them"}, {"label": "B", "text": "The largest 2"}, {"label": "C", "text": "The smallest 2"}, {"label": "D", "text": "The second largest"}], "answerKey": "C", "model_answer": "LECTURE: A Johansen test tests for the presence of cointegration among a set of variables. The test uses eigenvalue analysis to identify possible cointegration relationships among the variables.\nThe test uses the eigenvalues of the correlation matrix of the variables to determine the number of cointegration relationships that exist among the variables. The largest eigenvalues are used for this purpose.\nFor a system containing 4 variables, the trace test will use the largest 2 eigenvalues. The second largest eigenvalue will not be used.\nSOLUTION: The trace test for 2 cointegrating vectors will use the largest 2 eigenvalues. These are the eigenvalues with the largest absolute values.\nThe smallest 2 eigenvalues have smaller absolute values than the largest 2 eigenvalues. So, they will not be used in the test.\nThe second largest eigenvalue is larger than the smallest eigenvalues but smaller than the largest eigenvalues. So, it will not be used in the test.\nAll of the eigenvalues are used to form the correlation matrix. So, the test will use all of the eigenvalues. But, the largest 2 eigenvalues are the ones that are used to determine the number of cointegration relationships that exist among the variables."}
{"question": "Which of the following statements is true concerning variance decomposition analysis of VARs?\n\ni) Variance decompositions measure the impact of a unit shock to each of the variables on the VAR\n\nii) Variance decompositions can be thought of as measuring the proportion of the forecast error variance that is attributable to each variable\n\niii) The ordering of the variables is important for calculating impulse responses but not variance decompositions\n\niv) It is usual that most of the forecast error variance for a given variable is attributable to shocks to that variable", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: Variance decompositions can be thought of as measuring the proportion of the forecast error variance that is attributable to each variable. So, variable ordering is important for calculating variance decompositions.\nA unit shock to a variable will have the largest impact on the variable's forecast error variance. So, most of the forecast error variance for a given variable is attributable to shocks to that variable.\nSince variance decompositions measure the impact of a unit shock to each variable on the VAR, option (ii) and (iv) are correct."}
{"question": "Suppose that we have estimated a GARCH model for daily equity returns, and we are interested in producing a 10-day forecast of the volatility (measured by the standard deviation of returns) for use in a value at risk model. How could such a forecast most validly be calculated?", "choices": [{"label": "A", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up"}, {"label": "B", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts and add them up and take the square root"}, {"label": "C", "text": "Produce 1, 2, 3, ..., 10 step ahead conditional variance forecasts, take the square roots of each one and add them up"}, {"label": "D", "text": "Produce a 1-step ahead conditional variance forecast, take its square root and multiply it by the square root of 10"}], "answerKey": "B", "model_answer": "LECTURE: A value at risk (VaR) model estimates the potential loss of a portfolio over a specific time horizon with a certain level of confidence. For example, a VaR model may estimate that there is a 5% chance that the portfolio will lose more than $10 million within the next 10 days.\nOne input to a VaR model is the volatility of the portfolio. A GARCH model can be used to estimate the volatility of daily equity returns. One challenge with using a GARCH model for VaR is that the GARCH model produces conditional variance forecasts, which are squared. To calculate the volatility, you need to take the square root of the conditional variance.\nSOLUTION: To calculate the 10-day forecast of the volatility, you can take the square root of the sum of the squared 1-step ahead conditional variance forecasts. This is known as the conditional standard deviation.\nProducing 1, 2, 3, ..., 10 step ahead conditional variance forecasts and adding them up, and then taking the square root, is one way to calculate the 10-day forecast of the volatility.\nProducing 1, 2, 3, ..., 10 step ahead conditional variance forecasts, taking the square roots of each one, and then adding them up is another way to calculate the 10-day forecast of the volatility.\nProducing a 1-step ahead conditional variance forecast, taking its square root, and then multiplying it by the square root of 10 is not a valid way to calculate the 10-day forecast of the volatility."}
{"question": "Suppose that the value of $R^2$ for an estimated regression model is exactly zero. Which of the following are true?\n\ni) All coefficient estimates on the slopes will be zero\n\nii) The fitted line will be horizontal with respect to all of the explanatory variables\n\niii) The regression line has not explained any of the variability of y about its mean value\n\niv) The intercept coefficient estimate must be zero.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": ""}
{"question": "A white noise process will have\n\n(i) A zero mean\n\n(ii) A constant variance\n\n(iii) Autocovariances that are constant\n\n(iv) Autocovariances that are zero except at lag zero", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "LECTURE:Autocovariance is a measure of how two values of a random process change together. For a stationary process, the autocovariance is constant.\nThe mean of a process is a measure of the process's average value. A process with a zero mean has an average value of zero.\nThe variance of a process is a measure of the process's spread. A process with a constant variance has a constant spread.\nSOLUTION:A white noise process is a stationary process with a zero mean and a constant variance. So, the autocovariance is constant, and all autocovariances are zero except at lag zero."}
{"question": "Which of the following statements are true concerning the Box-Jenkins approach to diagnostic testing for ARMA models?\n\n(i) The tests will show whether the identified model is either too large or too small\n\n(ii) The tests involve checking the model residuals for autocorrelation, heteroscedasticity, and non-normality\n\n(iii) If the model suggested at the identification stage is appropriate, the acf and pacf for the residuals should show no additional structure\n\n(iv) If the model suggested at the identification stage is appropriate, the coefficients on the additional variables under the overfitting approach will be statistically insignificant", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: The Box-Jenkins approach to diagnostic testing for ARMA models involves a set of statistical tests that can show whether the identified model is appropriate. The tests described in the statement involve checking the model residuals for autocorrelation, heteroscedasticity, and non-normality.\nIf the model is appropriate, the acf and pacf for the residuals should show no additional structure. This means that the only patterns in the acf and pacf should be the patterns that were identified at the identification stage.\nThe coefficients on the additional variables under the overfitting approach will be statistically insignificant. This means that these variables do not add any predictive power to the model.\nIf the model is not appropriate, the tests can show which variables need to be added or removed."}
{"question": "Which one of the following would be a plausible response to a finding of residual non-normality?", "choices": [{"label": "A", "text": "Use a logarithmic functional form instead of a linear one"}, {"label": "B", "text": "Add lags of the variables on the right hand side of the regression model"}, {"label": "C", "text": "Estimate the model in first differenced form"}, {"label": "D", "text": "Remove any large outliers from the data."}], "answerKey": "D", "model_answer": ""}
{"question": "The fixed effects panel model is also sometimes known as", "choices": [{"label": "A", "text": "A seemingly unrelated regression model"}, {"label": "B", "text": "The least squares dummy variables approach"}, {"label": "C", "text": "The random effects model"}, {"label": "D", "text": "Heteroscedasticity and autocorrelation consistent"}], "answerKey": "B", "model_answer": "LECTURE:Fixed effects model are used to estimate the effect of a fixed set of covariates on a dependent variable. The fixed effects model is also known as the fixed effects panel data model because it is used to analyze panel data, which is a type of data that includes information on the same units over time.\nThe fixed effects model is similar to a random effects model, which is used to estimate the effect of a random set of covariates on a dependent variable. The main difference between the two models is that the random effects model uses a random set of covariates instead of a fixed set.\nOne key assumption of the fixed effects model is that the covariates are fixed over time. This means that the covariates do not change as the units grow older. This assumption is called the fixed effects assumption.\nAnother key assumption of the fixed effects model is that the error term is heteroscedastic and autocorrelated. This means that the variance of the error term is different across units and that the error term for one unit is correlated with the error term for other units.\nSOLUTION:The fixed effects panel data model is a type of fixed effects model. So, the fixed effects panel data model is sometimes known as a fixed effects model.\nThe least squares dummy variables approach is a type of regression model. So, the fixed effects panel data model is not sometimes known as the least squares dummy variables approach.\nThe random effects model is a type of fixed effects model. So, the fixed effects panel data model is sometimes known as the random effects model.\nHeteroscedasticity and autocorrelation consistent means that the model is consistent with the assumption that the error term is heteroscedastic and autocorrelated. So, the fixed effects panel data model is heteroscedasticity and autocorrelation consistent."}
{"question": "Which of the following statements is TRUE concerning OLS estimation?", "choices": [{"label": "A", "text": "OLS minimises the sum of the vertical distances from the points to the line"}, {"label": "B", "text": "OLS minimises the sum of the squares of the vertical distances from the points to the line"}, {"label": "C", "text": "OLS minimises the sum of the horizontal distances from the points to the line"}, {"label": "D", "text": "OLS minimises the sum of the squares of the horizontal distances from the points to the line."}], "answerKey": "B", "model_answer": "LECTURE: In least squares estimation, the line is the line that minimizes the sum of the squares of the vertical distances from the points to the line.\nBut you can use the following argument to show that OLS also minimizes the sum of the vertical distances from the points to the line.\nFirst, consider the equation of the line. This equation can be derived from the least squares equation by setting all the x's to 0:\ny = b0 + b1x.\nSubstitute x = 0 for each of the points in the dataset. These points give you the values of b0 and b1.\nThen, calculate the vertical distances from the points to the line. These distances are the squares of the differences between the actual y values and the predicted y values.\nThis shows that OLS minimizes the sum of the vertical distances from the points to the line."}
{"question": "If the standard tools for time-series analysis, such as estimation of the acf, pacf and spectral analysis, find no evidence of structure in the data, this implies that the data are which of the following?", "choices": [{"label": "A", "text": "Normally distributed"}, {"label": "B", "text": "Uncorrelated"}, {"label": "C", "text": "Independent"}, {"label": "D", "text": "Fat-tailed"}], "answerKey": "B", "model_answer": "SOLUTION: If the standard tools for time-series analysis find no evidence of structure in the data, this implies that the data are uncorrelated and independent.\nA random series is uncorrelated if the past values of the series have no influence on its future values. A random series is independent if the past values of the series are not statistically dependent on their own future values.\nNormally distributed data are not necessarily uncorrelated or independent.\nFat-tailed data are not necessarily uncorrelated or independent.\nCorrelation and independence are mutually exclusive. If a series is correlated, it is not independent, and vice versa."}
{"question": "If two variables, $x_t$ and $y_t$ are said to be cointegrated, which of the following statements are true?\n\ni) $x_t$ and $y_t$ must both be stationary\n\n\nii) Only one linear combination of $x_t$ and $y_t$ will be stationary\n\n\niii) The cointegrating equation for $x_t$ and $y_t$ describes the short-run relationship\n\nbetween the two series\n\n\niv) The residuals of a regression of $y_t$ on $x_t$ must be stationary", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "A", "model_answer": "SOLUTION: A cointegrated pair is a pair of non-stationary time series that are part of a stationary linear combination. The remaining part of the linear combination will be stationary.\nA cointegrating equation describes the long-run relationship between two non-stationary time series. It is the difference between a linear function of the two time series and a stationary time series. The right-hand side of the cointegrating equation is stationary.\nBecause the right-hand side of the cointegrating equation is stationary, the residuals of a regression of one time series on the other will be stationary.\nSo, the following statements are true.\n$x_t$ and $y_t$ must both be non-stationary.\nOnly one linear combination of $x_t$ and $y_t$ will be stationary.\nThe cointegrating equation for $x_t$ and $y_t$ describes the short-run relationship between the two series.\nThe residuals of a regression of $y_t$ on $x_t$ must be stationary."}
{"question": "A dependent variable whose values are not observable outside a certain range but where the corresponding values of the independent variables are still available would be most accurately described as what kind of variable?", "choices": [{"label": "A", "text": "Censored"}, {"label": "B", "text": "Truncated"}, {"label": "C", "text": "Multinomial variable"}, {"label": "D", "text": "Discrete choice"}], "answerKey": "A", "model_answer": "LECTURE: One of the key goals of statistical analysis is to learn how one or more independent variables affect a dependent variable. How can you determine what kind of variable the dependent variable is? One way is to look at the type of data that is measured.\nA variable is said to be discrete if the highest degree of the variable is 1. A variable is said to be continuous if the highest degree of the variable is 2.\nA variable is said to be multinomial if it is discrete and has three or more categories.\nA variable is said to be truncated if it is discrete and has a finite highest category.\nA variable is said to be censored if it is discrete and has a infinite highest category.\n\nSOLUTION: The fact that the dependent variable is not observable outside of a certain range suggests that the variable is truncated.\nA truncated variable has a finite highest category. This means that the dependent variable takes on a certain set of values within a certain range.\nThe fact that the corresponding values of the independent variables are still available suggests that the dependent variable is censored.\nA censored variable has an infinite highest category. This means that the dependent variable takes on a certain set of values within a certain range, but the highest category is not an observation \u2014 it is infinite."}
{"question": "A Hausman test would be used for", "choices": [{"label": "A", "text": "Determining whether an equation that is part of a simultaneous system is identified"}, {"label": "B", "text": "Determining whether a simultaneous framework is needed for a particular variable"}, {"label": "C", "text": "Determining whether 2SLS or ILS is optimal"}, {"label": "D", "text": "Determining whether the structural form equations can be obtained via substitution from the reduced forms"}], "answerKey": "B", "model_answer": "LECTURE: One of the key questions in empirical research is whether a particular equation is identified. An identified equation is one in which the coefficients are subject to constraints that are consistent with the underlying economic model.\nOne way to determine whether a particular equation is identified is to conduct a Hausman test. The Hausman test can be used to determine whether the coefficients on the variables in the equation are subject to significant constraints. If the coefficients are not subject to significant constraints, then the equation is not identified.\nThere are several types of Hausman tests, and the choice of which test to use depends on the specific model and the number of variables. One common test is the random effects test, which involves regressing the excluded variable on the included variable and testing whether the coefficient on the excluded variable is significantly different from zero.\nAnother way to determine whether a simultaneous framework is needed for a particular variable is to conduct a Ramsey RESET test. The RESET test can be used to determine the number of periods it takes for the variable to affect the endogenous variable in the model. If the variable is included in a simultaneous framework, then the RESET test can be used to determine the appropriate order of integration for the variable.\n2SLS and ILS are two alternative methods for estimating a structural equation when there are excluded variables. The choice of which method to use depends on the number of excluded variables and the form of the structural equation.\nThe 2SLS method uses two-stage least squares. In the first stage, the excluded variables are regressed on the endogenous variable. In the second stage, the structural equation is regressed on the first-stage coefficients.\nThe ILS method uses instrumental variables. In this case, the excluded variables are assumed to be functionally related to the endogenous variable, and a instrumental variable that is correlated with the excluded variable is used to estimate the structural equation.\nThe optimal method is 2SLS if the number of excluded variables is equal to the number of instruments. It is ILS if the number of excluded variables is greater than the number of instruments.\nThe correct way to obtain the structural form equations is by substitution from the reduced forms. The reduced forms are the forms of the equations resulting from regressing the endogenous variable on the excluded variables. The structural form equations are the forms of the equations that are part of a simultaneous system."}
{"question": "Under the matrix notation for the classical linear regression model, $y = X \\beta + u$, what are the dimensions of $u$?", "choices": [{"label": "A", "text": "T x k"}, {"label": "B", "text": "T x 1"}, {"label": "C", "text": "k x 1"}, {"label": "D", "text": "1 x 1"}], "answerKey": "B", "model_answer": "SOLUTION: The dimension of the error term $u$ is determined by its location in the matrix notation. The error term is located in the last column of the matrix. So, $u$ has dimension T x 1."}
{"question": "How many parameters will be required to be estimated in total for all equations of a standard form, unrestricted, tri-variate VAR(4), ignoring the intercepts?", "choices": [{"label": "A", "text": "12"}, {"label": "B", "text": "4"}, {"label": "C", "text": "3"}, {"label": "D", "text": "36"}], "answerKey": "D", "model_answer": "LECTURE: A vector auto regression (VAR) model is a statistical model that describes the relationships between multiple time series variables.\nA VAR(p) model includes up to p different lags of the variables in the model. For example, a VAR(2) model includes two lags of the variables.\nA tri-variate VAR(4) model includes four lags of each variable, and up to 10 parameters in total.\nThe number of parameters is determined by the number of equations and the number of variables.\nThere are 4 equations and 3 variables in a tri-variate VAR(4) model.\nSo, in total, 4 parameters will be estimated for the four equations, and 3 parameters will be estimated for each of the three variables, for a total of 36 parameters.\nHowever, the intercepts do not have to be estimated. An intercept is an additional parameter that can be added to the model. In a tri-variate VAR(4) model with no intercepts, a total of 36 parameters will be estimated."}
{"question": "A researcher tests for structural stability in the following regression model:\n\n$y_t = \\beta_1 + \\beta_2 x_{2t} + \\beta_3 x_{3t} + u_t$\n\nThe total sample of 200 observations is split exactly in half for the sub-sample regressions. Which would be the unrestricted residual sum of squares?", "choices": [{"label": "A", "text": "The RSS for the whole sample"}, {"label": "B", "text": "The RSS for the first sub-sample"}, {"label": "C", "text": "The RSS for the second sub-sample"}, {"label": "D", "text": "The sum of the RSS for the first and second sub-samples"}], "answerKey": "D", "model_answer": "LECTURE: A researcher tests for structural stability in the following regression model:\n$y\\_t = \\beta\\_1 + \\beta\\_2 x\\_{2t} + \\beta\\_3 x\\_{3t} + u\\_t$\nThe total sample of 200 observations is split exactly in half for the sub-sample regressions. Which would be the unrestricted residual sum of squares?\nThe residual sum of squares (RSS) is an unrestricted measure of the sum of the squares of the differences between the observed values of the dependent variable and the predicted values of the dependent variable.\nFor the whole sample, the RSS is given by the sum of the squared differences between the actual values of the dependent variable and the predicted values of the dependent variable based on the entire regression model.\nFor the first sub-sample, the RSS is given by the sum of the squared differences between the actual values of the dependent variable and the predicted values of the dependent variable based on the regression model including only the variables with subscripts less than 2.\nFor the second sub-sample, the RSS is given by the sum of the squared differences between the actual values of the dependent variable and the predicted values of the dependent variable based on the regression model including only the variables with subscripts less than 3.\nThe sum of the RSS for the first and second sub-samples is given by adding the RSS for the first sub-sample to the RSS for the second sub-sample."}
{"question": "Suppose that we are interested in testing the null hypothesis that a GARCH(2,2) model can be restricted to a process with a constant conditional variance using the likelihood ratio test approach. Which of the following statements are true?", "choices": [{"label": "A", "text": "The test statistic will follow a chi-squared distribution with 2 degrees of freedom under the null hypothesis"}, {"label": "B", "text": "The value of the log-likelihood function will almost always be bigger for the restricted model than for the unrestricted model"}, {"label": "C", "text": "If the relevant values of the log-likelihood functions are -112.3 and -118.4, the value of the test statistic is 12.2"}, {"label": "D", "text": "The likelihood ratio test compares the slopes of the log-likelihood function at the maximum and at the restricted parameter value."}], "answerKey": "C", "model_answer": "LECTURE: The F-test is a test of the null hypothesis that a model is equal to a restricted model. The test compares the ratio of the fitted values of the restricted model to the fitted values of the full model to the ratio of the standard errors of the restricted model to the standard errors of the full model.\nThe test statistic for the F-test is the F-distribution with degrees of freedom equal to the difference between the sizes of the two models. The null hypothesis is rejected if the test statistic is greater than the critical value from the F-distribution.\nThe likelihood ratio test is a test of the null hypothesis that a model is equal to a restricted model. The test compares the log-likelihood values of the restricted model and the full model.\nThe test statistic for the likelihood ratio test is the difference in the log-likelihood values. The null hypothesis is rejected if the test statistic is greater than a critical value from a chi-squared distribution with degrees of freedom equal to the difference between the sizes of the two models.\nThe value of the test statistic does not tell you whether to reject the null hypothesis. You need to decide how to reject the null hypothesis based on the critical value from the distribution.\nThe critical values for the F-test and the likelihood ratio test are different. So, the test that is more powerful will depend on the size of the models and the difference in the log-likelihood values.\nSOLUTION: The test statistic compares the slopes of the log-likelihood function at the maximum and at the restricted parameter value. So, the test will be more powerful as the difference in the log-likelihood values between the restricted and unrestricted models gets larger.\nThe likelihood ratio test is more powerful than the F-test when the models are large and the difference in the log-likelihood values between the restricted and unrestricted models is large.\nThe value of the test statistic does not tell you whether to reject the null hypothesis. You need to decide how to reject the null hypothesis based on the critical value from the distribution.\nIf the value of the test statistic is greater than 12.2, you would reject the null hypothesis.\nThe value of the log-likelihood function is -118.4. The test statistic is 12.2, which is greater than 12.2. So, you would reject the null hypothesis."}
{"question": "Which one of the following is NOT a plausible remedy for near multicollinearity?", "choices": [{"label": "A", "text": "Use principal components analysis"}, {"label": "B", "text": "Drop one of the collinear variables"}, {"label": "C", "text": "Use a longer run of data"}, {"label": "D", "text": "Take logarithms of each of the variables"}], "answerKey": "D", "model_answer": "LECTURE: Near multicollinearity is a condition where two or more predictor variables are highly correlated. In this case, the predictor variables are not adding much new information to the model.\nRemedies for near multicollinearity include:\nDropping one of the highly correlated predictor variables.\nUsing principal components analysis to rotate the predictor variables into a new set of uncorrelated components.\nUsing a longer run of data.\nUsing logarithms of each of the variables.\nThe logarithm remedy can work, but it is not a good choice for this problem. The logarithm of a variable is a transformation that changes the variable's scale. This transformation can help near multicollinearity, but it also changes the variable's mean and standard deviation. So, the logarithm remedy can change the distribution of the variables, which can affect the model's predictions.\nThe longer run of data remedy can also be effective, but it is not a good choice for this problem. This remedy assumes that the predictor variables are equally important. So, it does not address the problem that some predictor variables are not important because they are highly correlated with other predictor variables.\nThe principal components analysis remedy is a good choice. This remedy involves rotating the predictor variables into a new set of uncorrelated components. This transformation can change the number of predictor variables in the model. So, it may not be practical if you have a lot of predictor variables.\nThe best remedy is to drop one of the highly correlated predictor variables. This remedy addresses the problem at its source. If you have only a few predictor variables, you can simply exclude the highly correlated variable from the model. If you have a lot of predictor variables, you may need to use a algorithm to select the most important predictor variables."}
{"question": "Consider the following AR(2) process:\n\nyt = 1.5 yt-1 - 0.5 yt-2 + ut\n\nThis is a", "choices": [{"label": "A", "text": "Stationary process"}, {"label": "B", "text": "Unit root process"}, {"label": "C", "text": "Explosive process"}, {"label": "D", "text": "Stationary and unit root process"}], "answerKey": "B", "model_answer": ""}
{"question": "Which of the following could be used as a test for autocorrelation up to third order?", "choices": [{"label": "A", "text": "The Durbin Watson test"}, {"label": "B", "text": "White's test"}, {"label": "C", "text": "The RESET test"}, {"label": "D", "text": "The Breusch-Godfrey test"}], "answerKey": "D", "model_answer": "LECTURE: A statistical test can be designed to detect a specific type of bias or a specific type of assumption violation.\nMany tests can be used to detect a specific type of deviation from the assumptions of a model. For example, the Durbin-Watson test can be used to detect the presence of autocorrelation in a time series.\nThe Breusch-Godfrey test can be used to detect the presence of autocorrelation in a time series. The test can be used to confirm that the data is not autocorrelated.\nThe RESET test can be used to detect the presence of a unit root in a time series. A unit root is a type of autocorrelation that violates the assumptions of many time series models.\nThe White test can be used to detect the presence of autocorrelation in a time series. The test can be used to confirm that the data is not autocorrelated.\nSOLUTION: The Breusch-Godfrey test and the White test can be used to detect the presence of autocorrelation in a time series. These tests can be used to confirm that the data is not autocorrelated.\nThe RESET test can be used to detect the presence of a unit root in a time series. A unit root is a type of autocorrelation that violates the assumptions of many time series models.\nThe Durbin Watson test can be used to detect the presence of autocorrelation in a time series. The test can be used to confirm that the data is not autocorrelated."}
{"question": "The residual from a standard regression model is defined as", "choices": [{"label": "A", "text": "The difference between the actual value, y, and the mean, y-bar"}, {"label": "B", "text": "The difference between the fitted value, y-hat, and the mean, y-bar"}, {"label": "C", "text": "The difference between the actual value, y, and the fitted value, y-hat"}, {"label": "D", "text": "The square of the difference between the fitted value, y-hat, and the mean, y-bar"}], "answerKey": "C", "model_answer": "SOLUTION: The residual from a standard regression model is defined as the difference between the actual value and the predicted value.\nThe actual value is y, and the predicted value is y-hat.\nSo, the residual is the difference between y and y-hat.\nThis is equal to the difference between the actual value, y, and the fitted value, y-hat."}
{"question": "If OLS is applied separately to each equation that is part of a simultaneous system, the resulting estimates will be", "choices": [{"label": "A", "text": "Unbiased and consistent"}, {"label": "B", "text": "Biased but consistent"}, {"label": "C", "text": "Biased and inconsistent"}, {"label": "D", "text": "It is impossible to apply OLS to equations that are part of a simultaneous system"}], "answerKey": "C", "model_answer": "LECTURE: In a simultaneous system, there is an underlying economic mechanism that produces each of the equations. Estimating the parameters of each equation from the system as a whole can bias the estimates. This happens because the estimates affect the values of the right-hand sides of the equations, which in turn affect the estimates.\nWhen ordinary least squares (OLS) is applied separately to each equation in a simultaneous system, the resulting estimates will be biased but consistent. This means that the estimates will be biased, but the bias will be the same for all estimates. So, the estimates will converge to the same values as the number of iterations increases.\nOLS is a form of Gaussian regression. In Gaussian regression, the estimates are driven by the residuals. The residuals are the differences between the observed values and the predicted values. In a simultaneous system, the residuals affect the values of the right-hand sides of the equations. So, the estimates affect the residuals, and the residuals affect the estimates. This means that the estimates will be biased by the residuals.\nThe consistency of OLS in a simultaneous system is a result of the fact that the estimates affect the residuals, and the residuals affect the estimates. So, the estimates converge to the true values as the number of iterations increases.\nSOLUTION: When OLS is applied separately to each equation in a simultaneous system, the estimates will be biased but consistent."}
{"question": "Which one of the following is NOT an example of mis-specification of functional form?", "choices": [{"label": "A", "text": "Using a linear specification when y scales as a function of the squares of x"}, {"label": "B", "text": "Using a linear specification when a double-logarithmic model would be more appropriate"}, {"label": "C", "text": "Modelling y as a function of x when in fact it scales as a function of 1/x"}, {"label": "D", "text": "Excluding a relevant variable from a linear regression model"}], "answerKey": "D", "model_answer": "LECTURE: A functional form is a mathematical formula that describes a relationship between variables. When y is a function of x, the formula will show how x affects y.\nLinear specifications are a type of functional form. In a linear specification, the formula has the form y = a + bx, where x is the independent variable, and a and b are coefficients.\nExamples of functional forms other than the linear specification include:\nQuadratic specifications: y = a + bx + cx^2, where c is a coefficient.\nLogarithmic specifications: y = a + bx + cx^n, where n is a coefficient.\nDouble-logarithmic specifications: y = a + bx + cx^2 log(x), where log(x) is the logarithm of x with base e.\nExcluding a relevant variable from a linear regression model is an example of mis-specification of functional form. A linear regression model is a type of functional form. It is a model that uses a linear specification.\nWhen you exclude a relevant variable from a linear regression model, you are mis-specifying the functional form. This is because the functional form does not include all of the variables that affect the output variable. So, the model is not a good fit for the data."}
{"question": "Suppose that we wished to evaluate the factors that affected the probability that an investor would choose an equity fund rather than a bond fund or a cash investment. Which class of model would be most appropriate?", "choices": [{"label": "A", "text": "A logit model"}, {"label": "B", "text": "A multinomial logit"}, {"label": "C", "text": "A tobit model"}, {"label": "D", "text": "An ordered logit model"}], "answerKey": "B", "model_answer": "LECTURE: Logit is an acronym for logistic function. Logistic functions are used to model the probability that an event will occur. In the context of econometrics, logit models are used to estimate the probability that an investor will choose a particular type of investment.\nThe logit function is used to estimate the probability that a particular investment will be chosen. The logit function is a function of the estimated expected utility of the investment. The higher the estimated expected utility of the investment, the higher the probability that the investment will be chosen.\nA multinomial logit model is a special case of a logit model. It is used to estimate the probability that an investor will choose one of a set of mutually exclusive investments. For example, a multinomial logit model can be used to estimate the probability that an investor will choose a particular equity fund.\nA tobit model is a model that is used to estimate the probability that a dependent variable will be strictly between two threshold values. For example, a tobit model can be used to estimate the probability that an investor will choose a cash investment.\nAn ordered logit model is a model that is used to estimate the probability that a dependent variable will fall within a particular interval. For example, an ordered logit model can be used to estimate the probability that an investor will choose a bond fund.\nSOLUTION: A logit model is used to estimate the probability that an investor will choose an equity fund. A logit model is an appropriate model to use if the dependent variable is a probability.\nA multinomial logit model is a special case of a logit model. It is used to estimate the probability that an investor will choose one of a set of mutually exclusive investments. A multinomial logit model is appropriate if there are multiple investments that the investor can choose from.\nA tobit model is a model that is used to estimate the probability that a dependent variable will be strictly between two threshold values. A tobit model is not appropriate if the dependent variable is a probability.\nAn ordered logit model is a model that is used to estimate the probability that a dependent variable will fall within a particular interval. An ordered logit model is not appropriate if the dependent variable is a probability."}
{"question": "Which of the following statements will be true if the number of replications used in a Monte Carlo study is small?\n\ni) The statistic of interest may be estimated imprecisely\n\n\nii) The results may be affected by unrepresentative combinations of random draws\n\n\niii) The standard errors on the estimated quantities may be unacceptably large\n\n\niv) Variance reduction techniques can be used to reduce the standard errors", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "LECTURE: A Monte Carlo study uses random sampling to estimate the value of a probability. The accuracy of the estimate is determined by the number of replications used in the study.\nThe standard error of a Monte Carlo estimate is a measure of the accuracy of the estimate. It can be used to calculate a confidence interval for the value of the probability.\nThe value of the probability may be estimated imprecisely if the number of replications used in the Monte Carlo study is small. This means that the standard error of the estimate may be large.\nAs the standard error of the estimate gets larger, the confidence interval for the value of the probability may get wider. This means that the results of the study may be affected by unrepresentative combinations of random draws.\nVariance reduction techniques can be used to reduce the standard error of the estimate. These techniques involve changing the way the random samples are drawn.\nIf the number of replications used in a Monte Carlo study is small, the standard errors on the estimated quantities may be unacceptably large. This means that the results of the study may be affected by unrepresentative combinations of random draws.\nSOLUTION: If the number of replications used in a Monte Carlo study is small, the results may be affected by unrepresentative combinations of random draws. This means that the standard errors on the estimated quantities may be unacceptably large.\nVariance reduction techniques can be used to reduce the standard errors. These techniques involve changing the way the random samples are drawn.\nThe standard errors on the estimated quantities may be unacceptably large if the number of replications used in the Monte Carlo study is small. This means that variance reduction techniques may be needed to reduce the standard errors."}
{"question": "Which of the following is a disadvantage of the random effects approach to estimating a panel model?", "choices": [{"label": "A", "text": "The approach may not be valid if the composite error term is correlated with one or more of the explanatory variables"}, {"label": "B", "text": "The number of parameters to estimate may be large, resulting in a loss of degrees of freedom"}, {"label": "C", "text": "The random effects approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable."}, {"label": "D", "text": "All of (a) to (c) are potential disadvantages of the random effects approach."}], "answerKey": "A", "model_answer": "LECTURE: In the random effects model, the intercept and the slope are allowed to vary across panels. This allows the model to capture cross-sectional and temporal variation in the data.\nHowever, there are several potential disadvantages to the random effects approach. One disadvantage is that the approach may not be valid if the composite error term is correlated with one or more of the explanatory variables. This is known as a violation of the assumptions of the model and can lead to biased and unreliable parameter estimates.\nAnother potential disadvantage of the random effects approach is that the number of parameters to estimate may be large, resulting in a loss of degrees of freedom. This can make it difficult to identify important variables and can also lead to instability in the estimates.\nA third potential disadvantage of the random effects approach is that the approach can only capture cross-sectional heterogeneity and not temporal variation in the dependent variable. This means that the model can identify which panels have different values for the dependent variable, but it cannot identify how the dependent variable changes over time.\nSOLUTION: All of the potential disadvantages listed in the prompt are valid."}
{"question": "Which of the following could result in autocorrelated residuals?\n\ni) Slowness of response of the dependent variable to changes in the values of the independent variables\n\nii) Over-reactions of the dependent variable to changes in the independent variables\n\niii) Omission of relevant explanatory variables that are autocorrelated\n\niv) Outliers in the data", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: Autocorrelated residuals occur when the residuals from a statistical model are correlated with the values of the dependent variable. This can happen when relevant explanatory variables are omitted from the model.\nOmitting relevant explanatory variables that are autocorrelated can result in autocorrelated residuals. This can happen when the omitted variable is related to the dependent variable, or when it is highly correlated with one or more of the included variables.\nThe speed of response of the dependent variable to changes in the values of the independent variables does not necessarily affect the correlation of the residuals with the dependent variable.\nOver-reaction of the dependent variable to changes in the independent variables does not necessarily affect the correlation of the residuals with the dependent variable.\nOutliers in the data can affect the correlation of the residuals with the dependent variable."}
{"question": "Which of the following statements are true concerning the acf and pacf?\n\n(i) The acf and pacf are often hard to interpret in practice\n\n(ii) The acf and pacf can be difficult to calculate for some data sets\n\n(iii) Information criteria represent an alternative approach to model order determination\n\n(iv) If applied correctly, the acf and pacf will always deliver unique model selections", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: The autocorrelation function (ACF) and partial autocorrelation function (PACF) are used to model the relationship between a variable and its lag. The ACF shows how the variable relates to itself at different lags. The PACF shows how the variable relates to itself after accounting for the effects of lags that are not equal to zero.\nBoth the ACF and PACF can be difficult to calculate for some data sets. The more lags that are included in the calculation, the more complex the calculations become.\nInformation criteria represent an alternative approach to model order determination. Information criteria evaluate the quality of the fit of a model based on the size of the model and the residual sum of squares.\nThe ACF and PACF will always deliver unique model selections if applied correctly. However, the ACF and PACF cannot identify the correct number of lags."}
{"question": "Which of the following conditions are necessary for a series to be classifiable as a weakly stationary process?\n\n(i) It must have a constant mean\n\n(ii) It must have a constant variance\n\n(iii) It must have constant autocovariances for given lags\n\n(iv) It must have a constant probability distribution", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: The points in the previous answers are correct.\nA weakly stationary process has the following properties:\nIt has a constant mean.\nIt has a constant variance.\nIt has constant autocovariances for given lags.\nIt has a constant probability distribution.\nThe only property that is not necessary for a weakly stationary process is a constant mean."}
{"question": "Consider the following equation and determine the class of model that it best represents\n\n$y_{it} = \\alpha + \\beta_{it} + \\mu_i + \\nu_{it}$", "choices": [{"label": "A", "text": "An entity fixed effects model"}, {"label": "B", "text": "A time fixed effects model"}, {"label": "C", "text": "A random effects model"}, {"label": "D", "text": "A pure time series model"}], "answerKey": "A", "model_answer": "LECTURE: In economics, researchers often use regression analysis to estimate how an independent variable affects a dependent variable. One important type of regression analysis is instrumental variables analysis, which is used when the independent variable is endogenous (that is, it is determined by the dependent variable).\nOne way to do instrumental variables analysis is to include fixed effects in the regression analysis. Fixed effects are used to capture the fact that the independent variable may affect the dependent variable over time. For example, consider the following equation:\nyit = \u03b1 + \u03b2xit + \u03bci + \u03bdit\nIn this equation, yit is the dependent variable, xit is the independent variable, \u03bci is an error term, and \u03bdit is a fixed effect. The fixed effect is different for each unit (i).\nThe error term, \u03bdit, captures the part of the dependent variable that is not explained by the independent variable, the fixed effects, or the other variables in the model. In other words, \u03bdit is the part of the dependent variable that is unique to unit i at time t.\nThe pure time series model represents the case where the fixed effects are the only time-invariant part of the model. In this case, the error term, \u03bdit, is the only thing that changes over time.\nThe entity fixed effects model, the time fixed effects model, and the random effects model represent different ways of including fixed effects in the model.\nIn the entity fixed effects model, each unit i has its own fixed effect, \u03bdi. This model represents the case where units that are more similar to each other have similar values of the dependent variable.\nIn the time fixed effects model, each time t has its own fixed effect, \u03bdt. This model represents the case where units that are more similar to each other have similar values of the dependent variable over time.\nIn the random effects model, the fixed effects are random variables. This model allows the fixed effects to vary between units and over time.\nSOLUTION: The equation:\nyit = \u03b1 + \u03b2it + \u03bci + \u03bdit\nincludes fixed effects, \u03bci and \u03bdit. These fixed effects are different for each unit i. So, this equation is a random effects model."}
{"question": "Note that statistical tables are not necessary to answer this question. For a sample of 1000 observations, the Dickey-Fuller test statistic values are", "choices": [{"label": "A", "text": "More negative than (i.e. bigger in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "B", "text": "Less negative than (i.e. smaller in absolute value than) those in the left hand tail of a normal distribution"}, {"label": "C", "text": "Obtained from an analytical formula for the density of the Dickey-Fuller distribution"}, {"label": "D", "text": "More negative (i.e. bigger in absolute value) for a 10% size of test than a 5% test."}], "answerKey": "A", "model_answer": "SOLUTION: The Dickey-Fuller test statistic is a measure of the difference between the observed and the expected values of the test statistic. The expected value is based on the assumption that the null hypothesis is true.\nThe null hypothesis in the Dickey-Fuller test states that the time series is not stationary. The alternative hypothesis states that the time series is stationary.\nThe test statistic is more negative for a 10% size of test than a 5% test. This means that the test is more sensitive to a difference between the observed and expected values of the test statistic when the size of the test is 10% than when the size of the test is 5%.\nA test statistic that is more negative for a 10% size of test than a 5% test is more negative in absolute value than a test statistic that is more negative for a 5% size of test than a 1% test.\nSo, the test statistic values are more negative in absolute value than those in the left-hand tail of a normal distribution."}
{"question": "Suppose that a hypothesis test is conducted using a 5% significance level. Which of the following statements are correct?\n\n(i) The significance level is equal to the size of the test\n\n(ii) The significance level is equal to the power of the test\n\n(iii) 2.5% of the total distribution will be in each tail rejection region for a 2-sided test\n\n(iv) 5% of the total distribution will be in each tail rejection region for a 2-sided test.", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: The significance level is a level of probability. It is the probability of rejecting the null hypothesis when it is true.\nThe power of a test is a measure of the probability that the test will reject the null hypothesis when the null hypothesis is false.\nThe size of a test is a measure of the probability that the test will reject the null hypothesis when the null hypothesis is true.\nBecause the size of a test is a measure of the probability that the test will reject the null hypothesis when the null hypothesis is true, the size of the test is equal to the power of the test.\nSo, the significance level is equal to the size of the test.\nBecause the power of the test is equal to 1 - beta, and beta is equal to the size of the test divided by the standard deviation of the test statistic, the significance level is equal to the size of the test divided by the standard deviation of the test statistic.\nThis standard deviation of the test statistic is the same for all tests with the same statistic and sample size. So, the significance level is equal to the size of the test divided by a constant.\nFor a 2-sided test, the rejection regions are the tail regions. The most common choice for a 2-sided test is to use alpha = 0.05. So, the size of the test is 0.05.\nTo get the significance level, divide the size of the test by the standard deviation of the test statistic. The standard deviation of the test statistic is 0.5 for a 2-sided test. So, the significance level is 0.05 / 0.5 = 0.1.\nThis means that, for a 2-sided test, 10% of the total distribution will be in each tail rejection region."}
{"question": "Which one of the following criticisms of the Dickey-Fuller/Engle-Granger approach to dealing with cointegrated variables is overcome by the Engle-Yoo (EY) procedure?", "choices": [{"label": "A", "text": "In the context of small samples, Dickey Fuller tests are prone to conclude that there is a unit root in a series when there is not"}, {"label": "B", "text": "The Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one"}, {"label": "C", "text": "The variables are treated asymmetrically in the cointegrating tests"}, {"label": "D", "text": "It is not possible to perform tests about the cointegrating relationship"}], "answerKey": "D", "model_answer": "SOLUTION: The Engle-Yoo (EY) procedure can overcome the following criticisms of the Dickey-Fuller/Engle-Granger approach to dealing with cointegrated variables: In the context of small samples, Dickey Fuller tests are prone to conclude that there is a unit root in a series when there is not. The EY procedure uses a different statistical approach, and so it is not affected by this criticism. The Engle-Granger (EG) approach can only detect up to one cointegrating relationship even though there could be more than one. The EY procedure can detect all of the cointegrating relationships. The variables are treated asymmetrically in the cointegrating tests. The EY procedure does not treat the variables asymmetrically. It is not possible to perform tests about the cointegrating relationship. The EY procedure can perform tests about the cointegrating relationship. The cointegrated tests are based on the assumption that the variables are observed with the same frequency. The EY procedure can use lagged values to substitute for missing values, and so it is not affected by this criticism."}
{"question": "Consider a series that follows an MA(1) with zero mean and a moving average coefficient of 0.4. What is the value of the autocovariance at lag 1?", "choices": [{"label": "A", "text": "0.4"}, {"label": "B", "text": "1"}, {"label": "C", "text": "0.34"}, {"label": "D", "text": "It is not possible to determine the value of the autocovariances without knowing the disturbance variance."}], "answerKey": "D", "model_answer": "LECTURE: A time series is a sequence of data points, measured at successive times. Time series data often exhibit autocorrelation, which is the correlation between the values of the series at different times. That is, past values of the series are correlated with the present value.\nAutocorrelation can be used to model the behavior of a time series. One popular model for a time series is the moving average model. In a moving average model, the series is modeled as a sum of a trend, or main pattern, and a sequence of autocovariance terms.\nThe autocovariance is a measure of how closely the series tends to follow its own past values. The autocovariance at lag 1 is a measure of the correlation between the series at times zero and one.\nSOLUTION: An MA(1) time series has the form:\nY(t) = trend + autocovariance(1) + Y(t-1)\nY(t-1) is the value of the series at the previous time.\nSo, the value of the autocovariance at lag 1 is:\nautocovariance(1) = cor(Y(0), Y(1))\nY(0) and Y(1) are the values of the series at times zero and one, respectively.\nY(0) is equal to the trend, which is zero.\nY(1) is equal to the sum of the autocovariance terms and the disturbance term.\nThe disturbance term has a variance of 0.4.\nSo, the value of the autocovariance at lag 1 is:\nautocovariance(1) = cor(0, 0.4) = 0."}
{"question": "Which of the following estimation techniques are available for the estimation of over-identified systems of simultaneous equations?\n\ni) OLS\n\nii) ILS\n\niii) 2SLS\n\niv) IV", "choices": [{"label": "A", "text": "(iii) only"}, {"label": "B", "text": "(iii) and (iv) only"}, {"label": "C", "text": "(ii), (iii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii) and (iv)"}], "answerKey": "B", "model_answer": "SOLUTION: Over-identified systems have more equations than unknowns. In these cases, all of the parameters in the system are estimated.\nILS (iterative least squares) and 2SLS (two-stage least squares) are two techniques for estimating over-identified systems. ILS uses a iterative process to find the least-squares solution, and 2SLS uses two stages of regression to find the least-squares solution.\nOLS (ordinary least squares) is an estimation technique that assumes that the system is correctly specified. It is not appropriate for over-identified systems.\nIV (instrumental variables) is a technique for estimating the effect of one variable on another variable. It is not appropriate for over-identified systems."}
{"question": "Which one of the following statements best describes a Type II error?", "choices": [{"label": "A", "text": "It is the probability of incorrectly rejecting the null hypothesis"}, {"label": "B", "text": "It is equivalent to the power of the test"}, {"label": "C", "text": "It is equivalent to the size of the test"}, {"label": "D", "text": "It is the probability of failing to reject a null hypothesis that was wrong"}], "answerKey": "D", "model_answer": ""}
{"question": "Which one of the following would be the most appropriate as a 95% (two-sided) confidence interval for the intercept term of the model given in question 21?", "choices": [{"label": "A", "text": "(-4.79,2.19)"}, {"label": "B", "text": "(-4.16,4.16)"}, {"label": "C", "text": "(-1.98,1.98)"}, {"label": "D", "text": "(-5.46,2.86)"}], "answerKey": "D", "model_answer": "LECTURE: A confidence interval is a range of values within which the true value of a population parameter is likely to fall. For example, if a consultant wants to estimate the average number of hours that customers use a self-service tool, she can collect data from a random sample of customers and calculate a 95% confidence interval for the average number of hours.\nA two-sided confidence interval includes both a lower and an upper limit. A one-sided confidence interval specifies either a lower or an upper limit. For example, a one-sided confidence interval for the average number of hours might give an upper limit, indicating that customers spend an average of no more than a certain number of hours per day using the self-service tool.\nWhen the true value of a population parameter is unknown, a confidence interval is an estimate of its range. For example, the confidence interval for the average number of hours that customers use a self-service tool is an estimate of the range of hours that customers use the tool.\nSOLUTION: A confidence interval for the intercept term of a model is a range of values within which the true value of the intercept is likely to fall. If the consumer interest rate and the baseline model are both included in the model, then the intercept term represents the effect of the baseline model on the log of the hazard rate.\nThe baseline model includes only the consumer interest rate. So, the intercept term represents the effect of the consumer interest rate on the log of the hazard rate.\nSubtracting the intercept term from both sides of the log-hazard rate equation gives:\nlog(h) = consumer interest rate + effect of the baseline model on the log of the hazard rate\nThe effect of the baseline model on the log of the hazard rate is a measure of the difference the baseline model makes in the log of the hazard rate. This difference is called the baseline hazard.\nThe baseline hazard is an estimate of the effect of the baseline model on the log of the hazard rate. It can be interpreted as the difference in the log of the hazard rate that would be caused by the consumer interest rate alone.\nSo, the confidence interval for the intercept term is a range of values within which the baseline hazard is likely to fall.\nTo calculate a 95% confidence interval for the intercept term, use the following formulas:\nupper limit = consumer interest rate + 1.96 standard errors.\nlower limit = consumer interest rate - 1.96 standard errors.\nstandard errors = spread / sqrt(sample size).\nspread = the difference between the two log-hazard ratios that correspond to the consumer interest rate and the intercept term.\nThe spread is given in the question. It is the difference between the log-hazard ratios for the consumer interest rate of 4.5 and the intercept term.\nsample size = the number of observations.\nThe number of observations is given in the question.\nSubstitute the values into the formulas.\nThe upper limit is 4.5 + 1.96 / 2.02 = 4.5 + 0.975 = 5.475.\nThe lower limit is 4.5 - 1.96 / 2.02 = 4.5 - 0.975 = 3.525.\nSo, a 95% confidence interval for the intercept term is (3.525, 5.475)."}
{"question": "Which of the following are characteristics of a stationary process?\n\ni) It crosses its mean value frequently\n\n\nii) It has constant mean and variance\n\n\niii) It contains no trend component\n\n\niv) It will be stationary in first difference form", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "D", "model_answer": "SOLUTION: A stationary process has the following properties:\nIt has a constant mean.\nIt has a constant variance.\nIt contains no trend component.\nIt has a normal distribution of residuals.\nIt will be stationary in first difference form.\nSo, a process crosses its mean value frequently (property 1) and will be stationary in first difference form (property 5).\nA process with a constant mean and variance has a normal distribution of residuals (property 4).\nA process containing no trend component has a constant mean and variance (property 2 and 3).\nSo, a process that crosses its mean value frequently and will be stationary in first difference form will have a normal distribution of residuals."}
{"question": "Consider again the VAR model of equation 16. Which of the following conditions must hold for it to be said that there is bi-directional feedback?", "choices": [{"label": "A", "text": "The b and d coefficients significant and the a and c coefficients insignificant"}, {"label": "B", "text": "The a and c coefficients significant and the b and d coefficients insignificant"}, {"label": "C", "text": "The a and c coefficients significant"}, {"label": "D", "text": "The b and d coefficients significant"}], "answerKey": "D", "model_answer": "SOLUTION: Feedback is bi-directional if it affects the variables in both the short run and the long run. To determine which coefficients in the VAR model have significant effects on the long-run dynamics, look at the P.value for each coefficient in the model. The P.value shows how likely it is that each coefficient is significantly affecting the long-run dynamics of the variables.\nThe b and d coefficients are not significant, and their P.values are not equal to 0.05. So, there is no significant bi-directional feedback in the VAR model.\nThe a and c coefficients are significant, and their P.values are equal to 0.05. So, there is significant feedback in the VAR model that affects the long-run dynamics of the variables."}
{"question": "Consider the following sample autocorrelation estimates obtained using 250 data points:\n\n1) Lag 1 2 3\n\n2) Coefficient 0.2 -0.15 -0.1\n\n3) Assuming that the coefficients are approximately normally distributed, which of the coefficients are statistically significant at the 5% level?", "choices": [{"label": "A", "text": "1 only"}, {"label": "B", "text": "1 and 2 only"}, {"label": "C", "text": "1, 2 and 3 only"}, {"label": "D", "text": "It is not possible to determine the statistical significance since no standard errors have been given"}], "answerKey": "B", "model_answer": "SOLUTION: The null hypothesis is that the true coefficient is zero. The alternative hypothesis is that the true coefficient is not zero. To determine the statistical significance of each coefficient, compare its coefficient estimate to its standard error.\nFor the coefficients at lags 1, 2, and 3, the standard errors are given by:\nSE(1) = 0.10, SE(2) = 0.08, and SE(3) = 0.06.\nSo, for lag 1, the coefficient is 0.2 and the standard error is 0.10. Since 0.2 is more than 2 times the standard error, this coefficient is statistically significant at the 5% level.\nFor lag 2, the coefficient is -0.15 and the standard error is 0.08. Since -0.15 is less than the standard error, this coefficient is not statistically significant at the 5% level.\nFor lag 3, the coefficient is -0.1 and the standard error is 0.06. Since -0.1 is less than the standard error, this coefficient is not statistically significant at the 5% level."}
{"question": "Which one of the following is examined by looking at a goodness of fit statistic?", "choices": [{"label": "A", "text": "How well the population regression function fits the data"}, {"label": "B", "text": "How well the sample regression function fits the population regression function"}, {"label": "C", "text": "How well the sample regression function fits the data"}, {"label": "D", "text": "How well the population regression function fits the sample regression function."}], "answerKey": "C", "model_answer": "A goodness of fit statistic can examine how well the population regression function fits the data."}
{"question": "Which of the following statements are correct concerning the use of antithetic variates as part of a Monte Carlo experiment?\n\ni) Antithetic variates work by reducing the number of replications required to cover the whole probability space\n\nii) Antithetic variates involve employing a similar variable to that used in the simulation, but whose properties are known analytically\n\niii) Antithetic variates involve using the negative of each of the random draws and repeating the experiment using those values as the draws\n\niv) Antithetic variates involve taking one over each of the random draws and repeating the experiment using those values as the draws", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iv) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "B", "model_answer": ""}
{"question": "Which one of the following statements is true concerning alternative forecast accuracy measures?", "choices": [{"label": "A", "text": "Mean squared error is usually highly correlated with trading rule profitability"}, {"label": "B", "text": "Mean absolute error provides a quadratic loss function"}, {"label": "C", "text": "Mean absolute percentage error is a useful measure for evaluating asset return forecasts"}, {"label": "D", "text": "Mean squared error penalises large forecast errors disproportionately more than small forecast errors"}], "answerKey": "D", "model_answer": "SOLUTION: Mean squared error is a measure of the average squared difference between predicted and actual values. It is a common measure of forecast accuracy.\nMean absolute error is a measure of the average absolute difference between predicted and actual values. It is also a common measure of forecast accuracy.\nMean absolute percentage error is a measure of the average absolute difference between predicted and actual values, expressed as a percentage of the actual value. It is a useful measure of forecast accuracy for asset return data.\nMean squared error penalizes large forecast errors disproportionately more than small forecast errors. This means that large errors have a greater impact on the measure than small errors."}
{"question": "Which of the following criticisms of standard (\"plain vanilla\") GARCH models can be overcome by EGARCH models?\n\ni) Estimated coefficient values from GARCH models may be negative\n\nii) GARCH models cannot account for leverage effects\n\niii) The responsiveness of future volatility to positive and negative shocks is symmetric under a GARCH formulation\n\niv) GARCH models cannot allow for a feedback from the volatility to the returns", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: Look at the criticisms.\nGARCH models cannot account for leverage effects and the responsiveness of future volatility to positive and negative shocks is symmetric under a GARCH formulation.\nThese are major criticisms of standard GARCH models.\nOn the other hand, EGARCH models can account for leverage effects and the responsiveness of future volatility to positive and negative shocks is not necessarily symmetric under an EGARCH formulation.\nSo, these two criticisms of standard GARCH models can be overcome by EGARCH models."}
{"question": "Suppose that 100 separate firms were tested to determine how many of them \"beat the market\" using a Jensen-type regression, and it is found that 3 fund managers significantly do so. Does this suggest prima facie evidence for stock market inefficiency?", "choices": [{"label": "A", "text": "Yes"}, {"label": "B", "text": "No"}, {"label": "C", "text": "In order to answer this question, you would need to test every fund manager trading in that market"}, {"label": "D", "text": "There is insufficient information given in the question to draw a conclusion about market efficiency."}], "answerKey": "B", "model_answer": "LECTURE: Efficient markets theory is a concept in financial economics that describes how well prices of assets, such as stocks, reflect all available information. According to the efficient markets theory, prices are fully informed, which means that they reflect all publicly available information and all investors' expectations about future cash flows.\nOne of the implications of the efficient markets theory is that it is not possible to consistently beat the market. In other words, it is not possible to outperform the market by using information that is not already reflected in the market prices.\nSOLUTION: The fact that three fund managers significantly beat the market does not provide evidence that the market is inefficient. This is because beating the market is possible in an efficient market. According to the efficient markets theory, prices are fully informed, but this does not mean that it is not possible to find undervalued or overvalued assets. It is possible to beat the market by finding undervalued assets and holding them until their price increases.\nIn order to have evidence that the market is inefficient, you would need to show that the market prices are not fully informed. This could be the case if some investors have access to information that is not available to other investors. But, without further information, you cannot conclude that the market is inefficient based on the fact that three fund managers beat the market."}
{"question": "Which of the following are advantages of the use of panel data over pure cross-sectional or pure time-series modelling?\n\n(i) The use of panel data can increase the number of degrees of freedom and therefore the power of tests\n\n(ii) The use of panel data allows the average value of the dependent variable to vary either cross-sectionally or over time or both\n\n(iii) The use of panel data enables the researcher allows the estimated relationship between the independent and dependent variables to vary either cross-sectionally or over time or both", "choices": [{"label": "A", "text": "(i) only"}, {"label": "B", "text": "(i) and (ii) only"}, {"label": "C", "text": "(ii) only"}, {"label": "D", "text": "(i), (ii), and (iii)"}], "answerKey": "B", "model_answer": "SOLUTION: Panel data have the advantages of increased statistical power and the ability to examine the average value of the dependent variable over time or cross-sectionally or both. These advantages make it possible to examine the relationship between variables more accurately and to identify variables that are important determinants of the dependent variable.\nIn contrast, pure cross-sectional studies have the limitation that the estimates may be unstable. Pure time-series studies have the limitation that it is difficult to identify the causes of the change in the dependent variable."}
{"question": "If the Engle-Granger test is applied to the residuals of a potentially cointegrating regression, what would be the interpretation of the null hypothesis?", "choices": [{"label": "A", "text": "The variables are cointegrated"}, {"label": "B", "text": "The variables are not cointegrated"}, {"label": "C", "text": "Both variables are stationary"}, {"label": "D", "text": "Both variables are non-stationary"}], "answerKey": "B", "model_answer": "LECTURE: A regression model with cointegrated variables involves the use of two or more non-stationary time series. The Engle-Granger test can be used to determine if two variables are cointegrated.\nThe null hypothesis of the Engle-Granger test is that the variables are not cointegrated. The alternative hypothesis is that the variables are cointegrated.\nIf the null hypothesis is rejected, it suggests that the variables are cointegrated. This means that the variables are integrated of different orders. The variable that is integrating at a slower rate is called the dependent variable. The variable that is integrating at a faster rate is called the independent variable.\nIf the variables are cointegrated, you can use the OLS method to estimate the long-run parameters of the model.\nIf the null hypothesis is not rejected, it suggests that the variables are not cointegrated. This means that the variables are not integrated at all. In this case, you cannot use the OLS method to estimate the long-run parameters of the model.\nSOLUTION: The null hypothesis of the Engle-Granger test is that the variables are not cointegrated. If the null hypothesis is not rejected, it is evidence that the variables are not cointegrated.\nThe variables are not cointegrated."}
{"question": "Which of the following statements are true concerning the autocorrelation function (acf) and partial autocorrelation function (pacf)?\n\ni) The acf and pacf will always be identical at lag one whatever the model\n\nii) The pacf for an MA(q) model will in general be non-zero beyond lag q\n\niii) The pacf for an AR(p) model will be zero beyond lag p\n\niv) The acf and pacf will be the same at lag two for an MA(1) model", "choices": [{"label": "A", "text": "(ii) and (iv) only"}, {"label": "B", "text": "(i) and (iii) only"}, {"label": "C", "text": "(i), (ii), and (iii) only"}, {"label": "D", "text": "(i), (ii), (iii), and (iv)"}], "answerKey": "C", "model_answer": "SOLUTION: The acf and pacf are used to determine the order of a model.\nThe acf shows how the autocorrelations change as the lag increases. For an MA(1) model, the acf is non-zero at lag one and zero at other lags. For an AR(1) model, the acf is non-zero at lag one and increases as the lag increases (reflecting the correlation of the current observation with earlier observations). For an AR(2) model, the acf is non-zero at lags one and two, and increases as the lag increases.\nThe pacf shows how the partial correlations change as the lag increases. For an MA(1) model, the pacf is non-zero at lag one and zero at other lags. For an AR(1) model, the pacf is non-zero at lag one and decreases as the lag increases (reflecting the correlation of the error term with earlier observations). For an AR(2) model, the pacf is non-zero at lags one and two, and decreases as the lag increases.\nThere is one lag at which the acf and pacf are the same for an MA(1) model: lag one. So, the acf and pacf will be identical at lag one whatever the model.\nThe pacf for an MA(q) model will in general be non-zero beyond lag q. This is because the MA(q) model has autocorrelations only up to lag q.\nThe pacf for an AR(p) model will be zero beyond lag p. This is because the AR(p) model has partial correlations only up to lag p."}
